{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e705c7c9ba594203a546cae1f3282fd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b449a1a934e843a290641136585a357c","IPY_MODEL_94fb4429be2a4995864af78375b19ec7","IPY_MODEL_ed9df7b90ed44468ac378f5dc6733ab1"],"layout":"IPY_MODEL_0ce22cbcab594c9f8b7f734396268d87"}},"b449a1a934e843a290641136585a357c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec5ce73f53134f4e80814cb7b2a94174","placeholder":"​","style":"IPY_MODEL_790d70eaa1484883a890b3452a1fbfaf","value":"Sanity Checking DataLoader 0: 100%"}},"94fb4429be2a4995864af78375b19ec7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd3b53db7f5b4d7ba08c35741fcb16dc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6734921ad5d4770a4943b8e1f830b51","value":2}},"ed9df7b90ed44468ac378f5dc6733ab1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05874d2ee08a4b87bdc53278c6e9ec0b","placeholder":"​","style":"IPY_MODEL_bb09a233347045d1841c2f563b0dd033","value":" 2/2 [00:00&lt;00:00,  6.12it/s]"}},"0ce22cbcab594c9f8b7f734396268d87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ec5ce73f53134f4e80814cb7b2a94174":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790d70eaa1484883a890b3452a1fbfaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd3b53db7f5b4d7ba08c35741fcb16dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6734921ad5d4770a4943b8e1f830b51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05874d2ee08a4b87bdc53278c6e9ec0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb09a233347045d1841c2f563b0dd033":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25e971a494374bd2b67607fec5493568":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f97495d611f4260b44bbc0574d68733","IPY_MODEL_63e9c8fb9a164f43827f9e9aebc78056","IPY_MODEL_2db5e69cb39d4ec1b2767501303e7072"],"layout":"IPY_MODEL_faac0d105c244a8291f24ff77401a0d1"}},"6f97495d611f4260b44bbc0574d68733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c14c45f80720446a961a9318c35343d2","placeholder":"​","style":"IPY_MODEL_621a5fdc15a04fde93e38355badde153","value":"Epoch 4: 100%"}},"63e9c8fb9a164f43827f9e9aebc78056":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_999ffbccf7a34853b9bfc399d190d885","max":510,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c6b258223b446f5a9ca718720edcd1d","value":510}},"2db5e69cb39d4ec1b2767501303e7072":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ddfbd63d6094e139cb9b08b9e11b7c1","placeholder":"​","style":"IPY_MODEL_74ceb2c38dec4385a4628ba980ce277d","value":" 510/510 [02:55&lt;00:00,  2.90it/s, v_num=2, train_loss_step=0.0675, val_loss_step=0.122, val_loss_epoch=0.0642, train_loss_epoch=0.0466]"}},"faac0d105c244a8291f24ff77401a0d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"c14c45f80720446a961a9318c35343d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"621a5fdc15a04fde93e38355badde153":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"999ffbccf7a34853b9bfc399d190d885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c6b258223b446f5a9ca718720edcd1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ddfbd63d6094e139cb9b08b9e11b7c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74ceb2c38dec4385a4628ba980ce277d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"750d04563c794e698a664953f80a2a4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca3a6b73a48744ff9a5074255e99f280","IPY_MODEL_1807e8da6f514a8b9b147f52d7b27333","IPY_MODEL_3912e309ffc749c4871726a4eaa8043b"],"layout":"IPY_MODEL_1e417dcffca246e48a48aff2f0042184"}},"ca3a6b73a48744ff9a5074255e99f280":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea44e3ecb6714c9a96190428921b93bb","placeholder":"​","style":"IPY_MODEL_f5f683c3ad3147b9965bb13e7a1c7e3f","value":"Validation DataLoader 0: 100%"}},"1807e8da6f514a8b9b147f52d7b27333":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_006d5768faef47b0b4350ca548c74eec","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca0b2d919e404e8987f10f66a05202e6","value":128}},"3912e309ffc749c4871726a4eaa8043b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7ce61cd1099487999ffce727bbb8255","placeholder":"​","style":"IPY_MODEL_347cbee7988548b69924cdfb81e30303","value":" 128/128 [00:16&lt;00:00,  7.72it/s]"}},"1e417dcffca246e48a48aff2f0042184":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ea44e3ecb6714c9a96190428921b93bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5f683c3ad3147b9965bb13e7a1c7e3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"006d5768faef47b0b4350ca548c74eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca0b2d919e404e8987f10f66a05202e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7ce61cd1099487999ffce727bbb8255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"347cbee7988548b69924cdfb81e30303":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3458773742fe40a48c51dcd52fc4652b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1900eab39e18493a90d25e63466fe848","IPY_MODEL_a21ff787408745be819a5a2362236b07","IPY_MODEL_58960bd72daf4698b06d90351f68a5a5"],"layout":"IPY_MODEL_b6fa4e0e6b5a48188be077fcd9ee2760"}},"1900eab39e18493a90d25e63466fe848":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2d0df0cafd34520b5d4b9d8d1b0f60c","placeholder":"​","style":"IPY_MODEL_eecce48ed8db41ffbad1d086d116e8e1","value":"Validation DataLoader 0: 100%"}},"a21ff787408745be819a5a2362236b07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd60589aba33449c82bf529616504c23","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c224fefba4764121a518c64c4d30ddc2","value":128}},"58960bd72daf4698b06d90351f68a5a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49a6cb0d5f55448d873a9fe70945cf55","placeholder":"​","style":"IPY_MODEL_976ab6397ce54a0285e040a029510263","value":" 128/128 [00:16&lt;00:00,  7.67it/s]"}},"b6fa4e0e6b5a48188be077fcd9ee2760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"c2d0df0cafd34520b5d4b9d8d1b0f60c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eecce48ed8db41ffbad1d086d116e8e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd60589aba33449c82bf529616504c23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c224fefba4764121a518c64c4d30ddc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49a6cb0d5f55448d873a9fe70945cf55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"976ab6397ce54a0285e040a029510263":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da71ba8b587640d689437cc24650a352":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3db54ae8bc346899ecf5cc2d71e5862","IPY_MODEL_93f1d1737d184be6829b74a2ea76cd86","IPY_MODEL_8ecfaef84f2d4f17887767864b80d6ac"],"layout":"IPY_MODEL_9f09924ce0de4bdb837e6cafb397f687"}},"f3db54ae8bc346899ecf5cc2d71e5862":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1809693f8e34516a9bb75d30e5fc28d","placeholder":"​","style":"IPY_MODEL_d4d8867233564de488867de3c2b9e89f","value":"Validation DataLoader 0: 100%"}},"93f1d1737d184be6829b74a2ea76cd86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_20ed1eb66f69486f91f2701a16894db8","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f9eae37490b4534b92e6219ccf287f3","value":128}},"8ecfaef84f2d4f17887767864b80d6ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b4b76c08f69472bbcca39309c72e0c4","placeholder":"​","style":"IPY_MODEL_7c5c1959b8a9419b9d4ba2554ccc93d8","value":" 128/128 [00:16&lt;00:00,  7.66it/s]"}},"9f09924ce0de4bdb837e6cafb397f687":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"a1809693f8e34516a9bb75d30e5fc28d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d8867233564de488867de3c2b9e89f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20ed1eb66f69486f91f2701a16894db8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9eae37490b4534b92e6219ccf287f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b4b76c08f69472bbcca39309c72e0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5c1959b8a9419b9d4ba2554ccc93d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e27845142fd042919e447c66ab8490be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_433ea8a3414145d1a5251dd2e64bb41a","IPY_MODEL_d8e47e3ac6484bef8640cba4d425b8f5","IPY_MODEL_f30b207b3b134688a55ddb8e2b1fe277"],"layout":"IPY_MODEL_5235ae6ebca0403eae1853cf948d3b68"}},"433ea8a3414145d1a5251dd2e64bb41a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8c02834f09a4b518b21499f1130ed97","placeholder":"​","style":"IPY_MODEL_a5e58f21701446c8bd40811c561433cc","value":"Validation DataLoader 0: 100%"}},"d8e47e3ac6484bef8640cba4d425b8f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f8c4cb4ede94963bea6b5fe02b1252a","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53f747c9883c4f3db54b183cec4ab0bf","value":128}},"f30b207b3b134688a55ddb8e2b1fe277":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b688d854caff462a91f8c3d58ba69afd","placeholder":"​","style":"IPY_MODEL_590ae3f651f04d10bc0f29923bed1089","value":" 128/128 [00:16&lt;00:00,  7.70it/s]"}},"5235ae6ebca0403eae1853cf948d3b68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"d8c02834f09a4b518b21499f1130ed97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5e58f21701446c8bd40811c561433cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f8c4cb4ede94963bea6b5fe02b1252a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53f747c9883c4f3db54b183cec4ab0bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b688d854caff462a91f8c3d58ba69afd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"590ae3f651f04d10bc0f29923bed1089":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ccfda1b7d654c2ca18dff2c408e371c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9ad0071d2b7489898a2de0d7411e390","IPY_MODEL_41cb4dddc641438fbaaada433bba7715","IPY_MODEL_924b2eadc954483681d074f73304510d"],"layout":"IPY_MODEL_c51ba343ab6948cbb89a18840638d00c"}},"f9ad0071d2b7489898a2de0d7411e390":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b9a7700f11348868b822f2752893923","placeholder":"​","style":"IPY_MODEL_5e851628783546edb57ee19e8e5f2ee2","value":"Validation DataLoader 0: 100%"}},"41cb4dddc641438fbaaada433bba7715":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c1ec75c42cd4d7d9758ac70fcd5b3cf","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7abb05786884d3a974b24b943b4c9f0","value":128}},"924b2eadc954483681d074f73304510d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20d4399103d14cbca382f334ed18b71c","placeholder":"​","style":"IPY_MODEL_d61359b06eb9422f9a9178439ae2530b","value":" 128/128 [00:16&lt;00:00,  7.67it/s]"}},"c51ba343ab6948cbb89a18840638d00c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"8b9a7700f11348868b822f2752893923":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e851628783546edb57ee19e8e5f2ee2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c1ec75c42cd4d7d9758ac70fcd5b3cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7abb05786884d3a974b24b943b4c9f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20d4399103d14cbca382f334ed18b71c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d61359b06eb9422f9a9178439ae2530b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Train T5 on Lyrics Data Only"],"metadata":{"id":"I7kCjwu4Rl2U"}},{"cell_type":"markdown","source":["## Import Necessary Dependencies"],"metadata":{"id":"Tx_sv__vSP-G"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMq71VljYUei","outputId":"ece05a08-2212-4d98-f417-3cfeb0e5ec03","executionInfo":{"status":"ok","timestamp":1733444932424,"user_tz":480,"elapsed":34307,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n","Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.46.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bert-score\n","Successfully installed bert-score-0.3.13\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2c4311416ba5a71b0cae9166b532f1c2fcd2b8b1e81ea9a5f2da49c1cafe6d61\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}],"source":["# import needed dependencies for testing PoemSum model\n","!pip install pytorch-lightning transformers torch\n","\n","# Import needed dependencies while avoiding conflicts\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import (\n","    T5ForConditionalGeneration,\n","    T5TokenizerFast as T5Tokenizer,\n","    AdamW\n",")\n","import re\n","import os\n","from sklearn.model_selection import train_test_split\n","from typing import Dict, List\n","import numpy as np\n","from tqdm import tqdm\n","import gc\n","from transformers import get_linear_schedule_with_warmup\n","\n","# attempt to view summaries briefly under code blocks\n","import textwrap\n","def print_summary(text, width=70):\n","    print(textwrap.fill(text, width=width))\n","\n","# Evaluation model dependencies\n","# Install required packages\n","!pip install bert-score\n","!pip install rouge-score\n","\n","from bert_score import score\n","from sklearn.model_selection import train_test_split\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","from typing import Dict, List, Tuple"]},{"cell_type":"markdown","source":["## Data Preparation\n","\n","This file contains the T5 model trained on the song lyrics in our dataset only.\n","- Pulling in song data from Cleaned Song Files\n"],"metadata":{"id":"dq9Fv1GtblFQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OiBrKdgzSi9Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733445130603,"user_tz":480,"elapsed":26184,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"2e4d8f8e-6c6b-421c-dc6d-631a098d93c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#Train on all artists\n","\n","# Initialize an empty list to store DataFrames\n","df_list = []\n","\n","# even though the folder is in \"Shared with me\", call \"MyDrive\" to pull from Cleaned Song Files\n","folder_path = \"/content/drive/My Drive/266 Final Project/Cleaned Song Files\"\n","# Iterate through each file in the directory\n","for filename in os.listdir(folder_path):\n","  # Check if the file is a CSV file\n","  if filename.endswith('.csv'):\n","    #Construct the full file path\n","    file_path = os.path.join(folder_path, filename)\n","    # Read the CSV file and append it to the list\n","    df = pd.read_csv(file_path)\n","    df_list.append(df)\n","\n","# Concatenate all DataFrames in the list into a single DataFrame\n","df = pd.concat(df_list, ignore_index=True)"],"metadata":{"id":"43Y7qGqUFhgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#finding number of rows in df\n","print(len(df))\n","#printing first 3 rows\n","print(df.head(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29lsP3zBTwxD","executionInfo":{"status":"ok","timestamp":1733445725315,"user_tz":480,"elapsed":230,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"9e291659-77ff-45da-b12f-7941b4e6193f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3187\n","   Song ID            Title  \\\n","0  2266961     Back To Back   \n","1  1865293            Can I   \n","2    74017  Best I Ever Had   \n","\n","                                          Lyrics URL  \\\n","0       https://genius.com/Drake-back-to-back-lyrics   \n","1              https://genius.com/Drake-can-i-lyrics   \n","2  https://genius.com/Nicki-minaj-and-drake-best-...   \n","\n","                                Combined Annotations  \\\n","0  “Back to Back” is the second of Drake’s respon...   \n","1  In a swiftly deleted   on October 12, 2015, Dr...   \n","2  “Best I Ever Had”, by Drake comes off of the 2...   \n","\n","                                Wikipedia Annotation  \\\n","0  Back to Back or backtoback may refer to Film a...   \n","1  Can I may refer to Can I, a 2010 album by Jaic...   \n","2  Best I Ever Had may refer to Best I Ever Had D...   \n","\n","                                              Lyrics generated_annotation  \n","0  Oh man, oh man, oh man Not again Yeah, I learn...                  NaN  \n","1  Can I, baby? Can I, baby? Can I, baby? Can I, ...                  NaN  \n","2  Intro Drake You know alot of girls be thinking...                  NaN  \n"]}]},{"cell_type":"code","source":["# Calculate average string length of the column\n","average_length = df['Lyrics'].str.len().mean()\n","min_length = df['Lyrics'].str.len().min()\n","max_length = df['Lyrics'].str.len().max()\n","\n","# Display the result\n","print(\"Average string length of lyrics:\", average_length)\n","print(\"Min string length of lyrics:\", min_length)\n","print(\"Max string length of lyrics:\", max_length)"],"metadata":{"id":"_yM9fTn10qpw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733445727835,"user_tz":480,"elapsed":340,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"560c2e14-63d4-4394-e671-2366accf46f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average string length of lyrics: 1456.5506551613935\n","Min string length of lyrics: 12.0\n","Max string length of lyrics: 5686.0\n"]}]},{"cell_type":"code","source":["# Display the data types of each column in the DataFrame\n","print(df.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf60BGF0RIzZ","executionInfo":{"status":"ok","timestamp":1733445729411,"user_tz":480,"elapsed":223,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"cd74102c-00f5-4d30-eaa5-219ae4e80b69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Song ID                 object\n","Title                   object\n","Lyrics URL              object\n","Combined Annotations    object\n","Wikipedia Annotation    object\n","Lyrics                  object\n","generated_annotation    object\n","dtype: object\n"]}]},{"cell_type":"code","source":["# Force conversion to string and fill NaN with empty string\n","df['Lyrics'] = df['Lyrics'].astype(str).fillna('')\n","#df['summary'] = df['summary'].astype(str).fillna('')"],"metadata":{"id":"pPd-I0xMRfsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Class Modules"],"metadata":{"id":"hcct8nQQby2J"}},{"cell_type":"code","source":["# Custom Dataset class from PoemSum model\n","# CM made some minor modifications to hopefully improve efficiency\n","\n","class LyricsSummaryDataset(Dataset):\n","    def __init__(\n","        self,\n","        data: pd.DataFrame,\n","        tokenizer: T5Tokenizer,\n","        text_max_token_len: int = 1024, # CM reduced this from 2000\n","        summary_max_token_len: int = 512 # CM reduced from 10000\n","    ):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.text_max_token_len = text_max_token_len\n","        self.summary_max_token_len = summary_max_token_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index: int):\n","        data_row = self.data.iloc[index]\n","\n","        text_encoding = self.tokenizer(\n","            data_row[\"text\"],\n","            max_length=self.text_max_token_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        summary_encoding = self.tokenizer(\n","            data_row[\"summary\"],\n","            max_length=self.summary_max_token_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        labels = summary_encoding[\"input_ids\"]\n","        labels[labels == 0] = -100\n","\n","        return dict(\n","            text=data_row[\"text\"],\n","            summary=data_row[\"summary\"],\n","            text_input_ids=text_encoding[\"input_ids\"].flatten(),\n","            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n","            labels=labels.flatten(),\n","            labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n","        )\n","\n","# Lightning Data Module from Poem Sum\n","class LyricsSummaryDataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        train_df: pd.DataFrame,\n","        val_df: pd.DataFrame,\n","        tokenizer: T5Tokenizer,\n","        batch_size: int = 8,\n","        text_max_token_len: int = 1024, # CM increased from 512\n","        summary_max_token_len: int = 512 # CM increased from 256\n","    ):\n","        super().__init__()\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.text_max_token_len = text_max_token_len\n","        self.summary_max_token_len = summary_max_token_len\n","\n","    def setup(self, stage=None):\n","        self.train_dataset = LyricsSummaryDataset(\n","            self.train_df,\n","            self.tokenizer,\n","            self.text_max_token_len,\n","            self.summary_max_token_len\n","        )\n","        self.val_dataset = LyricsSummaryDataset(\n","            self.val_df,\n","            self.tokenizer,\n","            self.text_max_token_len,\n","            self.summary_max_token_len\n","        )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=4, # CM increased from 2\n","            pin_memory=True # CM added for GPU efficiency\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=4,  # CM increased from 2\n","            pin_memory=True # CM added for GPU efficiency\n","        )\n","\n","# Model Class\n","class LyricsSummaryModel(pl.LightningModule):\n","  # CM added learning rate here\n","    def __init__(\n","        self,\n","        model_name='t5-small',\n","        learning_rate=1e-4,\n","        weight_decay=0.01, # CM added\n","        warmup_steps=1000, # CM added\n","        ):\n","        super().__init__()\n","        self.model = T5ForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n","\n","        # CM added calls for learning_rate, weight_decay, warmup_steps and save_hyperparameters()\n","        self.learning_rate = learning_rate\n","        self.weight_decay = weight_decay\n","        self.warmup_steps = warmup_steps\n","        self.save_hyperparameters()\n","\n","    # CM adding in calculating_copying_penalty function during fine-tuning\n","    def calculate_copying_penalty(self, input_ids, output_ids):\n","        \"\"\"Calculate penalty for copying from input\"\"\"\n","        batch_size = input_ids.size(0)\n","        penalties = []\n","\n","        for i in range(batch_size):\n","            input_text = input_ids[i].tolist()\n","            output_text = output_ids[i].tolist()\n","\n","            # Calculate n-gram overlap\n","            n_gram_sizes = [2, 3, 4]\n","            overlap_ratios = []\n","\n","            for n in n_gram_sizes:\n","                input_ngrams = set()\n","                output_ngrams = set()\n","\n","                # Create n-grams for input and output\n","                for j in range(len(input_text) - n + 1):\n","                    input_ngrams.add(tuple(input_text[j:j+n]))\n","                for j in range(len(output_text) - n + 1):\n","                    output_ngrams.add(tuple(output_text[j:j+n]))\n","\n","                if output_ngrams:\n","                    overlap = len(input_ngrams.intersection(output_ngrams))\n","                    overlap_ratios.append(overlap / len(output_ngrams))\n","\n","            # Average overlap across different n-gram sizes\n","            penalties.append(sum(overlap_ratios) / len(overlap_ratios))\n","\n","        return torch.tensor(sum(penalties) / batch_size, device=input_ids.device)\n","\n","    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n","        output = self.model(\n","            input_ids=input_ids,    # CM added = input_ids\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            decoder_attention_mask=decoder_attention_mask\n","        )\n","        return output.loss, output.logits\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, output = self(\n","            batch[\"text_input_ids\"],\n","            batch[\"text_attention_mask\"],\n","            batch[\"labels_attention_mask\"],\n","            batch[\"labels\"]\n","        )\n","\n","        # CM added on_step and on_epoch statements\n","        self.log(\"train_loss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss, output = self(\n","            batch[\"text_input_ids\"],\n","            batch[\"text_attention_mask\"],\n","            batch[\"labels_attention_mask\"],\n","            batch[\"labels\"]\n","        )\n","\n","        # CM added on_step and on_epoch statements\n","        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","        # CM added to clean up memory\n","        del output\n","        torch.cuda.empty_cache()\n","\n","        return loss\n","    # CM updated function\n","    def configure_optimizers(self):\n","        # Create optimizer\n","        optimizer = AdamW(\n","            self.parameters(),\n","            lr=self.learning_rate,\n","            weight_decay=self.weight_decay\n","        )\n","\n","        # Create scheduler\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=self.warmup_steps,\n","            num_training_steps=self.trainer.estimated_stepping_batches\n","        )\n","\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"interval\": \"step\"\n","            }\n","        }"],"metadata":{"id":"TCBObUD_YcPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create and Train T5 model"],"metadata":{"id":"zhoIdvxjbrz8"}},{"cell_type":"markdown","source":["Modified baseline model so that we train the T5 model on lyrics data only, replacing \"summary\" column with \"Lyrics\"\n","- self-supervised learning\n","- hopefully this leads to a model that is better tailored to characteristics of song lyrics to ultimately create a model that generates better analyses/annotations"],"metadata":{"id":"-skOETbXrbPX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"syl1jjHPYQFd"}},{"cell_type":"code","source":["# added function for self-supervised learning fine-tuning\n","def create_target_summary(lyrics: str, max_words: int = 50) -> str:\n","    \"\"\"Create shorter target summaries for training\"\"\"\n","    lines = lyrics.split('\\n')\n","    filtered_lines = [line.strip() for line in lines if line.strip()]\n","\n","    # Try to identify chorus or repeated sections\n","    line_counts = {}\n","    for line in filtered_lines:\n","        line_counts[line] = line_counts.get(line, 0) + 1\n","\n","    # Get most repeated lines and important first/last lines\n","    important_lines = []\n","\n","    # Add most repeated lines (likely chorus)\n","    repeated_lines = sorted(line_counts.items(), key=lambda x: x[1], reverse=True)\n","    important_lines.extend(line[0] for line in repeated_lines[:2])\n","\n","    # Add first line if not already included\n","    if filtered_lines and filtered_lines[0] not in important_lines:\n","        important_lines.append(filtered_lines[0])\n","\n","    # Join and truncate\n","    summary = ' '.join(important_lines)\n","    words = summary.split()\n","    if len(words) > max_words:\n","        summary = ' '.join(words[:max_words])\n","\n","    return summary"],"metadata":{"id":"1sK_WnHK_IhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Do this before calling create_baseline_model to keep dedicated test set\n","train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"],"metadata":{"id":"6IEFtp2TbtD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_baseline_model(df, save_dir=\"checkpoints\", learning_rate=1e-4): # CM added learning rate here as well\n","  # 1. Initialize model and tokenizer\n","  print(\"Initializing model and tokenizer...\")\n","  MODEL_NAME = 't5-small'\n","  tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","  model = LyricsSummaryModel(\n","        MODEL_NAME,\n","        learning_rate=learning_rate # CM added learning_rate here\n","    )\n","\n","  # 2. Prepare data\n","  print(\"Preparing data...\")\n","\n","  # CM: The original code lacked a 'summary' column, causing a KeyError.\n","  # We'll add a dummy 'summary' column to the DataFrame.\n","\n","  # Clean data and handle NaN values\n","  df['Lyrics'] = df['Lyrics'].astype(str).fillna('')\n","  # df['summary'] = df['Lyrics'] # Using 'Lyrics' as target for self-supervised learning\n","  # Create shorter target summaries during training\n","  prepared_data = pd.DataFrame({\n","        'text': df.apply(\n","            lambda x: f\"Generate a brief summary capturing the main themes: {x['Lyrics']}\",\n","            axis=1\n","        ),\n","        'summary': df.apply(\n","            lambda x: ' '.join(x['Lyrics'].split()[:50]),  # Take first 50 words as target\n","            axis=1\n","        )\n","    })\n","\n","  # 3. Split data\n","  train_size = int(0.8 * len(prepared_data))\n","  train_data = prepared_data[:train_size]\n","  val_data = prepared_data[train_size:]\n","\n","  # 4. Set up data module\n","  data_module = LyricsSummaryDataModule(\n","      train_df=train_data,\n","      val_df=val_data,\n","      tokenizer=tokenizer,\n","      batch_size=4 # CM increased batch size from 2 to 4\n","  )\n","\n","  # 5. Set up trainer\n","  trainer = pl.Trainer(\n","      max_epochs=5, # CM increased epochs from 2 to 5.\n","      accumulate_grad_batches=4, # CM increased effective batch size from 2 to 4.\n","      gradient_clip_val=1.0,\n","      precision=16 if torch.cuda.is_available() else 32,\n","      enable_checkpointing=True,\n","      default_root_dir=save_dir,\n","      # CM added callbacks and logger sections below\n","      callbacks=[\n","          # Add early stopping\n","          pl.callbacks.EarlyStopping(\n","              monitor='val_loss',\n","              patience=5, # CM increased patience from 3 to 5.\n","              mode='min'\n","          ),\n","          # Add learning rate monitoring\n","          pl.callbacks.LearningRateMonitor(logging_interval='step')\n","          ],\n","      logger=pl.loggers.TensorBoardLogger(\n","          save_dir=save_dir,\n","          name='lyrics_model'\n","          ))\n","\n","  # 6. Train model\n","  print(\"Starting training...\")\n","  trainer.fit(model, data_module)\n","\n","  # 7. Save model and tokenizer\n","  print(\"Saving model and tokenizer...\")\n","  model_path = os.path.join(save_dir, \"final_model\")\n","  os.makedirs(model_path, exist_ok=True)\n","  model.model.save_pretrained(model_path)\n","  tokenizer.save_pretrained(model_path)\n","\n","  return model, tokenizer, trainer"],"metadata":{"id":"kuQ5bk7Ir4Zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run Model with Full Dataset"],"metadata":{"id":"kp1jAHOfTYdn"}},{"cell_type":"markdown","source":["Testing with all data"],"metadata":{"id":"XRTQgHGmmHD3"}},{"cell_type":"code","source":["# Train model on full dataset\n","print(f\"Full dataset size: {len(df)}\")\n","model, tokenizer, trainer = create_baseline_model(train_val_df)\n","\n","# Save to Google Drive\n","drive_path = '/content/drive/My Drive/266 Final Project/Our Models/lyrics_model_full'\n","os.makedirs(drive_path, exist_ok=True)\n","\n","print(f\"Saving model to {drive_path}...\")\n","model.model.save_pretrained(drive_path)\n","tokenizer.save_pretrained(drive_path)\n","\n","# Verify the save\n","if os.path.exists(drive_path):\n","    print(f\"Files saved in {drive_path}:\")\n","    print(os.listdir(drive_path))\n","else:\n","    print(\"Save path not found\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607,"referenced_widgets":["e705c7c9ba594203a546cae1f3282fd2","b449a1a934e843a290641136585a357c","94fb4429be2a4995864af78375b19ec7","ed9df7b90ed44468ac378f5dc6733ab1","0ce22cbcab594c9f8b7f734396268d87","ec5ce73f53134f4e80814cb7b2a94174","790d70eaa1484883a890b3452a1fbfaf","cd3b53db7f5b4d7ba08c35741fcb16dc","a6734921ad5d4770a4943b8e1f830b51","05874d2ee08a4b87bdc53278c6e9ec0b","bb09a233347045d1841c2f563b0dd033","25e971a494374bd2b67607fec5493568","6f97495d611f4260b44bbc0574d68733","63e9c8fb9a164f43827f9e9aebc78056","2db5e69cb39d4ec1b2767501303e7072","faac0d105c244a8291f24ff77401a0d1","c14c45f80720446a961a9318c35343d2","621a5fdc15a04fde93e38355badde153","999ffbccf7a34853b9bfc399d190d885","4c6b258223b446f5a9ca718720edcd1d","9ddfbd63d6094e139cb9b08b9e11b7c1","74ceb2c38dec4385a4628ba980ce277d","750d04563c794e698a664953f80a2a4b","ca3a6b73a48744ff9a5074255e99f280","1807e8da6f514a8b9b147f52d7b27333","3912e309ffc749c4871726a4eaa8043b","1e417dcffca246e48a48aff2f0042184","ea44e3ecb6714c9a96190428921b93bb","f5f683c3ad3147b9965bb13e7a1c7e3f","006d5768faef47b0b4350ca548c74eec","ca0b2d919e404e8987f10f66a05202e6","b7ce61cd1099487999ffce727bbb8255","347cbee7988548b69924cdfb81e30303","3458773742fe40a48c51dcd52fc4652b","1900eab39e18493a90d25e63466fe848","a21ff787408745be819a5a2362236b07","58960bd72daf4698b06d90351f68a5a5","b6fa4e0e6b5a48188be077fcd9ee2760","c2d0df0cafd34520b5d4b9d8d1b0f60c","eecce48ed8db41ffbad1d086d116e8e1","bd60589aba33449c82bf529616504c23","c224fefba4764121a518c64c4d30ddc2","49a6cb0d5f55448d873a9fe70945cf55","976ab6397ce54a0285e040a029510263","da71ba8b587640d689437cc24650a352","f3db54ae8bc346899ecf5cc2d71e5862","93f1d1737d184be6829b74a2ea76cd86","8ecfaef84f2d4f17887767864b80d6ac","9f09924ce0de4bdb837e6cafb397f687","a1809693f8e34516a9bb75d30e5fc28d","d4d8867233564de488867de3c2b9e89f","20ed1eb66f69486f91f2701a16894db8","0f9eae37490b4534b92e6219ccf287f3","4b4b76c08f69472bbcca39309c72e0c4","7c5c1959b8a9419b9d4ba2554ccc93d8","e27845142fd042919e447c66ab8490be","433ea8a3414145d1a5251dd2e64bb41a","d8e47e3ac6484bef8640cba4d425b8f5","f30b207b3b134688a55ddb8e2b1fe277","5235ae6ebca0403eae1853cf948d3b68","d8c02834f09a4b518b21499f1130ed97","a5e58f21701446c8bd40811c561433cc","9f8c4cb4ede94963bea6b5fe02b1252a","53f747c9883c4f3db54b183cec4ab0bf","b688d854caff462a91f8c3d58ba69afd","590ae3f651f04d10bc0f29923bed1089","5ccfda1b7d654c2ca18dff2c408e371c","f9ad0071d2b7489898a2de0d7411e390","41cb4dddc641438fbaaada433bba7715","924b2eadc954483681d074f73304510d","c51ba343ab6948cbb89a18840638d00c","8b9a7700f11348868b822f2752893923","5e851628783546edb57ee19e8e5f2ee2","2c1ec75c42cd4d7d9758ac70fcd5b3cf","d7abb05786884d3a974b24b943b4c9f0","20d4399103d14cbca382f334ed18b71c","d61359b06eb9422f9a9178439ae2530b"]},"id":"MrghHdPJseof","executionInfo":{"status":"ok","timestamp":1733380370812,"user_tz":480,"elapsed":885386,"user":{"displayName":"Chloe M","userId":"03955309346947875383"}},"outputId":"edd2b752-386e-4415-a512-60618261d502"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Full dataset size: 3187\n","Initializing model and tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"stream","name":"stdout","text":["Preparing data...\n","Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                       | Params | Mode\n","------------------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M | eval\n","------------------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n","0         Modules in train mode\n","277       Modules in eval mode\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e705c7c9ba594203a546cae1f3282fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e971a494374bd2b67607fec5493568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"750d04563c794e698a664953f80a2a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3458773742fe40a48c51dcd52fc4652b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da71ba8b587640d689437cc24650a352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27845142fd042919e447c66ab8490be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ccfda1b7d654c2ca18dff2c408e371c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"]},{"output_type":"stream","name":"stdout","text":["Saving model and tokenizer...\n","Saving model to /content/drive/My Drive/266 Final Project/Our Models/lyrics_model_full...\n","Files saved in /content/drive/My Drive/266 Final Project/Our Models/lyrics_model_full:\n","['config.json', 'generation_config.json', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'spiece.model', 'tokenizer.json']\n"]}]},{"cell_type":"markdown","source":["#Evaluation"],"metadata":{"id":"j6R5syop1Pns"}},{"cell_type":"code","source":["# initialize the saved model run\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","model_path = '/content/drive/My Drive/266 Final Project/Our Models/lyrics_model_full'\n","tokenizer = T5Tokenizer.from_pretrained(model_path)\n","model = T5ForConditionalGeneration.from_pretrained(model_path)\n","\n","print(\"Model and tokenizer loaded successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mT9W6av3WrvY","executionInfo":{"status":"ok","timestamp":1733447482115,"user_tz":480,"elapsed":847,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"a5f53f6c-edd1-4b32-f9c3-cc48e82085c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded successfully!\n"]}]},{"cell_type":"code","source":["# Check that test_df was correctly initialized\n","test_df.head()\n","test_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JpR5LD2Vb-jH","executionInfo":{"status":"ok","timestamp":1733447483759,"user_tz":480,"elapsed":239,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"f8589172-3251-441f-8eec-dc1941b17034"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(638, 7)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["def evaluate_self_supervised_model(\n","   model: T5ForConditionalGeneration,\n","   tokenizer: T5Tokenizer,\n","   test_data: pd.DataFrame,\n","   batch_size: int = 16  # Increased batch size\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","   \"\"\"\n","   Evaluate self-supervised lyrics model with optimized processing\n","   \"\"\"\n","   model.eval()\n","   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","   model = model.to(device)\n","\n","   evaluation_results = {\n","       'content_coverage': [],\n","       'consistency_score': [],\n","       'semantic_similarity': [],\n","       'rouge1_scores': [],\n","       'rouge2_scores': [],\n","       'rougeL_scores': [],\n","       'bert_scores': []\n","   }\n","\n","   examples = []\n","   previous_bert_score = 0.0\n","\n","   for idx in tqdm(range(0, len(test_data), batch_size)):\n","       batch_lyrics = test_data['Lyrics'].iloc[idx:idx + batch_size].tolist()\n","\n","       # Generate multiple summaries for each lyric\n","       summaries_per_lyric = []\n","       for _ in range(2):  # Reduced from 3 to 2 summaries\n","           inputs = tokenizer(\n","               [f\"summarize lyrics: {lyric}\" for lyric in batch_lyrics],\n","               padding=True,\n","               truncation=True,\n","               max_length=512,\n","               return_tensors=\"pt\"\n","           ).to(device)\n","\n","           with torch.no_grad():\n","               outputs = model.generate(\n","                   input_ids=inputs['input_ids'],\n","                   attention_mask=inputs['attention_mask'],\n","                   max_length=100,\n","                   min_length=30,\n","                   num_beams=4,\n","                   do_sample=True,\n","                   temperature=0.3,\n","                   top_k=50,\n","                   no_repeat_ngram_size=3,\n","                   length_penalty=0.8,\n","                   repetition_penalty=1.5\n","               )\n","\n","               decoded_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","               summaries_per_lyric.append(decoded_summaries)\n","\n","       # Evaluate each lyric's summaries\n","       for lyric_idx in range(len(batch_lyrics)):\n","           original_lyric = batch_lyrics[lyric_idx]\n","           lyric_summaries = [summaries[lyric_idx] for summaries in summaries_per_lyric]\n","\n","           # 1. Content Coverage Score\n","           coverage_score = calculate_content_coverage(original_lyric, lyric_summaries[0])\n","           evaluation_results['content_coverage'].append(coverage_score)\n","\n","           # 2. Consistency Score and ROUGE metrics\n","           rouge_scores = calculate_rouge_scores(lyric_summaries)\n","           evaluation_results['consistency_score'].append(np.mean(list(rouge_scores.values())))\n","           evaluation_results['rouge1_scores'].append(rouge_scores['rouge1'])\n","           evaluation_results['rouge2_scores'].append(rouge_scores['rouge2'])\n","           evaluation_results['rougeL_scores'].append(rouge_scores['rougeL'])\n","\n","           # 3. Semantic Similarity\n","           semantic_score = calculate_semantic_similarity(original_lyric, lyric_summaries[0])\n","           evaluation_results['semantic_similarity'].append(semantic_score)\n","\n","           # 4. BERTScore (computed less frequently)\n","           if lyric_idx % 8 == 0:  # Compute BERTScore less frequently\n","               P, R, F1 = score([lyric_summaries[0]], [original_lyric], lang='en', verbose=False)\n","               previous_bert_score = F1.mean().item()\n","               evaluation_results['bert_scores'].append(previous_bert_score)\n","           else:\n","               evaluation_results['bert_scores'].append(previous_bert_score)\n","\n","           # Store examples\n","           if len(examples) < 5:\n","               examples.append({\n","                   'lyrics': original_lyric,\n","                   'generated_summaries': lyric_summaries,\n","                   'metrics': {\n","                       'content_coverage': coverage_score,\n","                       'consistency': np.mean(list(rouge_scores.values())),\n","                       'rouge1': rouge_scores['rouge1'],\n","                       'rouge2': rouge_scores['rouge2'],\n","                       'rougeL': rouge_scores['rougeL'],\n","                       'semantic_similarity': semantic_score,\n","                       'bert_score': previous_bert_score\n","                   }\n","               })\n","\n","       # More frequent memory cleanup\n","       if idx % 5 == 0:\n","           torch.cuda.empty_cache()\n","\n","   # Aggregate results\n","   metrics = {\n","       'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","       'avg_consistency': np.mean(evaluation_results['consistency_score']),\n","       'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","       'avg_rouge1': np.mean(evaluation_results['rouge1_scores']),\n","       'avg_rouge2': np.mean(evaluation_results['rouge2_scores']),\n","       'avg_rougeL': np.mean(evaluation_results['rougeL_scores']),\n","       'avg_bert_score': np.mean(evaluation_results['bert_scores'])\n","   }\n","\n","   return metrics, examples\n","\n","def calculate_rouge_scores(summaries: List[str]) -> Dict[str, float]:\n","   \"\"\"Calculate individual ROUGE scores\"\"\"\n","   rouge_scorer_obj = rouge_scorer.RougeScorer(\n","       ['rouge1', 'rouge2', 'rougeL'],\n","       use_stemmer=True\n","   )\n","\n","   scores = {\n","       'rouge1': [],\n","       'rouge2': [],\n","       'rougeL': []\n","   }\n","\n","   for i in range(len(summaries)):\n","       for j in range(i + 1, len(summaries)):\n","           score = rouge_scorer_obj.score(summaries[i], summaries[j])\n","           scores['rouge1'].append(score['rouge1'].fmeasure)\n","           scores['rouge2'].append(score['rouge2'].fmeasure)\n","           scores['rougeL'].append(score['rougeL'].fmeasure)\n","\n","   return {\n","       'rouge1': np.mean(scores['rouge1']),\n","       'rouge2': np.mean(scores['rouge2']),\n","       'rougeL': np.mean(scores['rougeL'])\n","   }\n","\n","def calculate_content_coverage(lyrics: str, summary: str) -> float:\n","   lyrics_tokens = set(lyrics.lower().split())\n","   summary_tokens = set(summary.lower().split())\n","   overlap = len(lyrics_tokens.intersection(summary_tokens))\n","   coverage = overlap / len(lyrics_tokens)\n","   return coverage\n","\n","def calculate_semantic_similarity(lyrics: str, summary: str) -> float:\n","   lyrics_tokens = set(lyrics.lower().split())\n","   summary_tokens = set(summary.lower().split())\n","   intersection = len(lyrics_tokens.intersection(summary_tokens))\n","   union = len(lyrics_tokens.union(summary_tokens))\n","   return intersection / union if union > 0 else 0.0\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","   print(\"\\nEvaluation Results:\")\n","   print(f\"Average Content Coverage: {metrics['avg_content_coverage']:.3f}\")\n","   print(f\"Average Consistency: {metrics['avg_consistency']:.3f}\")\n","   print(f\"Average Semantic Similarity: {metrics['avg_semantic_similarity']:.3f}\")\n","   print(f\"Average ROUGE-1: {metrics['avg_rouge1']:.3f}\")\n","   print(f\"Average ROUGE-2: {metrics['avg_rouge2']:.3f}\")\n","   print(f\"Average ROUGE-L: {metrics['avg_rougeL']:.3f}\")\n","   print(f\"Average BERTScore: {metrics['avg_bert_score']:.3f}\")\n","\n","   print(\"\\nExample Generations:\")\n","   for i, example in enumerate(examples, 1):\n","       print(f\"\\nExample {i}:\")\n","       print(f\"Original Lyrics (truncated): {example['lyrics'][:200]}...\")\n","       print(\"\\nGenerated Summaries:\")\n","       for j, summary in enumerate(example['generated_summaries'], 1):\n","           print(f\"{j}. {summary}\")\n","       print(\"\\nMetrics:\")\n","       for metric, value in example['metrics'].items():\n","           print(f\"{metric}: {value:.3f}\")"],"metadata":{"id":"GIlliY8ThUak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run evaluation\n","metrics, examples = evaluate_self_supervised_model(\n","    model,\n","    tokenizer,\n","    test_data=test_df,  # Using your held-out test set\n","    batch_size=16\n",")\n","\n","# Print results\n","print_evaluation_results(metrics, examples)\n","\n","# If you want to see just the metrics without examples, you can do:\n","print(\"\\nJust the Metrics:\")\n","for metric_name, value in metrics.items():\n","    print(f\"{metric_name}: {value:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733447982557,"user_tz":480,"elapsed":464217,"user":{"displayName":"Chloe Alexandria McGlynn","userId":"15171080650626567424"}},"outputId":"893ce940-649c-4f3b-8502-a7aebb0e1e72","id":"Vdr3FGaubXNM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/40 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  2%|▎         | 1/40 [00:11<07:34, 11.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/40 [00:23<07:25, 11.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/40 [00:34<07:08, 11.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 10%|█         | 4/40 [00:51<08:08, 13.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 12%|█▎        | 5/40 [01:04<07:50, 13.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 15%|█▌        | 6/40 [01:15<07:01, 12.40s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/40 [01:26<06:36, 12.02s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 20%|██        | 8/40 [01:38<06:22, 11.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 22%|██▎       | 9/40 [01:49<06:06, 11.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 25%|██▌       | 10/40 [02:01<05:53, 11.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 28%|██▊       | 11/40 [02:12<05:32, 11.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 30%|███       | 12/40 [02:22<05:15, 11.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▎      | 13/40 [02:34<05:10, 11.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 35%|███▌      | 14/40 [02:46<04:57, 11.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 38%|███▊      | 15/40 [02:57<04:46, 11.46s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 40%|████      | 16/40 [03:09<04:39, 11.64s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▎     | 17/40 [03:20<04:18, 11.23s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▌     | 18/40 [03:31<04:10, 11.39s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 48%|████▊     | 19/40 [03:43<04:01, 11.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 20/40 [03:55<03:49, 11.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 52%|█████▎    | 21/40 [04:07<03:41, 11.64s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 22/40 [04:19<03:32, 11.78s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 57%|█████▊    | 23/40 [04:30<03:15, 11.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 60%|██████    | 24/40 [04:41<03:03, 11.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 62%|██████▎   | 25/40 [04:52<02:51, 11.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 65%|██████▌   | 26/40 [05:04<02:41, 11.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 27/40 [05:16<02:29, 11.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 70%|███████   | 28/40 [05:26<02:13, 11.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 72%|███████▎  | 29/40 [05:37<02:02, 11.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 75%|███████▌  | 30/40 [05:49<01:55, 11.55s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 78%|███████▊  | 31/40 [06:01<01:44, 11.57s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 80%|████████  | 32/40 [06:13<01:32, 11.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▎ | 33/40 [06:23<01:18, 11.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 85%|████████▌ | 34/40 [06:34<01:07, 11.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 88%|████████▊ | 35/40 [06:46<00:57, 11.42s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 90%|█████████ | 36/40 [06:58<00:46, 11.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▎| 37/40 [07:10<00:34, 11.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▌| 38/40 [07:20<00:22, 11.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 98%|█████████▊| 39/40 [07:32<00:11, 11.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 40/40 [07:43<00:00, 11.59s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results:\n","Average Content Coverage: 0.412\n","Average Consistency: 0.990\n","Average Semantic Similarity: 0.404\n","Average ROUGE-1: 0.991\n","Average ROUGE-2: 0.988\n","Average ROUGE-L: 0.991\n","Average BERTScore: 0.858\n","\n","Example Generations:\n","\n","Example 1:\n","Original Lyrics (truncated): (Someone for me) \n","(Someone for me) \n","I'm here alone on a Friday night \n","Waiting here beside the phone \n","The TV, radio, and me \n","Really ain't been getting along \n","\n","I wish that I could find a way \n","\n","To party ...\n","\n","Generated Summaries:\n","1. I'm here alone on a Friday night Waiting here beside the phone The TV, radio, and me Really ain't been getting along I wish that I could find a way to party to the break of day And there I'd be sure to meet The guy that would be special to me Then momma comes and asks me Why I am dreaming, sitting alone Why not go out and have some fun It's the only way I'\n","2. I'm here alone on a Friday night Waiting here beside the phone The TV, radio, and me Really ain't been getting along I wish that I could find a way to party to the break of day And there I'd be sure to meet The guy that would be special to me Then momma comes and asks me Why I am dreaming, sitting alone Why not go out and have some fun It's the only way I'\n","\n","Metrics:\n","content_coverage: 0.466\n","consistency: 1.000\n","rouge1: 1.000\n","rouge2: 1.000\n","rougeL: 1.000\n","semantic_similarity: 0.462\n","bert_score: 0.857\n","\n","Example 2:\n","Original Lyrics (truncated): A few stolen moments is all that we share\r\n","You've got your family and they need you there. \n","Though I try to resist being last on your list. \n","But no other man's gonna do. \n","So I'm saving all my love for...\n","\n","Generated Summaries:\n","1. I'm saving all my love for you. It's not very easy living alone. My friends try and tell me. Find a man of my own. But each time I try I just break down and cry. Cuz I'd rather be home feeling blue. Love gives you the right to be free. You said, be patient, just wait a little longer.\n","2. I'm saving all my love for you. It's not very easy living alone. My friends try and tell me. Find a man of my own. But each time I try I just break down and cry. Cuz I'd rather be home feeling blue. Love gives you the right to be free. You said, be patient, just wait a little longer.\n","\n","Metrics:\n","content_coverage: 0.445\n","consistency: 1.000\n","rouge1: 1.000\n","rouge2: 1.000\n","rougeL: 1.000\n","semantic_similarity: 0.445\n","bert_score: 0.857\n","\n","Example 3:\n","Original Lyrics (truncated): What it is, ho? Whats up? Every good girl needs a little thug Every block boy needs a little love If you put it down, Ima pick it up, up, up Cant you just see, its just me and you? Panoramic view, tha...\n","\n","Generated Summaries:\n","1. What it is, ho? Whats up? Every good girl needs a little thug Thug Every block boy needs. love If you put it down, Ima pick it up, up, Up Cant you just see, its just me and you? Panoramic view, thats the energy Thats that lemon pepper wing, Im a tenpiece, baby Bathroom fully in the bando He gon make it flip, do it with\n","2. What it is, ho? Whats up? Every good girl needs a little thug Thug Every block boy needs. love If you put it down, Ima pick it up, up, Up Cant you just see, its just me and you? Panoramic view, thats the energy Thats that lemon pepper wing, Im a tenpiece, baby Bathroom fully in the bando He gon make it flip, do it with\n","\n","Metrics:\n","content_coverage: 0.219\n","consistency: 1.000\n","rouge1: 1.000\n","rouge2: 1.000\n","rougeL: 1.000\n","semantic_similarity: 0.218\n","bert_score: 0.857\n","\n","Example 4:\n","Original Lyrics (truncated): Im good, yeah, Im feelin alright Baby, Ima have the best fuckin night of my life And wherever it takes me, Im down for the ride Baby, dont you know Im good, yeah, Im feelin alright Cause Im good, yeah...\n","\n","Generated Summaries:\n","1. Im good, yeah, Im feelin alright Baby, Ima have the best fuckin night of my life And wherever it takes me, Im down for the ride Baby, dont you know Im good\n","2. Im good, yeah, Im feelin alright Baby, Ima have the best fuckin night of my life And wherever it takes me, Im down for the ride Baby, dont you know Im good\n","\n","Metrics:\n","content_coverage: 0.338\n","consistency: 1.000\n","rouge1: 1.000\n","rouge2: 1.000\n","rougeL: 1.000\n","semantic_similarity: 0.338\n","bert_score: 0.857\n","\n","Example 5:\n","Original Lyrics (truncated): Going far, getting nowhere\r\n","Going far, the way you are\r\n","Going far, getting nowhere\r\n","Going far, the way you are\n","\n","Going far, getting nowhere\n","\n","The way you are\n","\n","\n","\n","Going far, getting nowhere\n","\n","The way you a...\n","\n","Generated Summaries:\n","1. going far, getting nowhere Going far, the way you are Going far and getting nowhere The way you're Going far. getting nowhere.\n","2. going far, getting nowhere Going far, the way you are Going far and getting nowhere The way you're Going far. getting nowhere.\n","\n","Metrics:\n","content_coverage: 0.188\n","consistency: 1.000\n","rouge1: 1.000\n","rouge2: 1.000\n","rougeL: 1.000\n","semantic_similarity: 0.173\n","bert_score: 0.857\n","\n","Just the Metrics:\n","avg_content_coverage: 0.412\n","avg_consistency: 0.990\n","avg_semantic_similarity: 0.404\n","avg_rouge1: 0.991\n","avg_rouge2: 0.988\n","avg_rougeL: 0.991\n","avg_bert_score: 0.858\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}