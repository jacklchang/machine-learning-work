{"cells":[{"cell_type":"markdown","source":["# T5-Small trained on the following data\n","\n","\n","1.   Genius Annotations\n","2.   Lyrics\n","3.   PoemSum Data\n","\n"],"metadata":{"id":"84Gov5qq1mXM"}},{"cell_type":"markdown","source":["# Install Required Dependencies"],"metadata":{"id":"FMCs0wZxnLus"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11188,"status":"ok","timestamp":1733641353705,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"EMq71VljYUei","outputId":"2aab5c05-11ee-4136-a44e-e82e8997b861"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n","Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"]}],"source":["# import needed dependencies for testing PoemSum model\n","!pip install pytorch-lightning transformers torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRJhMcgtYZTl"},"outputs":[],"source":["# Import needed dependencies while avoiding conflicts\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import (\n","    T5ForConditionalGeneration,\n","    T5TokenizerFast as T5Tokenizer,\n","    AdamW\n",")\n","import re\n","import os\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59JdfwDxijbV"},"outputs":[],"source":["import textwrap\n","def print_summary(text, width=70):\n","    print(textwrap.fill(text, width=width))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31969,"status":"ok","timestamp":1733641536513,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"N5CE2vZ_YmJn","outputId":"e9845614-7c5a-4553-fcf5-0d44bd745259"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hcct8nQQby2J"},"source":["# Class Modules\n","This step prepares data, training, and validating a text-to-text Transformer model for summarizing song lyrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCBObUD_YcPf"},"outputs":[],"source":["# Custom Dataset class from PoemSum model\n","class LyricsSummaryDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset for handling lyrics and their summaries.\n","\n","    Parameters:\n","    - data (pd.DataFrame): The dataset containing text and summary columns.\n","    - tokenizer (T5Tokenizer): The tokenizer to preprocess text and summary.\n","    - text_max_token_len (int): Maximum token length for text inputs.\n","    - summary_max_token_len (int): Maximum token length for summary inputs.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        data: pd.DataFrame,\n","        tokenizer: T5Tokenizer,\n","        text_max_token_len: int = 1024,\n","        summary_max_token_len: int = 256\n","    ):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.text_max_token_len = text_max_token_len\n","        self.summary_max_token_len = summary_max_token_len\n","\n","    def __len__(self):\n","        \"\"\"Returns the total number of rows in the dataset.\"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, index: int):\n","        \"\"\"\n","        Retrieves a single data row and preprocesses it for the model.\n","\n","        Parameters:\n","        - index (int): The index of the row to retrieve.\n","\n","        Returns:\n","        - dict: A dictionary containing preprocessed inputs and labels.\n","        \"\"\"\n","        data_row = self.data.iloc[index]\n","\n","        # Tokenize the text input\n","        text_encoding = self.tokenizer(\n","            data_row[\"text\"],\n","            max_length=self.text_max_token_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Tokenize the summary input\n","        summary_encoding = self.tokenizer(\n","            data_row[\"summary\"],\n","            max_length=self.summary_max_token_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Replace padding token IDs (0) in labels with -100 for loss calculation\n","        labels = summary_encoding[\"input_ids\"]\n","        labels[labels == 0] = -100\n","\n","        return dict(\n","            text=data_row[\"text\"],\n","            summary=data_row[\"summary\"],\n","            text_input_ids=text_encoding[\"input_ids\"].flatten(),\n","            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n","            labels=labels.flatten(),\n","            labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n","        )\n","\n","# Lightning Data Module from PoemSum\n","class LyricsSummaryDataModule(pl.LightningDataModule):\n","    \"\"\"\n","    LightningDataModule for preparing data loaders for training and validation.\n","\n","    Parameters:\n","    - train_df (pd.DataFrame): DataFrame for the training dataset.\n","    - val_df (pd.DataFrame): DataFrame for the validation dataset.\n","    - tokenizer (T5Tokenizer): The tokenizer for preprocessing.\n","    - batch_size (int): Number of samples per batch.\n","    - text_max_token_len (int): Maximum token length for text inputs.\n","    - summary_max_token_len (int): Maximum token length for summary inputs.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        train_df: pd.DataFrame,\n","        val_df: pd.DataFrame,\n","        tokenizer: T5Tokenizer,\n","        batch_size: int = 8,\n","        text_max_token_len: int = 512,\n","        summary_max_token_len: int = 256\n","    ):\n","        super().__init__()\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.text_max_token_len = text_max_token_len\n","        self.summary_max_token_len = summary_max_token_len\n","\n","    def setup(self, stage=None):\n","        \"\"\"\n","        Initializes the training and validation datasets.\n","\n","        Parameters:\n","        - stage (str): Stage of data preparation (unused in this case).\n","        \"\"\"\n","        self.train_dataset = LyricsSummaryDataset(\n","            self.train_df,\n","            self.tokenizer,\n","            self.text_max_token_len,\n","            self.summary_max_token_len\n","        )\n","        self.val_dataset = LyricsSummaryDataset(\n","            self.val_df,\n","            self.tokenizer,\n","            self.text_max_token_len,\n","            self.summary_max_token_len\n","        )\n","\n","    def train_dataloader(self):\n","        \"\"\"Creates a DataLoader for the training dataset.\"\"\"\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=2\n","        )\n","\n","    def val_dataloader(self):\n","        \"\"\"Creates a DataLoader for the validation dataset.\"\"\"\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=2\n","        )\n","\n","# Model Class\n","class LyricsSummaryModel(pl.LightningModule):\n","    \"\"\"\n","    PyTorch Lightning model for fine-tuning T5 for text summarization.\n","\n","    Parameters:\n","    - model_name (str): Pretrained model name or path from Hugging Face.\n","    \"\"\"\n","    def __init__(self, model_name='t5-small'):\n","        super().__init__()\n","        # Use T5ForConditionalGeneration, which has the required generation capabilities\n","        self.model = T5ForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n","\n","    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n","        \"\"\"\n","        Forward pass through the model.\n","\n","        Parameters:\n","        - input_ids (torch.Tensor): Tokenized input IDs.\n","        - attention_mask (torch.Tensor): Attention mask for input.\n","        - decoder_attention_mask (torch.Tensor): Attention mask for decoder input.\n","        - labels (torch.Tensor): Tokenized labels (optional).\n","\n","        Returns:\n","        - tuple: Loss and logits from the model output.\n","        \"\"\"\n","        output = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            decoder_attention_mask=decoder_attention_mask\n","        )\n","        return output.loss, output.logits\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"\n","        Performs a training step.\n","\n","        Parameters:\n","        - batch (dict): Batch of data containing inputs and labels.\n","        - batch_idx (int): Index of the batch.\n","\n","        Returns:\n","        - torch.Tensor: Training loss.\n","        \"\"\"\n","        loss, outputs = self(\n","            batch[\"text_input_ids\"],\n","            batch[\"text_attention_mask\"],\n","            batch[\"labels_attention_mask\"],\n","            batch[\"labels\"]\n","        )\n","        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"\n","        Performs a validation step.\n","\n","        Parameters:\n","        - batch (dict): Batch of data containing inputs and labels.\n","        - batch_idx (int): Index of the batch.\n","\n","        Returns:\n","        - torch.Tensor: Validation loss.\n","        \"\"\"\n","        loss, outputs = self(\n","            batch[\"text_input_ids\"],\n","            batch[\"text_attention_mask\"],\n","            batch[\"labels_attention_mask\"],\n","            batch[\"labels\"]\n","        )\n","        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        \"\"\"\n","        Configures the optimizer for the model.\n","\n","        Returns:\n","        - torch.optim.Optimizer: AdamW optimizer with a predefined learning rate.\n","        \"\"\"\n","        return AdamW(self.parameters(), lr=0.00001)\n"]},{"cell_type":"markdown","metadata":{"id":"dq9Fv1GtblFQ"},"source":["# Data Preparation\n","\n","* Updated token lengths to handle combined data better\n","* More robust data cleaning and filtering\n","* More explicit handling of multiple data sources\n","* Added early stopping to training\n","* Updated save path for combined model\n","* Better error handling throughout\n","\n","We don't include the poem test set because we're only interested in using the poem data (train and validation sets) for training the model to learn interpretive summarization patterns. The test set will only contain lyrics data since our end goal is to evaluate how well the model can summarize and interpret song lyrics.\n","This is different from Model 3 (Lyrics + Poem) where we loaded all poem data initially but then only used train/valid splits. In this combined model, we're being more explicit about:\n","\n","Using poem train/valid data only for training purposes\n","Using lyrics test set for final evaluation\n","Keeping our evaluation focused on the model's performance on lyrics, which is our target domain"]},{"cell_type":"code","source":["# Load lyrics data\n","df_list = []\n","folder_path = \"/content/drive/My Drive/266 Final Project/Cleaned Song Files\"\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.csv'):\n","        file_path = os.path.join(folder_path, filename)\n","        df = pd.read_csv(file_path)\n","        df_list.append(df)\n","\n","# Concatenate lyrics data\n","lyrics_df = pd.concat(df_list, ignore_index=True)"],"metadata":{"id":"BzwssnIMo5yj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load poem data keeping original splits\n","poem_train = pd.read_csv(\"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_train.csv\")\n","poem_valid = pd.read_csv(\"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_valid.csv\")\n","\n","# Basic data cleaning\n","def clean_text(text):\n","    if pd.isna(text):\n","        return \"\"\n","    text = str(text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text.strip()"],"metadata":{"id":"kybN8BcypyBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split lyrics data\n","train_lyrics, test_lyrics = train_test_split(lyrics_df, test_size=0.2, random_state=42)\n","train_lyrics, val_lyrics = train_test_split(train_lyrics, test_size=0.1, random_state=42)"],"metadata":{"id":"aRpDufuLqGPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clean and filter training data\n","train_lyrics_filtered = train_lyrics[\n","    (train_lyrics['Lyrics'].notna()) &\n","    (train_lyrics['Combined Annotations'].notna())\n","]\n","poem_train_filtered = poem_train[\n","    (poem_train['ctext'].notna()) &\n","    (poem_train['text'].notna()) &\n","    (poem_train['text'].str.strip() != '')\n","]\n","\n","# Clean and filter validation data\n","val_lyrics_filtered = val_lyrics[\n","    (val_lyrics['Lyrics'].notna()) &\n","    (val_lyrics['Combined Annotations'].notna())\n","]\n","poem_valid_filtered = poem_valid[\n","    (poem_valid['ctext'].notna()) &\n","    (poem_valid['text'].notna()) &\n","    (poem_valid['text'].str.strip() != '')\n","]"],"metadata":{"id":"BT4lpCXLqJYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Format training data\n","train_data = pd.DataFrame({\n","    'text': [\n","        *[f\"summarize lyrics and capture meaning: {clean_text(text)}\" for text in train_lyrics_filtered['Lyrics']],\n","        *[f\"summarize poem and capture meaning: {clean_text(text)}\" for text in poem_train_filtered['ctext']]\n","    ],\n","    'summary': [\n","        *[clean_text(text) for text in train_lyrics_filtered['Combined Annotations']],\n","        *[clean_text(text) for text in poem_train_filtered['text']]\n","    ]\n","})"],"metadata":{"id":"0DAzMr5vozvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Format validation data\n","val_data = pd.DataFrame({\n","    'text': [\n","        *[f\"summarize lyrics and capture meaning: {clean_text(text)}\" for text in val_lyrics_filtered['Lyrics']],\n","        *[f\"summarize poem and capture meaning: {clean_text(text)}\" for text in poem_valid_filtered['ctext']]\n","    ],\n","    'summary': [\n","        *[clean_text(text) for text in val_lyrics_filtered['Combined Annotations']],\n","        *[clean_text(text) for text in poem_valid_filtered['text']]\n","    ]\n","})\n","\n","# Ensure data types are strings\n","train_data['text'] = train_data['text'].astype(str)\n","train_data['summary'] = train_data['summary'].astype(str)\n","val_data['text'] = val_data['text'].astype(str)\n","val_data['summary'] = val_data['summary'].astype(str)"],"metadata":{"id":"bGX-rq5Xtipe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Old data preparation code"],"metadata":{"id":"jGh0woF1ovCY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"43Y7qGqUFhgx"},"outputs":[],"source":["# Train on all artists\n","\n","# Initialize an empty list to store DataFrames\n","df_list = []\n","\n","folder_path = \"/content/drive/My Drive/266 Final Project/Song Files\"\n","# Iterate through each file in the directory\n","for filename in os.listdir(folder_path):\n","    # Check if the file is a CSV file\n","    if filename.endswith('.csv'):\n","        # Construct the full file path\n","        file_path = os.path.join(folder_path, filename)\n","        # Read the CSV file and append it to the list\n","        df = pd.read_csv(file_path)\n","        df_list.append(df)\n","\n","# Concatenate all DataFrames in the list into a single DataFrame\n","df = pd.concat(df_list, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pK5rsGgDYq1l"},"outputs":[],"source":["# # be wary of extent of cleaning on text\n","\n","# def clean_text(text):\n","#     # Replace multiple spaces with a single space\n","#     text = re.sub(r'\\s+', ' ', text)\n","#     # Remove incomplete sentences\n","#     text = re.sub(r'(\\s*—\\s*)', '', text)\n","#     text = re.sub(r'\\s+[,\\.]', '', text)\n","#     text = re.sub(r'[\"\"]', '\"', text)\n","#     text = re.sub(r'[^\\w\\s.,!?]', '', text)\n","#     text = re.sub(r'\\s{2,}', ' ', text)\n","#     text = re.sub(r'\\s{2,}', '[missing]', text)\n","#     return text.strip()\n","\n","\n","# print(\"Original number of songs\", len(df))\n","\n","# # If there are no wiki annotations let's drop them\n","# df = df[df['Wikipedia Annotation'] != \"No Wikipedia annotation found (artist name not mentioned)\"]\n","\n","# print(\"After dropping no annotations\", len(df))\n","\n","# # Let's clean up the text a bit\n","# df['Lyrics'] = df['Lyrics'].apply(clean_text)\n","# df['generated_annotation'] = df['generated_annotation'].apply(clean_text)\n","\n","# # Calculate average string length of the column\n","# # average_length = df['Wikipedia Annotation'].str.len().mean()\n","# # min_length = df['Wikipedia Annotation'].str.len().min()\n","# # max_length = df['Wikipedia Annotation'].str.len().max()\n","\n","# # Display the result\n","# # print(\"Average string length of wiki:\", average_length)\n","# # print(\"Min string length of wiki:\", min_length)\n","# # print(\"Max string length of wiki:\", max_length)\n","\n","# # Keep only the columns we need\n","# # df = df[['Title', 'Lyrics', 'Wikipedia Annotation', 'Combined Annotations']]\n","\n","# print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1733432217583,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"_yM9fTn10qpw","outputId":"684f6836-340b-43a9-c49e-0fa04f901ffa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average string length of lyrics: 1893.4503688799464\n","Min string length of lyrics: 12\n","Max string length of lyrics: 6423\n"]}],"source":["# Calculate average string length of the column\n","average_length = df['Lyrics'].str.len().mean()\n","min_length = df['Lyrics'].str.len().min()\n","max_length = df['Lyrics'].str.len().max()\n","\n","# Display the result\n","print(\"Average string length of lyrics:\", average_length)\n","print(\"Min string length of lyrics:\", min_length)\n","print(\"Max string length of lyrics:\", max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vS9C3k10zlPl"},"outputs":[],"source":["poem_list = []\n","poem_test = pd.read_csv(\"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_test.csv\")\n","poem_list.append(poem_test)\n","poem_train = pd.read_csv(\"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_train.csv\")\n","poem_list.append(poem_train)\n","poem_valid = pd.read_csv(\"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_valid.csv\")\n","poem_list.append(poem_valid)\n","\n","# Concatenate all DataFrames in the list into a single DataFrame\n","poem_data = pd.concat(poem_list, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6T0IUWCqnOt"},"outputs":[],"source":["# Do this before calling create_baseline_model\n","train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","#do it for poetry data\n","train_val_poem, test_poem = train_test_split(poem_data, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"zhoIdvxjbrz8"},"source":["# Create and Train t5 model"]},{"cell_type":"markdown","source":[],"metadata":{"id":"UzDnUtPlsfkL"}},{"cell_type":"code","source":["def create_baseline_model(train_data, val_data, save_dir=\"checkpoints\"):\n","    \"\"\"Create and train model with combined data sources\"\"\"\n","    print(\"Initializing model and tokenizer...\")\n","    MODEL_NAME = 't5-small'\n","    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","    model = LyricsSummaryModel(MODEL_NAME)\n","\n","    print(\"Setting up data module...\")\n","    data_module = LyricsSummaryDataModule(\n","        train_df=train_data,\n","        val_df=val_data,\n","        tokenizer=tokenizer,\n","        batch_size=4,\n","        text_max_token_len=1024,\n","        summary_max_token_len=256\n","    )\n","\n","    print(\"Configuring trainer...\")\n","    trainer = pl.Trainer(\n","        max_epochs=3,\n","        accumulate_grad_batches=4,\n","        gradient_clip_val=1.0,\n","        precision=16 if torch.cuda.is_available() else 32,\n","        enable_checkpointing=True,\n","        default_root_dir=save_dir,\n","        callbacks=[\n","            pl.callbacks.EarlyStopping(\n","                monitor='val_loss',\n","                patience=3,\n","                mode='min'\n","            )\n","        ]\n","    )\n","\n","    print(\"Starting training...\")\n","    trainer.fit(model, data_module)\n","\n","    print(\"Saving model and tokenizer...\")\n","    drive_path = '/content/drive/My Drive/266 Final Project/Our Models/Combined Data'\n","    os.makedirs(drive_path, exist_ok=True)\n","    try:\n","        model.model.save_pretrained(drive_path)\n","        tokenizer.save_pretrained(drive_path)\n","        print(f\"Model and tokenizer successfully saved to {drive_path}\")\n","    except Exception as e:\n","        print(f\"Failed to save model and tokenizer: {e}\")\n","\n","    return model, tokenizer, trainer"],"metadata":{"id":"tcEqn0zzsN_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, tokenizer, trainer = create_baseline_model(train_data, val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533,"referenced_widgets":["81b99cb9486e47d2b8d46d69f0168957","cc6cfe2d1a9c4f3385bdd8f6aa8e7388","7553a7fed094460e87d8100f553dc99b","51ce57c11c264d0e96121eb2521e1be8","8f767507fbce4742a9a256bdd209ae25","bc713056af4a4353bbed3a7e9d6fa4a5","5d05a98afe3f4c7092061c3edf6cc5b9","aa555b44d92a4c47921e73ba14c01eac","55902086704f4af98d0ca7166772582d","e956e9e659bc47f0a6b0e0874933e483","41b5e62784a1460f90dff82364320f10","8ede9f856e4147309c06853bdd522122","5fa4941ac6064416ae538e7337be96f0","e2cacdf1e2564dcf953e2eb81b91da9e","fceac39341374e7685dd55afc829e90f","00fb315e0f474a90a93c1ab3465eaffd","740e6ef305e74ef89cb1af3e2d83106b","6adfe37a597e4075af11d391e4b933d9","4a02ce4aef3945648190d83ef374c6ef","04337c2ef44847c09c7e5a3351b1726b","191e251105bc42e59a85754e3c8f5dcf","e01348e4464a4184b8d9b7a903ea3c9d","5de8c30f91f947b2bb0a5a4a4a54ebd2","74b0db8b7cb9467f9369c511f4d24c87","abf04a00355b41eab426e1c133329046","02590ca280d144988f4943000179af2d","b31c662ee4d34fbf9162048a90cedaa7","fc0e7b3017484db295a8ef16473798e0","f9c2e1c574684b4c8809cc3bd1cdeac3","82336a5912d1444e9a332fb25d641d1f","dc34902cfb4945119369a2997560a67a","52d6084aeae846c3b14c285435a442c3","1839082a8a7f48419837d89ccbcb3435","59ee72774c7d40f79b96b474366b345f","52e63309627f49c6af4ed56e30d52236","57d80fc928e340c6ba3d6d93cbf0f090","0248b07362974dfdb1d1015131dc6b84","b170aaf5af4049a390cc186444b0d245","b9e97fa3ab424ee5ad50ac2c316d4c94","7b2c2a16ad8e420ca26f0691bc726892","cf06dd7a72aa4c2f8792672a5dc02c17","637ef16ab5d24ebd94297331861b2c8c","0e51c89c9bbc42df9ebb0aeb1d178ad5","3b1b229b6d6d4541ac578e2dc440bc15","e915324061c547a8ab5b150865aa908b","710779ecd118438bb8b10d26980f5731","965520ca605642c5a7fea06c7bf8eecd","5b86b6a1d58a43848999dee2b7946d24","f23855cc39814d3cb78345905ce994b9","f01406253f824f298ec74c20dc877ded","a46e6a699a314cf293f3c4ff443c6082","1ba8e4d5928d4a53b6a7865174907eea","4214294f538f49f18dcf23b5a2425bde","670c13365a3044a8818c0f0b645b2103","09bfdb93a366480881f016b550a61ab1"]},"id":"WuQmRJ-zsvqF","executionInfo":{"status":"ok","timestamp":1733502253834,"user_tz":480,"elapsed":878990,"user":{"displayName":"Chloe M","userId":"03955309346947875383"}},"outputId":"10047d76-b3fc-4907-bda8-90f01040b308"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing model and tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                       | Params | Mode\n","------------------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M | eval\n","------------------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n","0         Modules in train mode\n","277       Modules in eval mode\n"]},{"output_type":"stream","name":"stdout","text":["Setting up data module...\n","Configuring trainer...\n","Starting training...\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b99cb9486e47d2b8d46d69f0168957"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ede9f856e4147309c06853bdd522122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de8c30f91f947b2bb0a5a4a4a54ebd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ee72774c7d40f79b96b474366b345f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e915324061c547a8ab5b150865aa908b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"]},{"output_type":"stream","name":"stdout","text":["Saving model and tokenizer...\n","Model and tokenizer successfully saved to /content/drive/My Drive/266 Final Project/Our Models/Combined Data\n"]}]},{"cell_type":"markdown","source":["## (SKIP) Old approach to baseline model (before data pre-processing changes)"],"metadata":{"id":"fsImZjvdsOdx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELuENYcubWRr"},"outputs":[],"source":["def create_baseline_model(df, poem_df, save_dir=\"checkpoints\"):\n","    # 1. Initialize model and tokenizer\n","    print(\"Initializing model and tokenizer...\")\n","    MODEL_NAME = 't5-small'\n","    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","    model = LyricsSummaryModel(MODEL_NAME)\n","\n","    # 2. Prepare song lyrics data\n","    print(\"Preparing data...\")\n","\n","    # Handle missing or invalid data in Combined Annotations column\n","    df['Combined Annotations'] = df['Combined Annotations'].astype(str)\n","    df['Combined Annotations'] = df['Combined Annotations'].fillna('')\n","\n","    lyrics_data = pd.DataFrame({\n","        'text': train_val_df.apply(\n","            lambda x: f\"summarize lyrics and capture meaning: {x['Lyrics']}\",\n","            axis=1\n","        ),\n","        'summary': train_val_df['Combined Annotations'].apply(\n","            lambda x: f\"Meaning and themes: {' '.join(x.split()[:100])}\"\n","        )\n","    })\n","\n","    # 3. Prepare poem data\n","    poem_data = pd.DataFrame({\n","        'text': train_val_poem.apply(\n","            lambda x: f\"summarize poem and capture meaning: {x['ctext']}\",\n","            axis=1\n","        ),\n","        'summary': train_val_poem['text'].apply(\n","            lambda x: f\"Meaning and themes: {x}\"\n","        )\n","    })\n","\n","    # 4. Combine lyrics and poem data\n","    combined_data = pd.concat([lyrics_data, poem_data], ignore_index=True)\n","\n","    # Clean combined data\n","    combined_data['text'] = combined_data['text'].astype(str)\n","    combined_data['summary'] = combined_data['summary'].astype(str)\n","\n","    # Remove any rows with empty text or summaries\n","    combined_data = combined_data[combined_data['text'].str.len() > 10]\n","    combined_data = combined_data[combined_data['summary'].str.len() > 10]\n","\n","    # Shuffle the combined dataset\n","    combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # 5. Split data\n","    train_size = int(0.8 * len(combined_data))\n","    train_data = combined_data[:train_size]\n","    val_data = combined_data[train_size:]\n","\n","    # Print dataset statistics\n","    print(f\"\\nDataset Statistics:\")\n","    print(f\"Total samples: {len(combined_data)}\")\n","    print(f\"Training samples: {len(train_data)}\")\n","    print(f\"Validation samples: {len(val_data)}\")\n","\n","    # 6. Set up data module\n","    data_module = LyricsSummaryDataModule(\n","        train_df=train_data,\n","        val_df=val_data,\n","        tokenizer=tokenizer,\n","        batch_size=2\n","    )\n","\n","    # 7. Set up trainer\n","    trainer = pl.Trainer(\n","        max_epochs=1,\n","        accumulate_grad_batches=2,\n","        gradient_clip_val=1.0,\n","        precision=16 if torch.cuda.is_available() else 32,\n","        enable_checkpointing=True,\n","        default_root_dir=save_dir\n","    )\n","\n","    # 8. Train model\n","    print(\"Starting training...\")\n","    trainer.fit(model, data_module)\n","\n","    # 9. Save model and tokenizer\n","    print(\"Saving model and tokenizer...\")\n","    drive_path = '/content/drive/MyDrive/266 Final Project/Our Models/All Data'\n","    os.makedirs(drive_path, exist_ok=True)\n","\n","    try:\n","        model.model.save_pretrained(drive_path)\n","        tokenizer.save_pretrained(drive_path)\n","        print(f\"Model and tokenizer successfully saved to {drive_path}\")\n","    except Exception as e:\n","        print(f\"Failed to save model and tokenizer: {e}\")\n","\n","    return model, tokenizer, trainer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":929,"referenced_widgets":["f1deaa9dd3df4992a06d59e0befce881","d0b24ebc7c31483bb8d1420e9e5ddf4d","982d6fb31d4345668f1f1c00f32c2edb","13de83c6065748e28b804279a076ff95","ad7341e003e84a49911e68a753e9a0ea","c737e24f09ac4837a13f0e010a65272d","4252c4af33fd4aa2bd528c9bbb4b6e6d","e3cc5bc5a1a340c3b38952823566b75a","67a424d65b90482bb4318cd7fe176e7c","1228160cecba4967857b8977d53a407d","71b8946eff0f4e768747765a60dccac0"]},"executionInfo":{"elapsed":9346,"status":"error","timestamp":1733437366996,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"hGgMHYFqz6Rg","outputId":"5e6a015d-aaa7-4682-92ee-231b71658ec8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing model and tokenizer...\n","Preparing data...\n","\n","Dataset Statistics:\n","Total samples: 4793\n","Training samples: 3834\n","Validation samples: 959\n"]},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["Starting training...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                       | Params | Mode\n","------------------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M | eval\n","------------------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n","0         Modules in train mode\n","277       Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1deaa9dd3df4992a06d59e0befce881","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","INFO:pytorch_lightning.utilities.rank_zero:\n","Detected KeyboardInterrupt, attempting graceful shutdown ...\n"]},{"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    395\u001b[0m         )\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-b590ff74d35c>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         loss, outputs = self(\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-b590ff74d35c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_attention_mask, labels)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         output = self.model(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1855\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-1ac5fe9d3234>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_baseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_val_poem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-14d194d69b73>\u001b[0m in \u001b[0;36mcreate_baseline_model\u001b[0;34m(df, poem_df, save_dir)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# 8. Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# 9. Save model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"]}],"source":["model, tokenizer, trainer = create_baseline_model(train_val_df, train_val_poem)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvRCYWItio-Z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1733437378740,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"KutMMSKj6rTd","outputId":"c86c0987-f9d9-4a75-f2a1-9374b35683cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"]}],"source":["print(type(model.model))\n","# Output should be <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"]},{"cell_type":"markdown","metadata":{"id":"lrI1W_JCdZrX"},"source":[]},{"cell_type":"markdown","metadata":{"id":"fS73Kv92bvWm"},"source":["# (SKIP) Generate Song Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBJel-CpZ0cE"},"outputs":[],"source":["def generate_song_summary(model, tokenizer, data, song_index, max_length=150):\n","    \"\"\"Generate a summary for a single song\"\"\"\n","\n","    #training input format\n","    input_text = f\"summarize lyrics and capture meaning: {data.iloc[song_index]['Lyrics']}\"\n","\n","\n","    # Encode the text\n","    inputs = tokenizer.encode(\n","        input_text,\n","        max_length=5000,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Generate summary\n","    # The 'generate' method should be called directly on the 'model' object\n","    summary_ids = model.generate( # Removed 'model.' before generate\n","        inputs,\n","        max_length=300,\n","        min_length=100,\n","        num_beams=5,\n","        #temperature=0.9,\n","        length_penalty=0.5,\n","        early_stopping=True,\n","        no_repeat_ngram_size=2\n","    )\n","\n","    # Decode summary\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary"]},{"cell_type":"markdown","metadata":{"id":"bYo9TJbjP-Vt"},"source":["- Version with lyrics only\n","- How much does it improve when we add Wiki data?\n","- Try experimenting with .generate parameters\n","- Three approaches\n","  - Lyrics\n","  - Lyrics + Wiki\n","\n","- Goal involves summarization and commentary\n","  - need label data to reflect goal\n","  - challenge is data formatting in available label data\n","  - does using Wiki improve output\n","  - real data is slightly different from our goal\n","  - the model will train on this data, so won't learn our exact goal\n","  - framing challenge\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127816,"status":"ok","timestamp":1733432365581,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"G_G_ZC4-Zfv6","outputId":"2994e8b3-2981-4f54-e65b-ffcf38cc5549"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bahamas Promises\n","Meaning and themes: “Dods, Man Yeah, For All The Dogs” is a song that\n","focuses on the theme theme of the song. The song was titled “Fast\n","Pinky promises” by the singer-songwriter. It was released on November\n","1, 2015, and has been re-released on October 1, 2015. The track is now\n","available on iTunes and is available for download in the iTunes Store.\n","This is the first track to be released.\n"]}],"source":["# Usage example:\n","\"\"\"\n","generate_song_summary(model, tokenizer, df, song_index=0)\n","\"\"\"\n","\n","summary = generate_song_summary(model, tokenizer, df, song_index=1)\n","\n","print(df.iloc[1]['Title'])\n","print_summary(summary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733432365581,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"0K2ZkEN8BxRi","outputId":"df867988-c063-48c2-f5bb-404ae768be84"},"outputs":[{"name":"stdout","output_type":"stream","text":["This song unfolds as a poignant reflection on a troubled relationship.\n","The song’s title immediately conjures images of unfulfilled dreams and\n","broken commitments. As the track progresses, it becomes evident that\n","Drake, is addressing a woman named Hailey.  The song’s opening lines,\n","“Hailey, it’s sad that I know all the tea,” set a tone of\n","disappointment and disillusionment. Drake references “broken pinky\n","promises” and recounts how a trip to the Bahamas was marred by\n","Hailey’s actions. There’s a palpable sense of betrayal and a\n","realization that the relationship isn’t working.  Throughout the song,\n","the theme of disappointment and broken trust resurfaces. Drake\n","expresses weariness with Hailey’s apologies, indicating that he’s\n","reached a breaking point. The wordplay involving “No” in monogamy\n","suggests that the relationship has been marked by infidelity or a lack\n","of commitment.  As the song unfolds, the emotional weight becomes more\n","pronounced. Drake laments that Hailey lives in his mind, rent-free,\n","despite seemingly not reciprocating the same level of affection.\n","There’s a paradoxical feeling of being indispensable yet unmissed.\n","The song’s outro, with its repetition of “Dogs, man” and the album’s\n","title, “For All The Dogs,” hints at themes of loyalty and\n","companionship. It’s a poignant reminder that even in the face of\n","disappointment and heartache, there’s a longing for genuine connection\n","and loyalty.  This song offers a glimpse into the complexities of\n","human relationships, where promises are broken, trust is shattered,\n","and emotional scars linger. It’s a song that resonates with anyone who\n","has grappled with the aftermath of a failed love affair, a testament\n","to Drake’s ability to capture the raw emotions of his experiences in\n","his music.\n"]}],"source":["print_summary(df.iloc[1]['Combined Annotations'])\n"]},{"cell_type":"markdown","metadata":{"id":"j6R5syop1Pns"},"source":["#Evaluation"]},{"cell_type":"code","source":["# Check if model path exists and contents\n","import os\n","print(f\"Model path exists: {os.path.exists(model_path)}\")\n","print(\"Contents:\", os.listdir('/content/drive/My Drive/266 Final Project/Our Models/'))\n","\n","# Check test_lyrics structure\n","print(\"\\nTest data info:\")\n","print(test_lyrics.info())\n","\n","# Clean test_lyrics if needed\n","test_lyrics = test_lyrics.dropna(subset=['Lyrics', 'Combined Annotations'])\n","test_lyrics['Lyrics'] = test_lyrics['Lyrics'].astype(str)\n","test_lyrics['Combined Annotations'] = test_lyrics['Combined Annotations'].astype(str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymnro2zq5uL8","executionInfo":{"status":"ok","timestamp":1733641614943,"user_tz":480,"elapsed":712,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"e06cdb02-beb9-4790-d538-d1be55210c34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model path exists: False\n","Contents: ['lyrics_model_full', 'Lyrics + Genius', 'BART_All_Data', 'Lyrics + Poem Data', '.ipynb_checkpoints', 'Pegasus [OLD]', 'Combined Data (T5 All Data)', 'Archive', 'Pegasus', 'T5 Fine Tuned Model', 'BART Fine Tuned']\n","\n","Test data info:\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 638 entries, 1029 to 1356\n","Data columns (total 7 columns):\n"," #   Column                Non-Null Count  Dtype \n","---  ------                --------------  ----- \n"," 0   Song ID               635 non-null    object\n"," 1   Title                 629 non-null    object\n"," 2   Lyrics URL            626 non-null    object\n"," 3   Combined Annotations  629 non-null    object\n"," 4   Wikipedia Annotation  629 non-null    object\n"," 5   Lyrics                629 non-null    object\n"," 6   generated_annotation  3 non-null      object\n","dtypes: object(7)\n","memory usage: 39.9+ KB\n","None\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-16-47fa3127a3e0>:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_lyrics['Lyrics'] = test_lyrics['Lyrics'].astype(str)\n","<ipython-input-16-47fa3127a3e0>:13: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_lyrics['Combined Annotations'] = test_lyrics['Combined Annotations'].astype(str)\n"]}]},{"cell_type":"code","source":["# Load saved model\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","model_path = '/content/drive/MyDrive/266 Final Project/Our Models/Combined Data (T5 All Data)'\n","tokenizer = T5Tokenizer.from_pretrained(model_path)\n","model = T5ForConditionalGeneration.from_pretrained(model_path)\n","\n","print(\"Model and tokenizer loaded successfully!\")\n","\n","# We already have our test_lyrics from data preparation\n","print(f\"Test set size: {len(test_lyrics)}\")\n","print(test_lyrics.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o228HvZF3rk3","executionInfo":{"status":"ok","timestamp":1733641672191,"user_tz":480,"elapsed":24259,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"fe6de331-421f-4c7e-9d45-7350b9f3cd2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded successfully!\n","Test set size: 629\n","['Song ID', 'Title', 'Lyrics URL', 'Combined Annotations', 'Wikipedia Annotation', 'Lyrics', 'generated_annotation']\n"]}]},{"cell_type":"code","source":["try:\n","    print(\"Loading model...\")\n","    model = T5ForConditionalGeneration.from_pretrained(model_path)\n","    print(\"Loading tokenizer...\")\n","    tokenizer = T5Tokenizer.from_pretrained(model_path)\n","    print(\"Model and tokenizer loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}\")\n","    # Check contents of the model directory\n","    print(\"\\nModel directory contents:\")\n","    print(os.listdir(model_path))"],"metadata":{"id":"KgRj6Mcj6W4A","executionInfo":{"status":"ok","timestamp":1733641673523,"user_tz":480,"elapsed":1334,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"afdc7cea-4aac-4cdb-9ca4-8e3619f7fcb5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Loading tokenizer...\n","Model and tokenizer loaded successfully!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14854,"status":"ok","timestamp":1733641688375,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"},"user_tz":480},"id":"hhbecELMDQQ1","outputId":"aca641aa-56ee-43fd-e8d1-6977b9f8a1b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.46.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bert-score\n","Successfully installed bert-score-0.3.13\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2581f86b426b28e173e2cbd5a57e8476d1bd3e35b0013b154fac9cc7d4328395\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}],"source":["# Install required package\n","!pip install bert-score\n","from bert_score import score\n","import torch\n","from sklearn.model_selection import train_test_split\n","!pip install rouge-score # rouge-score is the correct package name, not rouge_score.\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from typing import List, Dict, Tuple\n","from transformers import T5Tokenizer\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","from tqdm import tqdm"]},{"cell_type":"code","source":["def evaluate_combined_model(\n","    model: T5ForConditionalGeneration,\n","    tokenizer: T5Tokenizer,\n","    test_data: pd.DataFrame,\n","    batch_size: int = 16\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    evaluation_results = {\n","        'content_coverage': [],\n","        'semantic_similarity': [],\n","        'rouge1': [],\n","        'rouge2': [],\n","        'rougeL': [],\n","        'bert_score': []\n","    }\n","\n","    examples = []\n","    previous_bert_score = 0.0\n","\n","    for idx in tqdm(range(0, len(test_data), batch_size)):\n","        batch_lyrics = test_data['Lyrics'].iloc[idx:idx + batch_size].tolist()\n","        batch_annotations = test_data['Combined Annotations'].iloc[idx:idx + batch_size].tolist()\n","\n","        inputs = tokenizer(\n","            [f\"summarize lyrics and capture meaning: {lyric}\" for lyric in batch_lyrics],\n","            padding=True,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_ids=inputs['input_ids'],\n","                attention_mask=inputs['attention_mask'],\n","                max_length=150,\n","                min_length=50,\n","                num_beams=4,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                no_repeat_ngram_size=3,\n","                length_penalty=1.0,\n","                repetition_penalty=1.2\n","            )\n","\n","            generated_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","        for i in range(len(generated_summaries)):\n","            original_lyric = batch_lyrics[i]\n","            generated_summary = generated_summaries[i]\n","            reference_annotation = batch_annotations[i]\n","\n","            metrics = calculate_metrics(\n","                original_lyric,\n","                generated_summary,\n","                reference_annotation\n","            )\n","\n","            for key, value in metrics.items():\n","                evaluation_results[key].append(value)\n","\n","            if len(examples) < 5:\n","                examples.append({\n","                    'lyrics': original_lyric,\n","                    'generated_summary': generated_summary,\n","                    'metrics': metrics\n","                })\n","\n","        if idx % 5 == 0:\n","            torch.cuda.empty_cache()\n","\n","    final_metrics = {\n","        'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","        'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","        'avg_rouge1': np.mean(evaluation_results['rouge1']),\n","        'avg_rouge2': np.mean(evaluation_results['rouge2']),\n","        'avg_rougeL': np.mean(evaluation_results['rougeL']),\n","        'avg_bert_score': np.mean(evaluation_results['bert_score'])\n","    }\n","\n","    return final_metrics, examples\n","\n","def calculate_metrics(lyrics: str, summary: str, annotation: str) -> Dict[str, float]:\n","    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    try:\n","        rouge_scores = rouge_scorer_obj.score(summary, annotation)\n","    except KeyError as e:\n","        print(f\"Error calculating ROUGE scores: {e}\")\n","        rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n","\n","    # BERTScore\n","    P, R, F1 = score([summary], [annotation], lang='en', verbose=False)\n","\n","    return {\n","        'content_coverage': calculate_content_coverage(lyrics, summary),\n","        'semantic_similarity': calculate_semantic_similarity(lyrics, summary),\n","        'rouge1': rouge_scores.get('rouge1', 0.0).fmeasure,\n","        'rouge2': rouge_scores.get('rouge2', 0.0).fmeasure,\n","        'rougeL': rouge_scores.get('rougeL', 0.0).fmeasure,\n","        'bert_score': F1.mean().item()\n","    }\n","\n","def calculate_content_coverage(lyrics: str, summary: str) -> float:\n","   \"\"\"Calculate content coverage between lyrics and summary\"\"\"\n","   # Handle NaN or float values\n","   if isinstance(lyrics, float) or isinstance(summary, float):\n","       return 0.0\n","\n","   try:\n","       lyrics_tokens = set(str(lyrics).lower().split())\n","       summary_tokens = set(str(summary).lower().split())\n","       overlap = len(lyrics_tokens.intersection(summary_tokens))\n","       coverage = overlap / len(lyrics_tokens) if lyrics_tokens else 0.0\n","       return coverage\n","   except Exception as e:\n","       print(f\"Error processing lyrics/summary: {e}\")\n","       return 0.0\n","\n","def calculate_semantic_similarity(lyrics: str, summary: str) -> float:\n","   \"\"\"Calculate semantic similarity using token overlap\"\"\"\n","   # Handle NaN or float values\n","   if isinstance(lyrics, float) or isinstance(summary, float):\n","       return 0.0\n","\n","   try:\n","       lyrics_tokens = set(str(lyrics).lower().split())\n","       summary_tokens = set(str(summary).lower().split())\n","       intersection = len(lyrics_tokens.intersection(summary_tokens))\n","       union = len(lyrics_tokens.union(summary_tokens))\n","       return intersection / union if union > 0 else 0.0\n","   except Exception as e:\n","       print(f\"Error processing lyrics/summary: {e}\")\n","       return 0.0\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","    print(\"\\nEvaluation Results:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.3f}\")\n","\n","    print(\"\\nExample Generations:\")\n","    for i, example in enumerate(examples, 1):\n","        print(f\"\\nExample {i}:\")\n","        print(f\"Original Lyrics (truncated): {example['lyrics'][:200]}...\")\n","        print(f\"\\nGenerated Summary: {example['generated_summary']}\")\n","        print(\"\\nMetrics:\")\n","        for metric, value in example['metrics'].items():\n","            print(f\"{metric}: {value:.3f}\")\n"],"metadata":{"id":"D2RWxazhzWlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_lyrics.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"xTqX0BSZ8FLE","executionInfo":{"status":"ok","timestamp":1733641688375,"user_tz":480,"elapsed":3,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"2aa9a3a8-55e9-48d0-e566-e71c3ae971fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Song ID                       Title  \\\n","1029   328891              Someone for Me   \n","1001   141615  Saving All My Love for You   \n","785   8827806      What It Is (Block Boy)   \n","411   3037193             I’m Good (Blue)   \n","1105  1342410             The Way You Are   \n","\n","                                             Lyrics URL  \\\n","1029  https://genius.com/Whitney-houston-someone-for...   \n","1001  https://genius.com/Whitney-houston-saving-all-...   \n","785   https://genius.com/Doechii-what-it-is-block-bo...   \n","411   https://genius.com/David-guetta-and-bebe-rexha...   \n","1105  https://genius.com/Tears-for-fears-the-way-you...   \n","\n","                                   Combined Annotations  \\\n","1029  “Someone for Me” is the third track from Whitn...   \n","1001  “Saving All My Love for You” is a song written...   \n","785                                                   ?   \n","411   “I’m Good (Blue)” is a song by David Guetta an...   \n","1105  “The Way You Are” was the first Tears for Fear...   \n","\n","                                   Wikipedia Annotation  \\\n","1029  No Wikipedia annotation found (artist name not...   \n","1001  \"Saving All My Love for You\" is a song written...   \n","785   What It Is Block Boy is a song by American rap...   \n","411   Im Good Blue is a song by French DJ and produc...   \n","1105  The Way You Are may refer to:\\n\\n\"The Way You ...   \n","\n","                                                 Lyrics generated_annotation  \n","1029  (Someone for me) \\n(Someone for me) \\nI'm here...                  NaN  \n","1001  A few stolen moments is all that we share\\r\\nY...                  NaN  \n","785   What it is, ho? Whats up? Every good girl need...                  NaN  \n","411   Im good, yeah, Im feelin alright Baby, Ima hav...                  NaN  \n","1105  Going far, getting nowhere\\r\\nGoing far, the w...                  NaN  "],"text/html":["\n","  <div id=\"df-9f6ae291-2e15-4bd4-8378-0912d89175ca\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Song ID</th>\n","      <th>Title</th>\n","      <th>Lyrics URL</th>\n","      <th>Combined Annotations</th>\n","      <th>Wikipedia Annotation</th>\n","      <th>Lyrics</th>\n","      <th>generated_annotation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1029</th>\n","      <td>328891</td>\n","      <td>Someone for Me</td>\n","      <td>https://genius.com/Whitney-houston-someone-for...</td>\n","      <td>“Someone for Me” is the third track from Whitn...</td>\n","      <td>No Wikipedia annotation found (artist name not...</td>\n","      <td>(Someone for me) \\n(Someone for me) \\nI'm here...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1001</th>\n","      <td>141615</td>\n","      <td>Saving All My Love for You</td>\n","      <td>https://genius.com/Whitney-houston-saving-all-...</td>\n","      <td>“Saving All My Love for You” is a song written...</td>\n","      <td>\"Saving All My Love for You\" is a song written...</td>\n","      <td>A few stolen moments is all that we share\\r\\nY...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>785</th>\n","      <td>8827806</td>\n","      <td>What It Is (Block Boy)</td>\n","      <td>https://genius.com/Doechii-what-it-is-block-bo...</td>\n","      <td>?</td>\n","      <td>What It Is Block Boy is a song by American rap...</td>\n","      <td>What it is, ho? Whats up? Every good girl need...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>411</th>\n","      <td>3037193</td>\n","      <td>I’m Good (Blue)</td>\n","      <td>https://genius.com/David-guetta-and-bebe-rexha...</td>\n","      <td>“I’m Good (Blue)” is a song by David Guetta an...</td>\n","      <td>Im Good Blue is a song by French DJ and produc...</td>\n","      <td>Im good, yeah, Im feelin alright Baby, Ima hav...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1105</th>\n","      <td>1342410</td>\n","      <td>The Way You Are</td>\n","      <td>https://genius.com/Tears-for-fears-the-way-you...</td>\n","      <td>“The Way You Are” was the first Tears for Fear...</td>\n","      <td>The Way You Are may refer to:\\n\\n\"The Way You ...</td>\n","      <td>Going far, getting nowhere\\r\\nGoing far, the w...</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f6ae291-2e15-4bd4-8378-0912d89175ca')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9f6ae291-2e15-4bd4-8378-0912d89175ca button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9f6ae291-2e15-4bd4-8378-0912d89175ca');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-05cdbe46-6587-4a00-a6b2-ad6f5b444ee0\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05cdbe46-6587-4a00-a6b2-ad6f5b444ee0')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-05cdbe46-6587-4a00-a6b2-ad6f5b444ee0 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"test_lyrics","summary":"{\n  \"name\": \"test_lyrics\",\n  \"rows\": 629,\n  \"fields\": [\n    {\n      \"column\": \"Song ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 624,\n        \"samples\": [\n          2381048,\n          4377458,\n          198462\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 626,\n        \"samples\": [\n          \"Anyway\",\n          \"Jesus Christ 2005 God Bless America\",\n          \"Almost Is Never Enough\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lyrics URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 624,\n        \"samples\": [\n          \"https://genius.com/Chris-brown-anyway-lyrics\",\n          \"https://genius.com/Billie-marten-toulouse-lyrics\",\n          \"https://genius.com/Kelly-clarkson-walk-away-lyrics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Combined Annotations\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 544,\n        \"samples\": [\n          \"     \\u2014Chelsea via Genius\",\n          \"\\u201cSex Tape\\u201d is the second single from Hippo Campus' EP   The humourous track addresses a former friend on   and written off the cuff in the studio.  Throughout the song, the band references summers spent in suburban Minnesota.\",\n          \"Marking the start of a  , \\u201cDaisies\\u201d is a self-empowerment anthem by American singer-songwriter Katy Perry. Shortly after the track\\u2019s release, Katy took to her   to share the inspiration behind the song:   She then elaborated on how the song has \\u201ctaken a new meaning [for her]\\u201d after parts of the world   due to the  .  Unlike her past lead singles which tend to be more upbeat and bubbly, \\u201cDaisies\\u201d features a mid-tempo production in its verses and a guitar in its chorus. This is also Katy\\u2019s first time working with   \\u2014 an American production and songwriting team who has previously worked on hits like January 2018\\u2019s \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Wikipedia Annotation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 319,\n        \"samples\": [\n          \"One Right Now is a song by American rapper and singer Post Malone and Canadian singersongwriter the Weeknd. It was released through Republic Records on November 5, 2021, as the lead single from Malones fourth studio album, Twelve Carat Toothache 2022. The artists wrote the song with Billy Walsh and producers Louis Bell, Brian Lee, and Andrew Bolooki. One Right Now is a synthpop song that is set in the key of D major. Background On November 2, 2021, Post Malone and the Weeknd posted a 7second snippet of the song titled PMTWORNUpdate.5.nonhyped.w1.mp3 on their Instagram accounts. The post received over 150,000 likes in one hour. It was initially unknown what the title of the collaboration would be until Malones manager Dre London revealed the title One Right Now. The song marks the first time the artists appeared on a song together. Composition Based on the teaser, the sound was described as a synthy midtempo club track. It sees the two harmonizing over an upbeat instrumental. Writing for Billboard, Starr Bowenbank referred to the instrumental as a poppy synth beat. Alex Zidel of HotNewHipHop predicted that the song will dominate the radio for the remainder of the year, and likely well into next. For the radio version, The Weeknd replaced his lyric censors from the official clean version with the newly recorded lyrics You think its so easy messin with my feelings and You probably slap all my enemies. Post Malone left his lyric censors as they were. They both sang That you told me that he fucked you on twice, separate then together, and tying to Post Malones record label, it was also left censored. Music video The music video for One Right Now, directed by Tanu Muino, was released on November 15, 2021. In the video, Malone and the Weeknd engage in a shootout with each others teams. It features the two artists as vengeful enemies in an elaborate scenario, according to Rolling Stone, that ends in a shootout where both are killed after shooting each other at the same time. The video also mentioned MoonPay, a cryptocurrency company that had paid Post Malone and The Weeknd for the placement. Personnel Credits adapted from Tidal and Genius. Post Malone vocals, songwriting The Weeknd vocals, songwriting Louis Bell songwriting, production, recording engineering, vocal production, keyboards, synthesizer, programming, drums Andrew Bolooki songwriting, production, programming, synthesizer Brian Lee songwriting, production Billy Walsh songwriting Manny Marroquin mixing Mike Bozzi mastering Charts Certifications Release history References External links One Right Now Audio on YouTube One Right Now Lyric video on YouTube\",\n          \"Ball for Me is a song by American musician Post Malone featuring vocals Nicki Minaj. It was written by the artists and producer Louis Bell. The song was released through Republic Records on May 8, 2018, as the fourth single from Malones second studio album, Beerbongs Bentleys 2018, the song was also featured on the soundtrack to the EA Sports Video Game Madden NFL 19. The song debuted and peaked at number 16 on the US Billboard Hot 100, reached the top 10 in Canada and the top 20 in Australia and Slovakia. Music video The official music video for Ball for Me was teased by Minaj through various social media platforms on June 27, 2018. Later on Queen Radio, Minaj confirmed that the music video wouldnt be released due to the fact that Malone didnt like his visuals. Credits and personnel Credits adapted from Post Malones official website. Post Malone vocals, composition Nicki Minaj vocals, composition Louis Bell composition, production, programming, recording, vocal production Mike Bozzi mastering Scott Desmarais mixing assistance Robin Florent mixing assistance Chris Galland mixing assistance Manny Marroquin mixing Charts Certifications Release history References\",\n          \"Matilda or Mathilda may refer to Animals Matilda chicken 19902006, Worlds Oldest Living Chicken record holder Mathilda gastropod, a genus of gastropods in the family Mathildidae Matilda horse 18241846, British Thoroughbred racehorse Matilda, a dog of the professional wrestling tagteam The British Bulldogs Arts and entertainment Fictional characters Matelda, also spelled Matilda, a character from Dante Alighieris Divine Comedy Matilda, a comic strip character from Dennis the Menace and Gnasher Matilda, a house robot in Robot Wars Matilda Quinn, a character in the 2019 comic series Chrononauts Futureshock Matilda Wormwood, title character of Roald Dahls novel Matilda One of the main characters from the Finnish game series Angry Birds Film Matilda 1978 film, an American comedy Matilda 1996 film, based on Roald Dahls novel Matilda 2017 film, \\u0430 Russian historical romantic drama Matilda the Musical film a Netflix adaptation of Matilda the Musical Literature Matilda Normanby novel, an 1825 novel by Lord Normanby Matilda novel, a 1988 childrens novel by Roald Dahl Mathilda novella, the second long work of fiction of Mary Shelley Music Waltzing Matilda, a song often described as Australias unofficial national anthem Matilda album, by Stateless, 2011 Matilda calypso song, composed by Norman Span King Radio, which in 1953 became known from the version by Harry Belafonte Matilda altJ song, 2012 Matilda Harry Styles song, 2022 Other uses in arts and entertainment Matilda the Musical, a 2010 stage musical based on Roald Dahls novel Matilda Awards, Australian performance awards People Matilda name also Mathilda and Mathilde, a female given name Empress Matilda 11021167, claimant to the English throne Matilda, Countess of Angus fl. 13th century, Scottish noblewoman Matilda, Countess of Rethel 10911151, French noblewoman Matilda of Amboise c. 1200 1256, French noblewoman Matilda of Andechs died 1245, daughter of Margrave Berthold I of Istria Matilda of Anjou c. 1106 1154, Duchess of Normandy Matilda of Bavaria disambiguation, several people Matilda of B\\u00e9thune died 1264, countess of Flanders Matilda of Boulogne disambiguation, several people Matilda of BrunswickL\\u00fcneburg 12761318, a German noblewoman Matilda of Carinthia died 1160 or 1161, daughter of Engelbert, Duke of Carinthia Matilda of England, Duchess of Saxony 11561189, daughter of Henry II Matilda of Flanders c. 10311083, wife of William the Conqueror Matilda of France 943981982, member of the Carolingian dynasty Matilda of Franconia c. 1027 1034, daughter of Emperor Conrad II Matilda of Frisia died 1044, wife of Henry I, King of the Franks Matilda of Ringelheim c. 894968, or Saint Matilda, a Saxon noblewoman Matilda of Scotland c. 10801118, wife of Henry I Matilda of Tuscany 10461115, Margravine of Tuscany Matilda of Vianden, Lady of Po\\u017eega c. 1215after 1255, wife of John Angelos of Syrmia Princess Mathilde Caroline of Bavaria 18131862, grand duchess of Hesse Transportation and military MATILDA, a remotecontrolled surveillance and reconnaissance robot Matilda I tank, a British infantry tank 19381940 Matilda II, a British infantry tank 19391955 HMS Matilda, the name of two Royal Navy ships Matilda ship, the name of several ships Other uses Matilda, a colloquial term for a swag bedroll carried by a swagman in the Australian bush Australia womens national soccer team, nicknamed Matildas Matilda mascot, the mascot of the 1982 Commonwealth Games in Australia Matilda Cruises, an Australia ferry and cruise service Matilda effect, a bias against acknowledging the achievements of women scientists whose work is attributed to their male colleagues. Matilda International Hospital, in Hong Kong See also All pages with titles beginning with Matilda All pages with titles containing Matilda All pages with titles beginning with Mathilda All pages with titles containing Mathilda Mathilde disambiguation Matilde\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lyrics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 621,\n        \"samples\": [\n          \"Bodies fill the fields I see, hungry heroes end\\r\\nNo one to play soldier now, no one to pretend \\nRunning blind through killing fields, bred to kill them all \\nVictim of what said should be, a servant till I fall \\nSoldier boy, made of clay \\n\\nNow an empty shell \\n\\nTwenty-one, only son \\n\\nBut he served us well \\n\\nBred to kill, not to care \\n\\nDo just as we say \\n\\nFinished here. Greetings, Death, \\n\\nHe's yours to take away \\n\\n\\n\\nBack to the front \\n\\nYou will do what I say, when I say \\n\\nBack to the front \\n\\nYou will die when I say, you must die \\n\\nBack to the front \\n\\nYou coward, you servant, you blind man \\n\\n\\n\\nBarking of machine-gun fire does nothing to me now \\n\\nSounding of the clock that ticks, get used to it somehow \\n\\nMore a man, more stripes you wear, glory-seeker trends \\n\\nBodies fill the fields I see \\n\\nThe slaughter never ends \\n\\n\\n\\nSoldier boy, made of clay \\n\\nNow an empty shell \\n\\nTwenty-one, only son \\n\\nBut he served us well \\n\\nBred to kill, not to care \\n\\nDo just as we say \\n\\nFinished here. Greetings, Death, \\n\\nHe's yours to take away \\n\\n\\n\\nBack to the front \\n\\nYou will do what I say, when I say \\n\\nBack to the front \\n\\nYou will die when I say, you must die \\n\\nBack to the front \\n\\nYou coward, you servant, you blind man \\n\\n\\n\\nWhy am I dying? \\n\\nKill, have no fear \\n\\nLie, live off lying \\n\\nHell, hell is here \\n\\n\\n\\nI was born for dying \\n\\n\\n\\nLife planned out before my birth, nothing could I say \\n\\nHad no chance to see myself, molded day by day \\n\\nLooking back I realize, nothing have I done \\n\\nLeft to die with only friend \\n\\nAlone I clench my gun \\n\\n\\n\\nSoldier boy, made of clay \\n\\nNow an empty shell \\n\\nTwenty-one, only son \\n\\nBut he served us well \\n\\nBred to kill, not to care \\n\\nJust do as we say \\n\\nFinished here. Greetings, Death, \\n\\nHe's yours to take away \\n\\n\\n\\nBack to the front \\n\\nYou will do what I say, when I say \\n\\nBack to the front \\n\\nYou will die when I say, you must die \\n\\nBack to the front \\n\\nYou coward, you servant, you blind man \\n\\n\\n\\nBack to the front\",\n          \"Somebody wrote Quit on a napkin, I took it out of my tip jar, laughin Like, Damn, what a asshole, man, Im just a flat broke boy with a guitar askin For anybody to pay attention, its not like anybody paid admission I was just wishin somebody would listen Give me a shot, and I aint talkin bout Canadian Mist then Florida, Georgia put me on the map, Morgan put me on the track Fucked around and penned a couple hits, chucked em out to see if theyd react Talked about the way of life I live, all the things the country life will give Its like I never left my neighborhood, but I burned out of it, and I turned out good, if you ask me But shit, I aint the G.O.A.T., Im th\\u0435 black sheep Hellbent to find closure, I cant l\\u0435t go A note somebody wrote like ten years ago, put a chip on my shoulder And if you wanted me to quit, you shouldve saved it, bro If you dont wanna start shit, dont say it And when it comes to the king, thats the radio If you dont like my shit, dont play it, I dont give a fuck Ima still book arenas and fill em up Ima still keep singin songs about drinkin a fifth of Jack D, throwin up, then wakin up alone in my truck I heard the voice of God that night He said, Keep goin, boy, its all alright Ima bless your soul, so one day down the road Youll be looking in the mirror like, I won that fight So before you choose hate, get to know a guy He might end up a poet that was born to fly He might end up a man of the people, damn, what a scene, a redneck glorified With a stack of awards on a napkin, a bored little bastard wrote to try to warn a guy Maybe Im just petty cause theyre just metal Wait, then again, so am I Fuck you\",\n          \"We keep going don't stop running\\r\\nThey keep selling, we dont want it\\r\\nSo close to it almost found a way\\r\\nTwo steps closer they keep coming\\r\\nWe keep yelling \\\"we dont want it\\\"\\r\\nAlmost better this things bound to break\\n\\nWhy dont you make yourself available\\n\\nSit back, she told me that she wanted it free\\n\\nIt's easy\\n\\nOh no in love im just an animal\\n\\nYou said \\\"you want me but you want me to leave, you want me but you want you want me to leave\\\"\\n\\nUntil the end because \\n\\nWe keep going don't stop running\\n\\nThey keep selling, we dont want it\\n\\nSo close to it almost found a way\\n\\nAnd everyday they get two steps closer\\n\\nThey keep coming we keep yelling \\\"we dont want it\\\"\\n\\nAlmost better this things bound to break\\n\\nAfter it all it's still repairable\\n\\nI know that I forgot my history\\n\\nIt's easy (eh)\\n\\nLets wake about at all the terrible\\n\\nI know that there's a million things we could be\\n\\nIf we could only (x4) agree\\n\\nUntil the end because\\n\\nWe keep going don't stop running\\n\\nThey keep selling, we dont want it\\n\\nSo close to it almost found a way\\n\\nAnd everyday they get two steps closer they keep coming\\n\\nWe keep yelling \\\"we dont want it\\\"\\n\\nAlmost better this things bound to break\\n\\nOnly was a holiday\\n\\nAlways playing never work\\n\\nbut they selling all the bathe\\n\\nand its the way I go bizerk I\\n\\n(no)(x16)\\n\\n\\n\\n(Merci \\u00e0 Mar\\u00eava pour cettes paroles)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_annotation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\\\"Beautiful Ghosts (From the Motion Picture Cats)\\\" is a song by Taylor Swift and Andrew Lloyd Webber, written for the 2019 film adaptation of *Cats*, in which Swift played Bombalurina.  The song, performed in the film by Francesca Hayward as Victoria, is considered the film's signature song. Swift performs the official single version played over the end credits. Judi Dench as Old Deuteronomy sings a 30-second reprise. \\n\\nReleased on November 15, 2019, the song received positive reviews from many critics, with praise directed at Swift's vocals and the song's haunting melody. Some critics, however, found the lyrics banal and the song derivative. \\\"Beautiful Ghosts\\\" was nominated for Best Original Song at the 77th Golden Globe Awards and Best Song Written for Visual Media at the 63rd Annual Grammy Awards.\\n\\nSwift and Lloyd Webber co-wrote the song, with Swift writing lyrics and both contributing to the music.  It was produced by Greg Wells, Lloyd Webber, and Tom Hooper. The orchestral ballad, in the key of E major, blends minor key verses with major key choruses.  It features a tempo of 60 beats per minute and complex time signatures.\\n\\n**Credits and Personnel (adapted from Tidal):**\\n*[Insert Tidal credits here]*\\n\",\n          \"\\\"All of the Girls You Loved Before\\\" is a synth-pop love song by Taylor Swift, surprise-released on March 17, 2023, ahead of her Eras Tour.  Originally written for her 2019 album *Lover* but ultimately left off, the song gained popularity after a demo leaked on TikTok.  It was later included on the deluxe edition *The More Lover Chapter*. Swift wrote and produced the track with Frank Dukes (Adam King Feeney) and Louis Bell.  The lyrics express gratitude for the women in her partner's past, recognizing their role in shaping him into the person he is now. Critics praised the song's affectionate lyrics and dreamy production. \\\"All of the Girls You Loved Before\\\" charted within the top 25 in several countries, reaching number 10 on the Billboard Global 200 and receiving certifications in Australia and the United Kingdom.\\n\\n**Personnel:**\\n\\n* Taylor Swift \\u2013 vocals, songwriter, producer\\n* Louis Bell \\u2013 producer, songwriter, recording engineer, programming\\n* Frank Dukes \\u2013 producer, songwriter, programming, keyboards, guitar\\n* Matthew Tavares \\u2013 guitar\\n* Serban Ghenea \\u2013 mixer\\n* Bryce Bordone \\u2013 mix engineer\\n* Randy Merrill \\u2013 mastering engineer\\n\",\n          \"\\\"Champagne Problems\\\" is a song by American singer-songwriter Taylor Swift from her ninth studio album, *Evermore* (2020). Co-written with Joe Alwyn (under the pseudonym William Bowery) and produced with Aaron Dessner, the ballad features piano, guitar, and choir vocals, creating a melancholic atmosphere. \\n\\nLyrically, the song tells the story of a woman who rejects her lover's marriage proposal at a Christmas party, taking responsibility for the subsequent heartbreak. The narrative, told from the second-person perspective, details the emotional fallout of the rejection and the narrator's observation of her former lover moving on.\\n\\n\\\"Champagne Problems\\\" received widespread critical acclaim, with praise directed towards Swift's songwriting, character development, and emotional depth. Critics highlighted the song's poignant lyrics, impactful bridge, and thematic similarities to Swift's earlier work.  It was frequently cited as a highlight of the *Evermore* album.\\n\\nCommercially, the song debuted within the top 25 of multiple charts globally, including the *Billboard* Global 200, US *Billboard* Hot 100, and Canadian Hot 100.  It also reached the top 25 in Ireland, Australia, Malaysia, Singapore, and New Zealand, and charted in the UK, Portugal, and Switzerland. The song's cultural impact extended to a themed hotel package and its inclusion in Swift's Eras Tour (2023-2024).\\n\\n\\n**Credits:**\\n\\n* Taylor Swift: vocals, songwriting, production\\n* Aaron Dessner: production, recording, piano, synthesizer, acoustic guitar, synth bass\\n* William Bowery: songwriting\\n* Jonathan Low: vocal recording, mixing, recording\\n* Greg Calbi: mastering\\n* Steve Fallone: mastering\\n* Logan Coale: upright bass\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#find unique urls in test_lyrics\n","test_lyrics['Lyrics URL'].nunique()\n","\n","#print all test urls\n","test_lyrics['Lyrics URL'].unique()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDSbOSAAGMjP","executionInfo":{"status":"ok","timestamp":1733642260521,"user_tz":480,"elapsed":995,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"fb8b6aad-e781-45cb-94cd-56aa46b1037f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['https://genius.com/Whitney-houston-someone-for-me-lyrics',\n","       'https://genius.com/Whitney-houston-saving-all-my-love-for-you-lyrics',\n","       'https://genius.com/Doechii-what-it-is-block-boy-lyrics',\n","       'https://genius.com/David-guetta-and-bebe-rexha-im-good-blue-lyrics',\n","       'https://genius.com/Tears-for-fears-the-way-you-are-lyrics',\n","       'https://genius.com/Shakira-que-me-quedes-tu-lyrics',\n","       'https://genius.com/Shakira-dare-la-la-la-lyrics',\n","       'https://genius.com/Samia-amelia-lyrics',\n","       'https://genius.com/Travis-scott-90210-lyrics',\n","       'https://genius.com/Hippo-campus-everything-at-once-lyrics',\n","       'https://genius.com/Green-day-basket-case-4-track-demo-lyrics',\n","       'https://genius.com/The-beach-boys-god-only-knows-master-track-mix-with-a-cappella-tag-lyrics',\n","       'https://genius.com/David-guetta-little-bad-girl-lyrics',\n","       'https://genius.com/Maroon-5-and-megan-thee-stallion-beautiful-mistakes-lyrics',\n","       'https://genius.com/One-direction-18-lyrics',\n","       'https://genius.com/Tennis-marathon-lyrics',\n","       'https://genius.com/Maggie-rogers-honey-lyrics',\n","       'https://genius.com/King-princess-only-time-makes-it-human-lyrics',\n","       'https://genius.com/Cimorelli-golden-hour-lyrics',\n","       'https://genius.com/Kelly-clarkson-its-quiet-uptown-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-commotion-live-lyrics',\n","       'https://genius.com/Coma-cinema-flower-pills-lyrics',\n","       'https://genius.com/Meghan-trainor-me-too-lyrics',\n","       'https://genius.com/Dua-lipa-break-my-heart-lyrics',\n","       'https://genius.com/Maroon-5-never-gonna-leave-this-bed-lyrics',\n","       'https://genius.com/Ice-spice-and-travis-scott-oh-shhh-lyrics',\n","       'https://genius.com/Sabrina-carpenter-bed-chem-lyrics',\n","       'https://genius.com/Camera-obscura-books-written-for-girls-live-acoustic-lyrics',\n","       'https://genius.com/The-beach-boys-dont-back-down-mono-lyrics',\n","       'https://genius.com/Britney-spears-born-to-make-you-happy-radio-edit-lyrics',\n","       'https://genius.com/Cody-johnson-aint-nothin-to-it-lyrics',\n","       'https://genius.com/Linkin-park-cure-for-the-itch-lyrics',\n","       'https://genius.com/Jason-aldean-you-make-it-easy-lyrics',\n","       'https://genius.com/Coi-leray-huddy-lyrics',\n","       'https://genius.com/Soccer-mommy-your-dog-lyrics',\n","       'https://genius.com/Whitney-houston-didnt-we-almost-have-it-all-lyrics',\n","       'https://genius.com/Selena-gomez-good-for-you-lyrics',\n","       'https://genius.com/Harry-styles-matilda-lyrics',\n","       'https://genius.com/Suki-waterhouse-valentine-lyrics',\n","       'https://genius.com/Lyn-lapid-itsy-bitsy-lyrics',\n","       'https://genius.com/Tai-verdes-solamente-lyrics',\n","       'https://genius.com/Lyn-lapid-july-lyrics',\n","       'https://genius.com/Bleachers-who-i-want-you-to-love-lyrics',\n","       'https://genius.com/Alexandra-savior-girlie-lyrics',\n","       'https://genius.com/Jordan-davis-buy-dirt-lyrics',\n","       'https://genius.com/Maggie-rogers-dog-years-lyrics',\n","       'https://genius.com/Finneas-around-my-neck-lyrics',\n","       'https://genius.com/Esmee-denters-bigger-than-the-world-lyrics',\n","       'https://genius.com/Astn-happier-than-ever-lyrics',\n","       'https://genius.com/Metallica-disposable-heroes-lyrics',\n","       'https://genius.com/Tears-for-fears-goodnight-song-lyrics',\n","       'https://genius.com/Green-day-best-thing-in-town-lyrics',\n","       'https://genius.com/Soccer-mommy-with-u-lyrics',\n","       'https://genius.com/Fun-some-nights-intro-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-down-on-the-corner-lyrics',\n","       'https://genius.com/Finneas-another-year-lyrics',\n","       'https://genius.com/Phoebe-bridgers-motion-sickness-lyrics',\n","       'https://genius.com/Bebe-rexha-gone-lyrics',\n","       'https://genius.com/Camera-obscura-shine-like-a-new-pin-lyrics',\n","       'https://genius.com/Shakira-objection-tango-lyrics',\n","       'https://genius.com/Drake-champagne-poetry-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-tombstone-shadow-lyrics',\n","       'https://genius.com/Haim-days-are-gone-lyrics',\n","       'https://genius.com/Finneas-little-window-lyrics',\n","       'https://genius.com/Zach-bryan-open-the-gate-lyrics',\n","       'https://genius.com/Heather-headley-jason-raize-max-casella-tom-alan-robbins-and-the-lion-king-ensemble-can-you-feel-the-love-tonight-lyrics',\n","       'https://genius.com/Chelsea-cutler-without-you-lyrics',\n","       'https://genius.com/Rex-orange-county-look-me-in-the-eyes-lyrics',\n","       'https://genius.com/Fleetwood-mac-angel-lyrics',\n","       'https://genius.com/Shakira-estoy-aqui-lyrics',\n","       'https://genius.com/Tennis-im-callin-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-fred-astaire-lyrics',\n","       'https://genius.com/Britney-spears-dont-keep-me-waiting-lyrics',\n","       'https://genius.com/Stephen-sanchez-until-i-found-you-lyrics',\n","       'https://genius.com/Fleetwood-mac-like-it-this-way-lyrics',\n","       'https://genius.com/Lil-baby-and-gunna-drip-too-hard-lyrics',\n","       'https://genius.com/Tennis-pollen-song-lyrics',\n","       'https://genius.com/Green-day-hold-on-lyrics',\n","       'https://genius.com/Electric-guest-dollar-lyrics',\n","       'https://genius.com/Linkin-park-husky-hit-the-floor-demo-lyrics',\n","       'https://genius.com/Between-friends-whats-up-lyrics',\n","       'https://genius.com/Marshmello-and-kane-brown-miles-on-it-lyrics',\n","       'https://genius.com/Electric-guest-american-daydream-lyrics',\n","       'https://genius.com/Bleachers-chinatown-live-at-radio-city-music-hall-lyrics',\n","       'https://genius.com/Lil-durk-david-ruffin-lyrics',\n","       'https://genius.com/Ariana-grande-imperfect-for-you-lyrics',\n","       'https://genius.com/London-grammar-nightcall-lyrics',\n","       'https://genius.com/Metallica-hero-of-the-day-lyrics',\n","       'https://genius.com/Coldplay-every-teardrop-is-a-waterfall-lyrics',\n","       'https://genius.com/Post-malone-over-now-lyrics',\n","       'https://genius.com/Haim-los-angeles-lyrics',\n","       'https://genius.com/Maggie-rogers-and-del-water-gap-new-song-lyrics',\n","       'https://genius.com/Doja-cat-agora-hills-lyrics',\n","       'https://genius.com/Jordan-davis-almost-maybes-lyrics',\n","       'https://genius.com/Tv-girl-daughter-of-a-cop-lyrics',\n","       'https://genius.com/Abba-summer-night-city-single-version-lyrics',\n","       'https://genius.com/Morgan-wallen-last-night-lyrics',\n","       'https://genius.com/Green-day-boulevard-of-broken-dreams-lyrics',\n","       'https://genius.com/Rex-orange-county-the-table-lyrics',\n","       'https://genius.com/Between-friends-bruise-lyrics',\n","       'https://genius.com/Maggie-rogers-love-you-for-a-long-time-lyrics',\n","       'https://genius.com/Harry-styles-adore-you-lyrics',\n","       'https://genius.com/Billie-marten-liquid-love-lyrics',\n","       'https://genius.com/Britney-spears-mona-lisa-lyrics',\n","       'https://genius.com/Fleetwood-mac-merry-go-round-take-1-lyrics',\n","       'https://genius.com/Ed-sheeran-castle-on-the-hill-lyrics',\n","       'https://genius.com/Wham-last-christmas-lyrics',\n","       'https://genius.com/Haim-the-steps-lyrics',\n","       'https://genius.com/The-1975-oh-caroline-lyrics',\n","       'https://genius.com/Tessa-violet-breakdown-lyrics',\n","       'https://genius.com/Harry-styles-carolina-lyrics',\n","       'https://genius.com/Gunna-car-sick-lyrics',\n","       'https://genius.com/Whitney-houston-i-have-nothing-lyrics',\n","       'https://genius.com/Abba-ring-ring-bara-du-slog-en-signal-swedish-version-lyrics',\n","       'https://genius.com/Selena-gomez-bad-liar-lyrics',\n","       'https://genius.com/Coi-leray-no-more-parties-lyrics',\n","       'https://genius.com/Tears-for-fears-change-lyrics',\n","       'https://genius.com/Justin-timberlake-livin-off-the-land-lyrics',\n","       'https://genius.com/Sam-smith-baby-you-make-me-crazy-lyrics',\n","       'https://genius.com/One-direction-steal-my-girl-lyrics',\n","       'https://genius.com/Shakira-me-enamore-lyrics',\n","       'https://genius.com/Ajr-worlds-smallest-violin-lyrics',\n","       'https://genius.com/Justin-timberlake-breeze-off-the-pond-lyrics',\n","       'https://genius.com/Fleetwood-mac-as-long-as-you-follow-lyrics',\n","       'https://genius.com/Parker-mccollum-to-be-loved-by-you-lyrics',\n","       'https://genius.com/Bleachers-alfies-song-not-so-typical-love-song-lyrics',\n","       'https://genius.com/Soccer-mommy-bloodstream-lyrics',\n","       'https://genius.com/Qveen-herby-wap-lyrics',\n","       'https://genius.com/King-princess-dotted-lines-lyrics',\n","       'https://genius.com/Slnm-duet-slnm-remix-lyrics',\n","       'https://genius.com/Green-day-give-me-novacaine-lyrics',\n","       'https://genius.com/Isaac-dunbar-take-it-slow-lyrics',\n","       'https://genius.com/Tears-for-fears-brian-wilson-said-lyrics',\n","       'https://genius.com/Kelly-clarkson-bad-reputation-lyrics',\n","       'https://genius.com/Sza-kill-bill-lyrics',\n","       'https://genius.com/Isaac-dunbar-mime-lyrics',\n","       'https://genius.com/Gus-dapperton-amadelle-with-love-lyrics',\n","       'https://genius.com/Madonna-goodbye-to-innocence-lyrics',\n","       'https://genius.com/Post-malone-and-the-weeknd-one-right-now-lyrics',\n","       'https://genius.com/Morgan-wallen-dangerous-lyrics',\n","       'https://genius.com/Coi-leray-players-lyrics',\n","       'https://genius.com/Morgan-wallen-lies-lies-lies-lyrics',\n","       'https://genius.com/Tyler-the-creator-a-boy-is-a-gun-lyrics',\n","       'https://genius.com/Sza-prom-lyrics',\n","       'https://genius.com/Madonna-erotica-live-lyrics',\n","       'https://genius.com/Fleetwood-mac-allow-me-one-more-show-lyrics',\n","       'https://genius.com/Phoebe-bridgers-moon-song-lyrics',\n","       'https://genius.com/Sza-ghost-in-the-machine-lyrics',\n","       'https://genius.com/Chappell-roan-casual-lyrics',\n","       'https://genius.com/Ashe-ashe-lyrics',\n","       'https://genius.com/Olivia-rodrigo-bad-idea-right-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-colorful-lyrics',\n","       'https://genius.com/Luna-li-confusion-song-lyrics', nan,\n","       'https://genius.com/Sam-smith-lay-me-down-lyrics',\n","       'https://genius.com/Sza-broken-clocks-lyrics',\n","       'https://genius.com/Maggie-rogers-echo-hymn-intro-lyrics',\n","       'https://genius.com/Hippo-campus-way-it-goes-lyrics',\n","       'https://genius.com/Doja-cat-like-that-lyrics',\n","       'https://genius.com/Cults-you-know-what-i-mean-lyrics',\n","       'https://genius.com/Arcade-fire-we-exist-lyrics',\n","       'https://genius.com/Madonna-4-minutes-lyrics',\n","       'https://genius.com/Cults-always-forever-lyrics',\n","       'https://genius.com/Electric-guest-the-bait-the-shoes-remix-lyrics',\n","       'https://genius.com/One-direction-moments-lyrics',\n","       'https://genius.com/The-1975-me-lyrics',\n","       'https://genius.com/Doja-cat-balut-lyrics',\n","       'https://genius.com/Metallica-chasing-light-lyrics',\n","       'https://genius.com/Kimbra-lightyears-lyrics',\n","       'https://genius.com/One-direction-little-things-live-version-from-the-motion-picture-one-direction-this-is-us-lyrics',\n","       'https://genius.com/Lainey-wilson-watermelon-moonshine-lyrics',\n","       'https://genius.com/Au-revoir-simone-knight-of-wands-lyrics',\n","       'https://genius.com/Meghan-trainor-after-you-lyrics',\n","       'https://genius.com/Tears-for-fears-sorry-lyrics',\n","       'https://genius.com/Maggie-rogers-retrograde-lyrics',\n","       'https://genius.com/Meghan-trainor-im-a-lady-lyrics',\n","       'https://genius.com/Beabadoobee-sunny-day-lyrics',\n","       'https://genius.com/Frankie-cosmos-im-fried-lyrics',\n","       'https://genius.com/Kim-petras-brrr-lyrics',\n","       'https://genius.com/Jason-aldean-got-what-i-got-lyrics',\n","       'https://genius.com/One-direction-one-thing-lyrics',\n","       'https://genius.com/Doja-cat-imagine-lyrics',\n","       'https://genius.com/London-grammar-talking-lyrics',\n","       'https://genius.com/Sza-anything-lyrics',\n","       'https://genius.com/David-guetta-everytime-we-touch-lyrics',\n","       'https://genius.com/Haim-another-try-lyrics',\n","       'https://genius.com/Cardi-b-best-life-lyrics',\n","       'https://genius.com/Ice-spice-phat-butt-lyrics',\n","       'https://genius.com/Billie-marten-boxes-lyrics',\n","       'https://genius.com/Morgan-wallen-cover-me-up-lyrics',\n","       'https://genius.com/Billie-marten-heaven-lyrics',\n","       'https://genius.com/Alice-kristiansen-lost-my-mind-lyrics',\n","       'https://genius.com/Old-dominion-nowhere-fast-lyrics',\n","       'https://genius.com/Lainey-wilson-heart-like-a-truck-lyrics',\n","       'https://genius.com/Sam-smith-im-not-the-only-one-lyrics',\n","       'https://genius.com/Chappell-roan-pink-pony-club-lyrics',\n","       'https://genius.com/Alexandra-savior-crying-all-the-time-lyrics',\n","       'https://genius.com/Selena-gomez-de-una-vez-lyrics',\n","       'https://genius.com/Lil-uzi-vert-aye-lyrics',\n","       'https://genius.com/Isaac-dunbar-makeup-drawer-lyrics',\n","       'https://genius.com/Au-revoir-simone-love-you-dont-know-me-lyrics',\n","       'https://genius.com/Katy-perry-growing-pains-lyrics',\n","       'https://genius.com/London-grammar-big-picture-lyrics',\n","       'https://genius.com/Katy-perry-i-think-im-ready-lyrics',\n","       'https://genius.com/Abba-andante-andante-spanish-version-lyrics',\n","       'https://genius.com/Dayglow-something-lyrics',\n","       'https://genius.com/Drake-back-to-back-lyrics',\n","       'https://genius.com/Ashe-i-wanna-love-you-but-i-dont-lyrics',\n","       'https://genius.com/The-beach-boys-all-summer-long-lyrics',\n","       'https://genius.com/Between-friends-drive-over-me-lyrics',\n","       'https://genius.com/Cody-johnson-the-painter-lyrics',\n","       'https://genius.com/Tears-for-fears-the-working-hour-piano-version-lyrics',\n","       'https://genius.com/Soccer-mommy-im-on-fire-lyrics',\n","       'https://genius.com/Kimbra-love-in-high-places-lyrics',\n","       'https://genius.com/Gus-dapperton-nomadicon-lyrics',\n","       'https://genius.com/Ariana-grande-and-nathan-sykes-almost-is-never-enough-lyrics',\n","       'https://genius.com/Kelly-clarkson-walk-away-lyrics',\n","       'https://genius.com/Kimbra-sweet-relief-lyrics',\n","       'https://genius.com/Sam-smith-koffee-and-jessie-reyez-gimme-lyrics',\n","       'https://genius.com/Selena-gomez-and-rauw-alejandro-baila-conmigo-lyrics',\n","       'https://genius.com/Luke-combs-doin-this-lyrics',\n","       'https://genius.com/The-1975-its-not-living-if-its-not-with-you-lyrics',\n","       'https://genius.com/Electric-guest-control-lyrics',\n","       'https://genius.com/Drake-7969-santa-lyrics',\n","       'https://genius.com/Taylor-swift-christmas-tree-farm-lyrics',\n","       'https://genius.com/Isaac-dunbar-intimate-moments-lyrics',\n","       'https://genius.com/Cimorelli-cant-stop-the-feeling-lyrics',\n","       'https://genius.com/Maroon-5-wipe-your-eyes-lyrics',\n","       'https://genius.com/Abba-im-a-marionette-lyrics',\n","       'https://genius.com/Metro-boomin-around-me-lyrics',\n","       'https://genius.com/Maggie-rogers-say-it-lyrics',\n","       'https://genius.com/Frankie-cosmos-accommodate-lyrics',\n","       'https://genius.com/Ashe-pull-the-plug-lyrics',\n","       'https://genius.com/Jelly-roll-son-of-a-sinner-lyrics',\n","       'https://genius.com/Alvin-and-the-chipmunks-bad-romance-lyrics',\n","       'https://genius.com/Suki-waterhouse-faded-lyrics',\n","       'https://genius.com/Bebe-rexha-mine-lyrics',\n","       'https://genius.com/Post-malone-candy-paint-lyrics',\n","       'https://genius.com/Green-day-ashley-lyrics',\n","       'https://genius.com/Jeremy-zucker-and-chelsea-cutler-lonely-alone-lyrics',\n","       'https://genius.com/Sza-open-arms-lyrics',\n","       'https://genius.com/Kimbra-everlovin-ya-lyrics',\n","       'https://genius.com/Green-day-haushinka-lyrics',\n","       'https://genius.com/Tessa-violet-good-things-go-bad-lyrics',\n","       'https://genius.com/Maroon-5-must-get-out-lyrics',\n","       'https://genius.com/One-direction-rock-me-lyrics',\n","       'https://genius.com/Au-revoir-simone-stars-lyrics',\n","       'https://genius.com/Green-day-brutal-love-lyrics',\n","       'https://genius.com/Abba-people-need-love-lyrics',\n","       'https://genius.com/Kane-brown-what-ifs-lyrics',\n","       'https://genius.com/Pinkpantheress-passion-lyrics',\n","       'https://genius.com/Whitney-houston-all-at-once-lyrics',\n","       'https://genius.com/April-wine-bad-side-of-the-moon-lyrics',\n","       'https://genius.com/Au-revoir-simone-shadows-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-bad-moon-rising-lyrics',\n","       'https://genius.com/Britney-spears-im-a-slave-4-u-lyrics',\n","       'https://genius.com/Isaac-dunbar-colony-lyrics',\n","       'https://genius.com/Kim-petras-fade-away-lyrics',\n","       'https://genius.com/Soccer-mommy-last-girl-lyrics',\n","       'https://genius.com/Tears-of-cherry-mona-lisa-lyrics',\n","       'https://genius.com/One-direction-what-makes-you-beautiful-lyrics',\n","       'https://genius.com/Jelly-roll-dead-man-walking-lyrics',\n","       'https://genius.com/Tears-for-fears-start-of-the-breakdown-lyrics',\n","       'https://genius.com/King-princess-1950-lyrics',\n","       'https://genius.com/Kristopher-singer-songwriter-into-you-lyrics',\n","       'https://genius.com/Kim-petras-everybody-dies-lyrics',\n","       'https://genius.com/Metallica-one-lyrics',\n","       'https://genius.com/Coldplay-and-bts-my-universe-lyrics',\n","       'https://genius.com/Whitney-houston-i-learned-from-the-best-lyrics',\n","       'https://genius.com/Kane-brown-homesick-lyrics',\n","       'https://genius.com/Coma-cinema-we-are-only-time-lyrics',\n","       'https://genius.com/Christian-leave-10-steps-lyrics',\n","       'https://genius.com/Ajr-humpty-dumpty-lyrics',\n","       'https://genius.com/Madonna-bye-bye-baby-first-day-ruff-lyrics',\n","       'https://genius.com/Imagine-dragons-hear-me-lyrics',\n","       'https://genius.com/Beabadoobee-beatopia-cultsong-lyrics',\n","       'https://genius.com/Suki-waterhouse-coolest-place-in-the-world-lyrics',\n","       'https://genius.com/Camera-obscura-razzle-dazzle-rose-lyrics',\n","       'https://genius.com/Mariah-carey-all-i-want-for-christmas-is-you-lyrics',\n","       'https://genius.com/Charli-xcx-break-the-rules-lyrics',\n","       'https://genius.com/Bebe-rexha-i-am-lyrics',\n","       'https://genius.com/Dayglow-deep-end-lyrics',\n","       'https://genius.com/Linkin-park-h-vltg3-lyrics',\n","       'https://genius.com/Britney-spears-mood-ring-by-demand-lyrics',\n","       'https://genius.com/Beabadoobee-are-you-sure-lyrics',\n","       'https://genius.com/Hippo-campus-i-got-time-lyrics',\n","       'https://genius.com/Madonna-erotica-lyrics',\n","       'https://genius.com/Dayglow-like-ivy-lyrics',\n","       'https://genius.com/Gracie-abrams-i-miss-you-im-sorry-lyrics',\n","       'https://genius.com/Pinkpantheress-boys-a-liar-lyrics',\n","       'https://genius.com/Post-malone-enemies-lyrics',\n","       'https://genius.com/Maggie-rogers-want-want-lyrics',\n","       'https://genius.com/Lil-baby-all-in-lyrics',\n","       'https://genius.com/Stephen-sanchez-this-thing-called-love-lyrics',\n","       'https://genius.com/Fleetwood-mac-man-of-the-world-lyrics',\n","       'https://genius.com/Hippo-campus-sex-tape-lyrics',\n","       'https://genius.com/Katy-perry-every-day-is-a-holiday-lyrics',\n","       'https://genius.com/Ashe-emotional-lyrics',\n","       'https://genius.com/Maroon-5-nothing-lasts-forever-lyrics',\n","       'https://genius.com/Tv-girl-one-of-these-mornings-lyrics',\n","       'https://genius.com/Justin-timberlake-cry-me-a-river-lyrics',\n","       'https://genius.com/Ashe-real-love-lyrics',\n","       'https://genius.com/Linkin-park-healing-foot-lyrics',\n","       'https://genius.com/Shakira-ciega-sordomuda-lyrics',\n","       'https://genius.com/Camera-obscura-french-navy-lyrics',\n","       'https://genius.com/Katy-perry-i-kissed-a-girl-lyrics',\n","       'https://genius.com/Mariah-carey-against-all-odds-lyrics',\n","       'https://genius.com/One-direction-kids-in-america-lyrics',\n","       'https://genius.com/Abba-se-me-esta-escapando-lyrics',\n","       'https://genius.com/Samia-nanana-lyrics',\n","       'https://genius.com/Billie-marten-live-lyrics',\n","       'https://genius.com/Cole-swindell-let-me-see-ya-girl-lyrics',\n","       'https://genius.com/Shakira-donde-estas-corazon-lyrics',\n","       'https://genius.com/Lovejoy-the-perfect-pair-spotify-singles-lyrics',\n","       'https://genius.com/Doja-cat-cyber-sex-lyrics',\n","       'https://genius.com/Christian-leave-companion-lyrics',\n","       'https://genius.com/Harry-styles-fine-line-lyrics',\n","       'https://genius.com/Britney-spears-dont-hang-up-lyrics',\n","       'https://genius.com/Taylor-swift-begin-again-lyrics',\n","       'https://genius.com/Ashe-sometimes-people-suck-lyrics',\n","       'https://genius.com/Kimbra-posse-brass-knuckles-remix-lyrics',\n","       'https://genius.com/Maggie-rogers-i-still-do-lyrics',\n","       'https://genius.com/Finneas-how-it-ends-lyrics',\n","       'https://genius.com/Sza-julia-lyrics',\n","       'https://genius.com/Maroon-5-dont-wanna-know-solo-version-lyrics',\n","       'https://genius.com/Maroon-5-tickets-lyrics',\n","       'https://genius.com/Slayyyter-everytime-lyrics',\n","       'https://genius.com/Sza-love-galore-version-2-lyrics',\n","       'https://genius.com/Haim-forever-lindstrm-and-prins-thomas-remix-lyrics',\n","       'https://genius.com/Luke-combs-beer-never-broke-my-heart-lyrics',\n","       'https://genius.com/Alexandra-savior-the-archer-lyrics',\n","       'https://genius.com/Syml-wheres-my-love-lyrics',\n","       'https://genius.com/Camera-obscura-honey-in-the-sun-lyrics',\n","       'https://genius.com/Imagine-dragons-30-lives-lyrics',\n","       'https://genius.com/Kimbra-old-flame-claude-vonstroke-remix-lyrics',\n","       'https://genius.com/Cults-bumper-lyrics',\n","       'https://genius.com/Sasha-alex-sloan-adult-lyrics',\n","       'https://genius.com/Kelly-clarkson-since-u-been-gone-lyrics',\n","       'https://genius.com/Kim-petras-heart-to-break-lyrics',\n","       'https://genius.com/Maggie-rogers-drunk-lyrics',\n","       'https://genius.com/Tessa-violet-bad-ideas-lo-fi-lyrics',\n","       'https://genius.com/One-direction-viva-la-vida-lyrics',\n","       'https://genius.com/Sza-f2f-lyrics',\n","       'https://genius.com/Haim-feel-the-thunder-lyrics',\n","       'https://genius.com/The-corrs-dreams-unplugged-version-lyrics',\n","       'https://genius.com/Imagine-dragons-follow-you-lyrics',\n","       'https://genius.com/Dustin-hulett-one-number-away-lyrics',\n","       'https://genius.com/Maggie-rogers-horses-lyrics',\n","       'https://genius.com/Mariah-carey-all-my-life-lyrics',\n","       'https://genius.com/Beabadoobee-the-way-things-go-lyrics',\n","       'https://genius.com/Lyn-lapid-cross-ur-mind-lyrics',\n","       'https://genius.com/One-direction-temporary-fix-lyrics',\n","       'https://genius.com/London-grammar-baby-its-you-lyrics',\n","       'https://genius.com/King-princess-forget-about-it-lyrics',\n","       'https://genius.com/Camera-obscura-other-towns-and-cities-lyrics',\n","       'https://genius.com/Ariana-grande-everyday-lyrics',\n","       'https://genius.com/Jontron-firework-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-miss-templetons-7000th-dream-lyrics',\n","       'https://genius.com/Chris-brown-body-shots-lyrics',\n","       'https://genius.com/Olivia-rodrigo-get-him-back-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-travelin-band-lyrics',\n","       'https://genius.com/Phoebe-bridgers-georgia-lyrics',\n","       'https://genius.com/Wham-everything-she-wants-lyrics',\n","       'https://genius.com/Linkin-park-carousel-xero-demo-unreleased-1999-demo-lyrics',\n","       'https://genius.com/Fujii-kaze-good-as-hell-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-where-are-all-the-scientists-now-lyrics',\n","       'https://genius.com/Kimbra-two-way-street-lyrics',\n","       'https://genius.com/Britney-spears-me-against-the-music-lyrics',\n","       'https://genius.com/Beabadoobee-coming-home-lyrics',\n","       'https://genius.com/Bleachers-i-wanna-get-better-lyrics',\n","       'https://genius.com/Isaac-dunbar-apartment-a-lyrics',\n","       'https://genius.com/Sabrina-carpenter-run-and-hide-lyrics',\n","       'https://genius.com/Abba-one-man-one-woman-lyrics',\n","       'https://genius.com/Whitney-houston-his-eye-is-on-the-sparrow-lyrics',\n","       'https://genius.com/Kimbra-rescue-him-lyrics',\n","       'https://genius.com/Nle-choppa-famous-hoes-lyrics',\n","       'https://genius.com/Post-malone-cooped-up-lyrics',\n","       'https://genius.com/Katy-perry-bon-appetit-lyrics',\n","       'https://genius.com/Maroon-5-dont-wanna-know-extended-version-lyrics',\n","       'https://genius.com/Girl-in-red-pick-me-lyrics',\n","       'https://genius.com/Cults-bad-things-remix-lyrics',\n","       'https://genius.com/The-beach-boys-getcha-back-lyrics',\n","       'https://genius.com/Britney-spears-breathe-on-me-lyrics',\n","       'https://genius.com/Lyn-lapid-east-side-lyrics',\n","       'https://genius.com/Bleachers-dont-go-dark-lyrics',\n","       'https://genius.com/Sza-garden-say-it-like-dat-lyrics',\n","       'https://genius.com/Maroon-5-doin-dirt-lyrics',\n","       'https://genius.com/Wet-love-heals-lyrics',\n","       'https://genius.com/Imagine-dragons-im-so-sorry-lyrics',\n","       'https://genius.com/Green-day-extraordinary-girl-lyrics',\n","       'https://genius.com/Sam-smith-and-normani-dancing-with-a-stranger-lyrics',\n","       'https://genius.com/Tessa-violet-you-are-not-my-friend-lyrics',\n","       'https://genius.com/Au-revoir-simone-dont-see-the-sorrow-keith-murray-remix-lyrics',\n","       'https://genius.com/Ed-sheeran-and-travis-scott-antisocial-lyrics',\n","       'https://genius.com/Au-revoir-simone-fallen-snow-the-teenagers-remix-lyrics',\n","       'https://genius.com/Bebe-rexha-better-mistakes-lyrics',\n","       'https://genius.com/Rex-orange-county-the-shade-lyrics',\n","       'https://genius.com/Britney-spears-and-backstreet-boys-matches-lyrics',\n","       'https://genius.com/David-guetta-dont-leave-me-alone-lyrics',\n","       'https://genius.com/Tennis-dimming-light-lyrics',\n","       'https://genius.com/Maroon-5-what-lovers-do-lyrics',\n","       'https://genius.com/Maggie-rogers-by-morning-lyrics',\n","       'https://genius.com/Justin-timberlake-better-not-together-lyrics',\n","       'https://genius.com/Tai-verdes-real-world-lyrics',\n","       'https://genius.com/Joshua-chew-break-my-heart-again-cover-lyrics',\n","       'https://genius.com/Ed-sheeran-best-part-of-me-lyrics',\n","       'https://genius.com/Bleachers-91-lyrics',\n","       'https://genius.com/Maroon-5-goodnight-goodnight-lyrics',\n","       'https://genius.com/King-princess-let-us-die-lyrics',\n","       'https://genius.com/Ice-spice-and-nicki-minaj-barbie-world-lyrics',\n","       'https://genius.com/The-beach-boys-county-fair-lyrics',\n","       'https://genius.com/Maggie-rogers-james-lyrics',\n","       'https://genius.com/Katy-perry-daisies-lyrics',\n","       'https://genius.com/Imagine-dragons-birds-lyrics',\n","       'https://genius.com/Samia-fit-n-full-lyrics',\n","       'https://genius.com/Christian-leave-safe-and-sound-lyrics',\n","       'https://genius.com/Isaac-dunbar-banish-the-banshee-lyrics',\n","       'https://genius.com/Haim-leaning-on-you-lyrics',\n","       'https://genius.com/Whitney-houston-so-emotional-lyrics',\n","       'https://genius.com/Tennis-i-miss-that-feeling-lyrics',\n","       'https://genius.com/Fleetwood-mac-caroline-lyrics',\n","       'https://genius.com/Au-revoir-simone-take-me-as-i-am-lyrics',\n","       'https://genius.com/Sam-smith-love-me-more-lyrics',\n","       'https://genius.com/Bebe-rexha-grace-lyrics',\n","       'https://genius.com/Post-malone-hateful-lyrics',\n","       'https://genius.com/Abba-so-long-lyrics',\n","       'https://genius.com/Between-friends-affection-lyrics',\n","       'https://genius.com/London-grammar-bones-of-ribbon-annotated',\n","       'https://genius.com/Gus-dapperton-post-humorous-lyrics',\n","       'https://genius.com/Sabrina-carpenter-slim-pickins-lyrics',\n","       'https://genius.com/Green-day-drama-queen-lyrics',\n","       'https://genius.com/Fleetwood-mac-little-lies-lyrics',\n","       'https://genius.com/Post-malone-ball-for-me-lyrics',\n","       'https://genius.com/Tears-for-fears-gods-mistake-lyrics',\n","       'https://genius.com/Bright-campa-loving-is-easy-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-porterville-lyrics',\n","       'https://genius.com/Metallica-lords-of-summer-lyrics',\n","       'https://genius.com/Meghan-trainor-champagne-problems-lyrics',\n","       'https://genius.com/Tv-girl-its-not-something-lyrics',\n","       'https://genius.com/Samia-stellate-lyrics',\n","       'https://genius.com/Ashe-i-hope-you-die-first-lyrics',\n","       'https://genius.com/Mariah-carey-heartbreaker-lyrics',\n","       'https://genius.com/Dua-lipa-levitating-lyrics',\n","       'https://genius.com/Sza-normal-girl-lyrics',\n","       'https://genius.com/Rex-orange-county-worth-it-lyrics',\n","       'https://genius.com/Christian-leave-hope-lyrics',\n","       'https://genius.com/Sam-smith-good-thing-lyrics',\n","       'https://genius.com/Chris-brown-anyway-lyrics',\n","       'https://genius.com/Metallica-all-within-my-hands-lyrics',\n","       'https://genius.com/Tears-for-fears-sketches-of-pain-lyrics',\n","       'https://genius.com/Sza-love-galore-alt-version-lyrics',\n","       'https://genius.com/Whitney-houston-fine-lyrics',\n","       'https://genius.com/Tears-for-fears-i-believe-a-soulful-re-recording-lyrics',\n","       'https://genius.com/Sza-doves-in-the-wind-remix-lyrics',\n","       'https://genius.com/One-direction-over-again-lyrics',\n","       'https://genius.com/Syml-girl-lyrics',\n","       'https://genius.com/Beabadoobee-pictures-of-us-lyrics',\n","       'https://genius.com/Syml-everything-all-at-once-lyrics',\n","       'https://genius.com/Au-revoir-simone-the-disco-song-lyrics',\n","       'https://genius.com/Madonna-holiday-lyrics',\n","       'https://genius.com/Luke-combs-when-it-rains-it-pours-lyrics',\n","       'https://genius.com/Between-friends-blushing-lyrics',\n","       'https://genius.com/Kimbra-the-way-we-were-lyrics',\n","       'https://genius.com/Tears-for-fears-break-it-down-again-lyrics',\n","       'https://genius.com/Tears-for-fears-fish-out-of-water-lyrics',\n","       'https://genius.com/Post-malone-i-like-you-a-happier-song-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-suzie-q-lyrics',\n","       'https://genius.com/Jordan-davis-singles-you-up-lyrics',\n","       'https://genius.com/Coi-leray-spend-it-lyrics',\n","       'https://genius.com/Suki-waterhouse-brutally-lyrics',\n","       'https://genius.com/Doja-cat-alone-lyrics',\n","       'https://genius.com/Whitney-houston-the-star-spangled-banner-live-from-super-bowl-xxv-lyrics',\n","       'https://genius.com/Linkin-park-friendly-fire-lyrics',\n","       'https://genius.com/Metallica-orion-lyrics',\n","       'https://genius.com/Pitbull-get-it-started-lyrics',\n","       'https://genius.com/The-beach-boys-add-some-music-to-your-day-lyrics',\n","       'https://genius.com/Between-friends-iloveyou-lyrics',\n","       'https://genius.com/Abba-dancing-queen-lyrics',\n","       'https://genius.com/Tyler-hubbard-5-foot-9-lyrics',\n","       'https://genius.com/King-princess-prophet-lyrics',\n","       'https://genius.com/Shakira-un-poco-de-amor-lyrics',\n","       'https://genius.com/Metallica-loverman-lyrics',\n","       'https://genius.com/Sza-awkward-lyrics',\n","       'https://genius.com/Lizzo-good-as-hell-lyrics',\n","       'https://genius.com/Morgan-wallen-one-thing-at-a-time-lyrics',\n","       'https://genius.com/Abba-rock-n-roll-band-lyrics',\n","       'https://genius.com/The-1975-jesus-christ-2005-god-bless-america-lyrics',\n","       'https://genius.com/Linkin-park-iridescent-lyrics',\n","       'https://genius.com/Haim-falling-duke-dumont-remix-lyrics',\n","       'https://genius.com/Ajr-big-idea-lyrics',\n","       'https://genius.com/Gus-dapperton-fill-me-up-anthem-lyrics',\n","       'https://genius.com/Madonna-bitch-im-madonna-lyrics',\n","       'https://genius.com/Carmen-twillie-and-lebo-m-circle-of-life-lyrics',\n","       'https://genius.com/Christian-leave-love-unfinished-lyrics',\n","       'https://genius.com/London-grammar-everyone-else-annotated',\n","       'https://genius.com/Panic-at-the-disco-idgaf-lyrics',\n","       'https://genius.com/Finneas-i-lost-a-friend-lyrics',\n","       'https://genius.com/Billie-marten-toulouse-lyrics',\n","       'https://genius.com/Linkin-park-esaul-xero-demo-lyrics',\n","       'https://genius.com/Jason-aldean-why-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-hey-tonight-lyrics',\n","       'https://genius.com/Glee-cast-all-i-want-for-christmas-is-you-lyrics',\n","       'https://genius.com/Fun-all-alright-lyrics',\n","       'https://genius.com/Ed-sheeran-celestial-lyrics',\n","       'https://genius.com/Tears-for-fears-the-hurting-lyrics',\n","       'https://genius.com/Tv-girl-not-allowed-lyrics',\n","       'https://genius.com/Camera-obscura-pop-goes-pop-lyrics',\n","       'https://genius.com/Cole-swindell-aint-worth-the-whiskey-lyrics',\n","       'https://genius.com/Metallica-hardwired-lyrics',\n","       'https://genius.com/Green-day-good-riddance-lyrics',\n","       'https://genius.com/Green-day-father-of-all-lyrics',\n","       'https://genius.com/Lyn-lapid-and-ruth-b-do-u-really-lyrics',\n","       'https://genius.com/Ajr-the-dj-is-crying-for-help-lyrics',\n","       'https://genius.com/Chelsea-cutler-marlboro-lights-lyrics',\n","       'https://genius.com/Gus-dapperton-i-have-lost-my-pearls-lyrics',\n","       'https://genius.com/Maroon-5-stutter-lyrics',\n","       'https://genius.com/The-beach-boys-california-girls-stereo-remix-lyrics',\n","       'https://genius.com/Justin-timberlake-drown-lyrics',\n","       'https://genius.com/Tv-girl-hate-yourself-lyrics',\n","       'https://genius.com/Finneas-happy-now-lyrics',\n","       'https://genius.com/Tv-girl-birds-dont-sing-lyrics',\n","       'https://genius.com/Chelsea-cutler-the-reason-lyrics',\n","       'https://genius.com/Au-revoir-simone-just-like-a-tree-lyrics',\n","       'https://genius.com/Imagine-dragons-fire-in-these-hills-lyrics',\n","       'https://genius.com/Whitney-houston-how-will-i-know-lyrics',\n","       'https://genius.com/Coma-cinema-burn-a-church-lyrics',\n","       'https://genius.com/Camera-obscura-i-dont-do-crowds-lyrics',\n","       'https://genius.com/Gus-dapperton-sockboy-lyrics',\n","       'https://genius.com/Tennis-wounded-heart-lyrics',\n","       'https://genius.com/Lyn-lapid-like-you-want-me-to-lyrics',\n","       'https://genius.com/Phoebe-bridgers-scott-street-lyrics',\n","       'https://genius.com/Coldplay-in-my-place-lyrics',\n","       'https://genius.com/Tv-girl-pantyhose-lyrics',\n","       'https://genius.com/The-beach-boys-girls-on-the-beach-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-door-to-door-live-lyrics',\n","       'https://genius.com/21-savage-lil-durk-and-metro-boomin-dangerous-lyrics',\n","       'https://genius.com/Hippo-campus-flood-lyrics',\n","       'https://genius.com/Lil-baby-and-fridayy-forever-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-the-one-lyrics',\n","       'https://genius.com/Lizzo-special-lyrics',\n","       'https://genius.com/Beabadoobee-glue-song-remix-lyrics',\n","       'https://genius.com/Ajr-sober-up-lyrics',\n","       'https://genius.com/Kelly-clarkson-all-i-ever-wanted-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-lighting-myself-on-fire-lyrics',\n","       'https://genius.com/Kimbra-home-lyrics',\n","       'https://genius.com/Tears-for-fears-secrets-lyrics',\n","       'https://genius.com/Tennis-fields-of-blue-lyrics',\n","       'https://genius.com/Bleachers-margo-lyrics',\n","       'https://genius.com/Suki-waterhouse-gateway-drug-lyrics',\n","       'https://genius.com/Maroon-5-harder-to-breathe-lyrics',\n","       'https://genius.com/Latto-big-mama-lyrics',\n","       'https://genius.com/Billie-eilish-all-the-good-girls-go-to-hell-lyrics',\n","       'https://genius.com/Lizzo-birthday-girl-lyrics',\n","       'https://genius.com/Morgan-wallen-warning-lyrics',\n","       'https://genius.com/Lovejoy-consequences-lyrics',\n","       'https://genius.com/Camera-obscura-cri-du-coeur-lyrics',\n","       'https://genius.com/Imagine-dragons-ill-make-it-up-to-you-lyrics',\n","       'https://genius.com/Shakira-and-grupo-frontera-entre-parentesis-lyrics',\n","       'https://genius.com/Steve-lacy-static-lyrics',\n","       'https://genius.com/Hippo-campus-mistakes-lyrics',\n","       'https://genius.com/Madonna-devil-pray-demo-lyrics',\n","       'https://genius.com/Ed-sheeran-cross-me-lyrics',\n","       'https://genius.com/Maggie-rogers-anywhere-with-you-lyrics',\n","       'https://genius.com/Ajr-the-dumb-song-lyrics',\n","       'https://genius.com/Kimbra-come-into-my-head-lyrics',\n","       'https://genius.com/King-princess-homegirl-lyrics',\n","       'https://genius.com/Fleetwood-mac-monday-morning-lyrics',\n","       'https://genius.com/Bleachers-big-life-live-at-radio-city-music-hall-lyrics',\n","       'https://genius.com/Beabadoobee-this-is-how-it-went-lyrics',\n","       'https://genius.com/Fleetwood-mac-dont-stop-lyrics',\n","       'https://genius.com/Sam-smith-lose-you-lyrics',\n","       'https://genius.com/Chelsea-cutler-you-are-losing-me-lyrics',\n","       'https://genius.com/Rex-orange-county-it-gets-better-lyrics',\n","       'https://genius.com/Jasmine-thompson-and-sabrina-carpenter-sign-of-the-times-lyrics',\n","       'https://genius.com/The-beach-boys-cotton-fields-the-cotton-song-lyrics',\n","       'https://genius.com/Lizzo-if-you-love-me-lyrics',\n","       'https://genius.com/Lil-baby-go-hard-lyrics',\n","       'https://genius.com/Selena-gomez-and-the-scene-love-you-like-a-love-song-lyrics',\n","       'https://genius.com/Alexandra-savior-mystery-girl-lyrics',\n","       'https://genius.com/Kelly-clarkson-already-gone-lyrics',\n","       'https://genius.com/The-beach-boys-california-dreamin-lyrics',\n","       'https://genius.com/21-savage-and-metro-boomin-glock-in-my-lap-lyrics',\n","       'https://genius.com/Luna-li-thats-life-lyrics',\n","       'https://genius.com/Charli-xcx-delicious-lyrics',\n","       'https://genius.com/Maggie-rogers-alaska-lyrics',\n","       'https://genius.com/Phoebe-bridgers-smoke-signals-lyrics',\n","       'https://genius.com/Kimbra-good-intent-lyrics',\n","       'https://genius.com/Hardy-quit-lyrics',\n","       'https://genius.com/Tennis-late-night-lyrics',\n","       'https://genius.com/Jukebox-the-ghost-everybodys-lonely-lyrics',\n","       'https://genius.com/Tyler-the-creator-48-lyrics',\n","       'https://genius.com/Frankie-cosmos-being-alive-lyrics',\n","       'https://genius.com/Fun-we-are-young-betatraxx-remix-lyrics',\n","       'https://genius.com/Shakira-rabiosa-lyrics',\n","       'https://genius.com/Fun-stitch-me-up-lyrics',\n","       'https://genius.com/Kimbra-sally-i-can-see-you-lyrics',\n","       'https://genius.com/Metallica-phantom-lord-lyrics',\n","       'https://genius.com/Suki-waterhouse-to-get-you-lyrics',\n","       'https://genius.com/David-guetta-love-is-gone-lyrics',\n","       'https://genius.com/Travis-scott-3500-lyrics',\n","       'https://genius.com/Madonna-bad-girl-lyrics',\n","       'https://genius.com/Finneas-break-my-heart-again-lyrics',\n","       'https://genius.com/Clairo-alewife-lyrics',\n","       'https://genius.com/David-guetta-becky-hill-and-ella-henderson-crazy-what-love-can-do-lyrics',\n","       'https://genius.com/Original-broadway-cast-of-the-lion-king-circle-of-life-broadway-version-lyrics',\n","       'https://genius.com/Luke-combs-better-together-lyrics',\n","       'https://genius.com/Elton-john-border-song-lyrics',\n","       'https://genius.com/Luke-combs-aint-no-love-in-oklahoma-lyrics',\n","       'https://genius.com/Tennis-no-exit-lyrics',\n","       'https://genius.com/Mariah-carey-and-whitney-houston-when-you-believe-lyrics',\n","       'https://genius.com/Syml-clean-eyes-lyrics',\n","       'https://genius.com/Bebe-rexha-i-got-you-lyrics',\n","       'https://genius.com/King-princess-pain-lyrics',\n","       'https://genius.com/Kim-petras-close-your-eyes-lyrics',\n","       'https://genius.com/Morgan-wallen-somebodys-problem-lyrics',\n","       'https://genius.com/Taylor-swift-bigger-than-the-whole-sky-lyrics',\n","       'https://genius.com/Bleachers-goodbye-lyrics',\n","       'https://genius.com/One-direction-infinity-lyrics',\n","       'https://genius.com/Mariah-carey-against-all-odds-take-a-look-at-me-now-lyrics',\n","       'https://genius.com/Lil-durk-broadway-girls-lyrics',\n","       'https://genius.com/Creedence-clearwater-revival-born-on-the-bayou-live-v2-lyrics',\n","       'https://genius.com/Parker-mccollum-handle-on-you-lyrics',\n","       'https://genius.com/Chelsea-cutler-stay-anything-lyrics',\n","       'https://genius.com/Christian-leave-superstar-lyrics',\n","       'https://genius.com/Soccer-mommy-yellow-is-the-color-of-her-eyes-lyrics'],\n","      dtype=object)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["\n","#find song id for begin again - https://genius.com/Taylor-swift-bigger-than-the-whole-sky-lyrics'\n","test_lyrics[test_lyrics['Lyrics URL'] == 'https://genius.com/Whitney-houston-saving-all-my-love-for-you-lyrics']\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150},"id":"56y6wrKLIZky","executionInfo":{"status":"ok","timestamp":1733643270261,"user_tz":480,"elapsed":446,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"1bb82c3a-6469-4335-83a0-0da77925d4e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Song ID                       Title  \\\n","1001  141615  Saving All My Love for You   \n","\n","                                             Lyrics URL  \\\n","1001  https://genius.com/Whitney-houston-saving-all-...   \n","\n","                                   Combined Annotations  \\\n","1001  “Saving All My Love for You” is a song written...   \n","\n","                                   Wikipedia Annotation  \\\n","1001  \"Saving All My Love for You\" is a song written...   \n","\n","                                                 Lyrics generated_annotation  \n","1001  A few stolen moments is all that we share\\r\\nY...                  NaN  "],"text/html":["\n","  <div id=\"df-23b7851e-b5f1-44e6-a1b2-bb078f52f634\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Song ID</th>\n","      <th>Title</th>\n","      <th>Lyrics URL</th>\n","      <th>Combined Annotations</th>\n","      <th>Wikipedia Annotation</th>\n","      <th>Lyrics</th>\n","      <th>generated_annotation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1001</th>\n","      <td>141615</td>\n","      <td>Saving All My Love for You</td>\n","      <td>https://genius.com/Whitney-houston-saving-all-...</td>\n","      <td>“Saving All My Love for You” is a song written...</td>\n","      <td>\"Saving All My Love for You\" is a song written...</td>\n","      <td>A few stolen moments is all that we share\\r\\nY...</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23b7851e-b5f1-44e6-a1b2-bb078f52f634')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-23b7851e-b5f1-44e6-a1b2-bb078f52f634 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-23b7851e-b5f1-44e6-a1b2-bb078f52f634');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["#run model eval on example song from test df\n","test_lyrics[test_lyrics['Song ID'] == 3244194]\n","metrics, examples = evaluate_combined_model(model, tokenizer, test_lyrics[test_lyrics['Song ID'] == 96432])\n","print_evaluation_results(metrics, examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUxVCVZfHQH6","executionInfo":{"status":"ok","timestamp":1733642387693,"user_tz":480,"elapsed":4142,"user":{"displayName":"Sahana Sankar","userId":"09821259821122047380"}},"outputId":"a3ec4690-a28b-44d5-c331-52574c21973b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 1/1 [00:03<00:00,  3.86s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results:\n","avg_content_coverage: 0.078\n","avg_semantic_similarity: 0.072\n","avg_rouge1: 0.242\n","avg_rouge2: 0.039\n","avg_rougeL: 0.166\n","avg_bert_score: 0.832\n","\n","Example Generations:\n","\n","Example 1:\n","Original Lyrics (truncated): Took a deep breath in the mirror He didnt like it when I wore high heels But I do Turn the lock and put my headphones on He always said he didnt get this song But I do, I do Walked in expecting youd b...\n","\n","Generated Summary: 'Standard' is a song about a girl's life in a cafe. It's the first time the song has been released. The song is based on a stanzas of the song.\n","\n","Metrics:\n","content_coverage: 0.078\n","semantic_similarity: 0.072\n","rouge1: 0.242\n","rouge2: 0.039\n","rougeL: 0.166\n","bert_score: 0.832\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["metrics, examples = evaluate_combined_model(model, tokenizer, test_lyrics)\n","print_evaluation_results(metrics, examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPik40cDzc9i","executionInfo":{"status":"ok","timestamp":1733505653910,"user_tz":480,"elapsed":669635,"user":{"displayName":"Chloe M","userId":"03955309346947875383"}},"outputId":"b5f2b39c-3ed1-442a-9140-8b07a4d81e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/40 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  2%|▎         | 1/40 [00:16<10:57, 16.86s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/40 [00:34<10:59, 17.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/40 [00:52<10:50, 17.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 10%|█         | 4/40 [01:09<10:20, 17.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 12%|█▎        | 5/40 [01:26<10:00, 17.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 15%|█▌        | 6/40 [01:43<09:49, 17.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/40 [02:00<09:20, 16.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 20%|██        | 8/40 [02:17<09:10, 17.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 22%|██▎       | 9/40 [02:34<08:49, 17.07s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 25%|██▌       | 10/40 [02:51<08:29, 17.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 28%|██▊       | 11/40 [03:07<08:08, 16.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 30%|███       | 12/40 [03:25<07:55, 16.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▎      | 13/40 [03:42<07:40, 17.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 35%|███▌      | 14/40 [04:00<07:33, 17.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 38%|███▊      | 15/40 [04:17<07:08, 17.12s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 40%|████      | 16/40 [04:34<06:55, 17.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▎     | 17/40 [04:52<06:38, 17.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▌     | 18/40 [05:08<06:11, 16.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 48%|████▊     | 19/40 [05:25<05:57, 17.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 20/40 [05:42<05:40, 17.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 52%|█████▎    | 21/40 [05:58<05:18, 16.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 22/40 [06:16<05:09, 17.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 57%|█████▊    | 23/40 [06:33<04:51, 17.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 60%|██████    | 24/40 [06:49<04:29, 16.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 62%|██████▎   | 25/40 [07:06<04:10, 16.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 65%|██████▌   | 26/40 [07:23<03:55, 16.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 27/40 [07:40<03:38, 16.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 70%|███████   | 28/40 [07:56<03:20, 16.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 72%|███████▎  | 29/40 [08:13<03:05, 16.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 75%|███████▌  | 30/40 [08:31<02:50, 17.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 78%|███████▊  | 31/40 [08:47<02:32, 16.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 80%|████████  | 32/40 [09:05<02:15, 16.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▎ | 33/40 [09:21<01:57, 16.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 85%|████████▌ | 34/40 [09:38<01:41, 16.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 88%|████████▊ | 35/40 [09:55<01:24, 16.94s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 90%|█████████ | 36/40 [10:12<01:07, 16.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▎| 37/40 [10:28<00:50, 16.68s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▌| 38/40 [10:45<00:33, 16.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 98%|█████████▊| 39/40 [11:03<00:17, 17.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 40/40 [11:09<00:00, 16.73s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results:\n","avg_content_coverage: 0.129\n","avg_semantic_similarity: 0.106\n","avg_rouge1: 0.215\n","avg_rouge2: 0.047\n","avg_rougeL: 0.141\n","avg_bert_score: 0.833\n","\n","Example Generations:\n","\n","Example 1:\n","Original Lyrics (truncated): (Someone for me) \n","(Someone for me) \n","I'm here alone on a Friday night \n","Waiting here beside the phone \n","The TV, radio, and me \n","Really ain't been getting along \n","\n","I wish that I could find a way \n","\n","To party ...\n","\n","Generated Summary: 'Songs' is one of the most popular songs of the year. The song is based on the lyrics of the song. It is a song about a young woman who loves to dance with a man who is not a fan of music, but a person who has a passion for music.\n","\n","Metrics:\n","content_coverage: 0.102\n","semantic_similarity: 0.086\n","rouge1: 0.159\n","rouge2: 0.000\n","rougeL: 0.091\n","bert_score: 0.836\n","\n","Example 2:\n","Original Lyrics (truncated): A few stolen moments is all that we share\r\n","You've got your family and they need you there. \n","Though I try to resist being last on your list. \n","But no other man's gonna do. \n","So I'm saving all my love for...\n","\n","Generated Summary: 'Darlin' is a song about a family member’s relationship with a friend. The song is based on a story of a man who loves to be a woman. The lyrics are a tribute to a couple of friends and family members.\n","\n","Metrics:\n","content_coverage: 0.100\n","semantic_similarity: 0.087\n","rouge1: 0.195\n","rouge2: 0.046\n","rougeL: 0.135\n","bert_score: 0.834\n","\n","Example 3:\n","Original Lyrics (truncated): What it is, ho? Whats up? Every good girl needs a little thug Every block boy needs a little love If you put it down, Ima pick it up, up, up Cant you just see, its just me and you? Panoramic view, tha...\n","\n","Generated Summary: . The song is based on the lyrics of the album, which was released on October 16, 2018. The song was originally titled ‘Hop Up’, and the song was titled 'Hop up’. It was originally written in a new album, titled “Hopup Up” and 'Opportunity Up’.\n","\n","Metrics:\n","content_coverage: 0.033\n","semantic_similarity: 0.030\n","rouge1: 0.000\n","rouge2: 0.000\n","rougeL: 0.000\n","bert_score: 0.787\n","\n","Example 4:\n","Original Lyrics (truncated): Im good, yeah, Im feelin alright Baby, Ima have the best fuckin night of my life And wherever it takes me, Im down for the ride Baby, dont you know Im good, yeah, Im feelin alright Cause Im good, yeah...\n","\n","Generated Summary: “It’s Good, yeah, Im feelin alright Baby” is a song that echoes the lyrics of the song. It’s a new song about the song, and the song is based on the song’s lyrics.\n","\n","Metrics:\n","content_coverage: 0.113\n","semantic_similarity: 0.094\n","rouge1: 0.286\n","rouge2: 0.091\n","rougeL: 0.196\n","bert_score: 0.858\n","\n","Example 5:\n","Original Lyrics (truncated): Going far, getting nowhere\r\n","Going far, the way you are\r\n","Going far, getting nowhere\r\n","Going far, the way you are\n","\n","Going far, getting nowhere\n","\n","The way you are\n","\n","\n","\n","Going far, getting nowhere\n","\n","The way you a...\n","\n","Generated Summary: 'The Way You Are Going far, getting nowhere Going far. The way you are Going far is going far. It's going far far, the way you're Going far and getting nowhere. The song begins with a stanza that focuses on the lyrics of the song.\n","\n","Metrics:\n","content_coverage: 0.229\n","semantic_similarity: 0.172\n","rouge1: 0.231\n","rouge2: 0.112\n","rougeL: 0.191\n","bert_score: 0.813\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# Old way\n"],"metadata":{"id":"6Z1MXMy17-f2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":455,"referenced_widgets":["37d5862988274dfab1ae850de6a65cba","b5044d46be50462f8c85181b756b0e54","9fbce32ae0e14f9d806d8c2efdc2c2d8","2680285991af4350941d6be0110f7e61","91dfe46a33f84cf3902e377acee4aaa9","bb3934249665409d9b9ab155b7af22ea","0cd63cd74f1d4fa797e01fff05b25fed","cc289f3a8e38441291c48e5dd569e114","6b63bb3a641b4a138a2671acdbc67c40","93a84430121948af945c3c6773893c26","5df1fabd49f54bb5a1edd5f5eb613536","6747504b14c04d19b6aad9281a9114c1","b5a725d369fd4f9687e663d46b4e3a38","10f1fed72f1b4d4993f30bd375d54cf6","a16711800eea4c27a28ddf526f572979","9edd9fe6e3054e4d8e0046cd9fcdc86d","379b309902fd429dad0b27cef6237339","9a8942ad0ef841159bb39893a5667d3c","defdeb9c7adb4e3ba84b659edcd1e68a","bde2fd8a99564570889ec8656feed599","55d00e13126147dab0f81fc94f786b90","1daac04e74ca424589a8fec76a9e39ea","bd173f7fe5494ee2ae9e5e173cbf7ae2","71791bd524bd4c748e2c85606ad168eb","b4419d0f73044a51a392e6db6a7a5498","161f0ae414be471ba5f0d45e6915267b","b9b78ea979684754b06e454bb56bf936","727ab4aa20e74db2a9206b27358ae476","2442eabb81584dea850e594b209d6fc5","d47160ac27f74be48681351e9f369496","ac3de79ec3a54abb89273afacca92091","e8b9cdb6d620490e9b2df1e9de029105","9e3216dee31546b2a037f103ee463395","58788b7e0adf476fa49296cb0bcf2d24","a9ab55c15b9a4f1eb37653ab41234bce","a8268dbcd2ad403f854165bd6a841a81","39e8d5b8494c47138acecc7dab523855","e699c0f1c10c44a9bf2929152c856d42","c13fa3c0a0f044a184bc8d8aae0f005e","fd5193ef8b4342be89a88b3825d503bb","ba8d1d33021b49718130bddb032612fb","d7161c89dff64e02ac91607a8e346b22","4cb08a2f1bf040d5b39c81e97dbc7c60","56def7c5cd8b4c9da5d61ca10cfaca4d","7b0626f3be414bd0a41da5fe386fd3fe","717118aaea9f4847b80a821cce99821c","6f1876164760489a8207b9468c6479dd","7bff4a81cf46475b929bc6592c2e4161","7606289d67a747b98f3f707af238bdee","de342fedd2384f19902e4465cee106d4","86d0c30ee992459dae44b579f0579fd4","0f9263259c3946b8bcb91efebb17f6cb","1e13aac03ad14ba892d61b67c7f0cfc3","952a62fcebd649d6b2dc88e3bee069d3","6160b7c75f3e4ec7ac842acafb840fab"]},"id":"o3lBzoH2Eaui","outputId":"f72c4de8-607b-4442-8c8c-9ee5fd587e4f"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/75 [00:00<?, ?it/s]\u001b[A"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d5862988274dfab1ae850de6a65cba","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6747504b14c04d19b6aad9281a9114c1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd173f7fe5494ee2ae9e5e173cbf7ae2","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58788b7e0adf476fa49296cb0bcf2d24","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b0626f3be414bd0a41da5fe386fd3fe","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  1%|▏         | 1/75 [10:12<12:35:37, 612.67s/it]\u001b[A\n","  3%|▎         | 2/75 [18:38<11:08:54, 549.79s/it]\u001b[A\n","  4%|▍         | 3/75 [24:56<9:25:33, 471.30s/it] \u001b[A\n","  5%|▌         | 4/75 [33:27<9:36:25, 487.12s/it]\u001b[A\n","  7%|▋         | 5/75 [43:16<10:11:04, 523.77s/it]\u001b[A\n","  8%|▊         | 6/75 [53:02<10:26:29, 544.77s/it]\u001b[A\n","  9%|▉         | 7/75 [1:01:58<10:14:12, 541.95s/it]\u001b[A\n"," 11%|█         | 8/75 [1:11:18<10:11:42, 547.80s/it]\u001b[A\n"," 12%|█▏        | 9/75 [1:17:29<9:01:47, 492.53s/it] \u001b[A\n"," 13%|█▎        | 10/75 [1:27:37<9:32:20, 528.31s/it]\u001b[A\n"," 15%|█▍        | 11/75 [1:39:21<10:20:52, 582.07s/it]\u001b[A\n"," 16%|█▌        | 12/75 [1:46:23<9:20:04, 533.40s/it] \u001b[A\n"," 17%|█▋        | 13/75 [1:55:44<9:19:42, 541.65s/it]\u001b[A\n"," 19%|█▊        | 14/75 [2:03:07<8:40:19, 511.80s/it]\u001b[A\n"," 20%|██        | 15/75 [2:12:26<8:46:09, 526.17s/it]\u001b[A\n"," 21%|██▏       | 16/75 [2:21:56<8:50:10, 539.16s/it]\u001b[A"]}],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from typing import List, Dict, Tuple\n","from transformers import T5Tokenizer\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","from tqdm import tqdm\n","\n","def evaluate_self_supervised_model(\n","    model: LyricsSummaryModel,\n","    tokenizer: T5Tokenizer,\n","    test_data: pd.DataFrame,\n","    batch_size: int = 8\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","    \"\"\"\n","    Evaluate self-supervised lyrics model with the following metrics:\n","    1. Consistency of outputs\n","    2. Coverage of key lyrics content\n","    3. Semantic similarity within lyrics context\n","    4. BERTScore for semantic evaluation\n","    \"\"\"\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    evaluation_results = {\n","        'content_coverage': [],\n","        'consistency_score': [],\n","        'semantic_similarity': [],\n","        'bert_score': []\n","    }\n","\n","    examples = []\n","\n","    for idx in tqdm(range(0, len(test_data), batch_size)):\n","        batch_lyrics = test_data['Lyrics'].iloc[idx:idx + batch_size].tolist()\n","\n","        # Generate multiple summaries for each lyric to test consistency\n","        summaries_per_lyric = []\n","        for _ in range(3):  # Generate 3 summaries per lyric\n","            inputs = tokenizer(\n","                [f\"summarize lyrics: {lyric}\" for lyric in batch_lyrics],\n","                padding=True,\n","                truncation=True,\n","                max_length=512,\n","                return_tensors=\"pt\"\n","            ).to(device)\n","\n","            with torch.no_grad():\n","                outputs = model.model.generate(\n","                    input_ids=inputs['input_ids'],\n","                    attention_mask=inputs['attention_mask'],\n","                    max_length=100,\n","                    min_length=30,\n","                    num_beams=4,\n","                    do_sample=True,\n","                    temperature=0.3,\n","                    top_k=50,\n","                    no_repeat_ngram_size=3,\n","                    length_penalty=0.8,\n","                    repetition_penalty=1.5\n","                )\n","\n","                decoded_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","                summaries_per_lyric.append(decoded_summaries)\n","\n","        # Evaluate each lyric's summaries\n","        for lyric_idx in range(len(batch_lyrics)):\n","            original_lyric = batch_lyrics[lyric_idx]\n","            lyric_summaries = [summaries[lyric_idx] for summaries in summaries_per_lyric]\n","\n","            # 1. Content Coverage Score\n","            coverage_score = calculate_content_coverage(original_lyric, lyric_summaries[0])\n","            evaluation_results['content_coverage'].append(coverage_score)\n","\n","            # 2. Consistency Score across multiple generations\n","            consistency_score = calculate_consistency_score(lyric_summaries)\n","            evaluation_results['consistency_score'].append(consistency_score)\n","\n","            # 3. Semantic Similarity\n","            semantic_score = calculate_semantic_similarity(original_lyric, lyric_summaries[0])\n","            evaluation_results['semantic_similarity'].append(semantic_score)\n","\n","            # 4. BERTScore\n","            P, R, F1 = score(\n","                [lyric_summaries[0]],\n","                [original_lyric],\n","                model_type=\"microsoft/deberta-xlarge-mnli\",\n","                device=device\n","            )\n","            evaluation_results['bert_score'].append(F1.mean().item())\n","\n","            # Store examples\n","            if len(examples) < 5:  # Store first 5 examples\n","                examples.append({\n","                    'lyrics': original_lyric,\n","                    'generated_summaries': lyric_summaries,\n","                    'metrics': {\n","                        'content_coverage': coverage_score,\n","                        'consistency': consistency_score,\n","                        'semantic_similarity': semantic_score,\n","                        'bert_score': F1.mean().item()\n","                    }\n","                })\n","\n","        torch.cuda.empty_cache()\n","\n","    # Aggregate results\n","    metrics = {\n","        'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","        'avg_consistency': np.mean(evaluation_results['consistency_score']),\n","        'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","        'avg_bert_score': np.mean(evaluation_results['bert_score'])\n","    }\n","\n","    return metrics, examples\n","\n","\n","def calculate_content_coverage(lyrics: str, summary: str) -> float:\n","    lyrics_tokens = set(lyrics.lower().split())\n","    summary_tokens = set(summary.lower().split())\n","    overlap = len(lyrics_tokens.intersection(summary_tokens))\n","    coverage = overlap / len(lyrics_tokens)\n","    return coverage\n","\n","\n","def calculate_consistency_score(summaries: List[str]) -> float:\n","    if len(summaries) < 2:\n","        return 1.0\n","\n","    rouge_scorer_obj = rouge_scorer.RougeScorer(\n","        ['rouge1', 'rouge2', 'rougeL'],\n","        use_stemmer=True\n","    )\n","\n","    scores = []\n","    for i in range(len(summaries)):\n","        for j in range(i + 1, len(summaries)):\n","            score = rouge_scorer_obj.score(summaries[i], summaries[j])\n","            avg_score = (\n","                score['rouge1'].fmeasure +\n","                score['rouge2'].fmeasure +\n","                score['rougeL'].fmeasure\n","            ) / 3\n","            scores.append(avg_score)\n","\n","    return np.mean(scores)\n","\n","\n","def calculate_semantic_similarity(lyrics: str, summary: str) -> float:\n","    lyrics_tokens = set(lyrics.lower().split())\n","    summary_tokens = set(summary.lower().split())\n","    intersection = len(lyrics_tokens.intersection(summary_tokens))\n","    union = len(lyrics_tokens.union(summary_tokens))\n","    return intersection / union if union > 0 else 0.0\n","\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","    print(\"\\nEvaluation Results:\")\n","    print(f\"Average Content Coverage: {metrics['avg_content_coverage']:.3f}\")\n","    print(f\"Average Consistency: {metrics['avg_consistency']:.3f}\")\n","    print(f\"Average Semantic Similarity: {metrics['avg_semantic_similarity']:.3f}\")\n","    print(f\"Average BERTScore: {metrics['avg_bert_score']:.3f}\")\n","\n","    print(\"\\nExample Generations:\")\n","    for i, example in enumerate(examples, 1):\n","        print(f\"\\nExample {i}:\")\n","        print(f\"Original Lyrics (truncated): {example['lyrics'][:200]}...\")\n","        print(\"\\nGenerated Summaries:\")\n","        for j, summary in enumerate(example['generated_summaries'], 1):\n","            print(f\"{j}. {summary}\")\n","        print(\"\\nMetrics:\")\n","        for metric, value in example['metrics'].items():\n","            print(f\"{metric}: {value:.3f}\")\n","\n","\n","# Usage example\n","metrics, examples = evaluate_self_supervised_model(\n","    model,\n","    tokenizer,\n","    test_data=test_df,\n","    batch_size=8\n",")\n","\n","print_evaluation_results(metrics, examples)\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["jGh0woF1ovCY","fsImZjvdsOdx","fS73Kv92bvWm","6Z1MXMy17-f2"],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0cd63cd74f1d4fa797e01fff05b25fed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f9263259c3946b8bcb91efebb17f6cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10f1fed72f1b4d4993f30bd375d54cf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_defdeb9c7adb4e3ba84b659edcd1e68a","max":792,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bde2fd8a99564570889ec8656feed599","value":792}},"1228160cecba4967857b8977d53a407d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13de83c6065748e28b804279a076ff95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1228160cecba4967857b8977d53a407d","placeholder":"​","style":"IPY_MODEL_71b8946eff0f4e768747765a60dccac0","value":" 1/2 [00:03&lt;00:03,  0.31it/s]"}},"161f0ae414be471ba5f0d45e6915267b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8b9cdb6d620490e9b2df1e9de029105","placeholder":"​","style":"IPY_MODEL_9e3216dee31546b2a037f103ee463395","value":" 899k/899k [00:00&lt;00:00, 17.6MB/s]"}},"1daac04e74ca424589a8fec76a9e39ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e13aac03ad14ba892d61b67c7f0cfc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2442eabb81584dea850e594b209d6fc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2680285991af4350941d6be0110f7e61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93a84430121948af945c3c6773893c26","placeholder":"​","style":"IPY_MODEL_5df1fabd49f54bb5a1edd5f5eb613536","value":" 52.0/52.0 [00:00&lt;00:00, 2.70kB/s]"}},"379b309902fd429dad0b27cef6237339":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d5862988274dfab1ae850de6a65cba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5044d46be50462f8c85181b756b0e54","IPY_MODEL_9fbce32ae0e14f9d806d8c2efdc2c2d8","IPY_MODEL_2680285991af4350941d6be0110f7e61"],"layout":"IPY_MODEL_91dfe46a33f84cf3902e377acee4aaa9"}},"39e8d5b8494c47138acecc7dab523855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb08a2f1bf040d5b39c81e97dbc7c60","placeholder":"​","style":"IPY_MODEL_56def7c5cd8b4c9da5d61ca10cfaca4d","value":" 456k/456k [00:00&lt;00:00, 20.8MB/s]"}},"4252c4af33fd4aa2bd528c9bbb4b6e6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb08a2f1bf040d5b39c81e97dbc7c60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55d00e13126147dab0f81fc94f786b90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56def7c5cd8b4c9da5d61ca10cfaca4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58788b7e0adf476fa49296cb0bcf2d24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9ab55c15b9a4f1eb37653ab41234bce","IPY_MODEL_a8268dbcd2ad403f854165bd6a841a81","IPY_MODEL_39e8d5b8494c47138acecc7dab523855"],"layout":"IPY_MODEL_e699c0f1c10c44a9bf2929152c856d42"}},"5df1fabd49f54bb5a1edd5f5eb613536":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6160b7c75f3e4ec7ac842acafb840fab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6747504b14c04d19b6aad9281a9114c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5a725d369fd4f9687e663d46b4e3a38","IPY_MODEL_10f1fed72f1b4d4993f30bd375d54cf6","IPY_MODEL_a16711800eea4c27a28ddf526f572979"],"layout":"IPY_MODEL_9edd9fe6e3054e4d8e0046cd9fcdc86d"}},"67a424d65b90482bb4318cd7fe176e7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b63bb3a641b4a138a2671acdbc67c40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f1876164760489a8207b9468c6479dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f9263259c3946b8bcb91efebb17f6cb","max":3035556896,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e13aac03ad14ba892d61b67c7f0cfc3","value":3035556896}},"717118aaea9f4847b80a821cce99821c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de342fedd2384f19902e4465cee106d4","placeholder":"​","style":"IPY_MODEL_86d0c30ee992459dae44b579f0579fd4","value":"pytorch_model.bin: 100%"}},"71791bd524bd4c748e2c85606ad168eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_727ab4aa20e74db2a9206b27358ae476","placeholder":"​","style":"IPY_MODEL_2442eabb81584dea850e594b209d6fc5","value":"vocab.json: 100%"}},"71b8946eff0f4e768747765a60dccac0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"727ab4aa20e74db2a9206b27358ae476":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7606289d67a747b98f3f707af238bdee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b0626f3be414bd0a41da5fe386fd3fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_717118aaea9f4847b80a821cce99821c","IPY_MODEL_6f1876164760489a8207b9468c6479dd","IPY_MODEL_7bff4a81cf46475b929bc6592c2e4161"],"layout":"IPY_MODEL_7606289d67a747b98f3f707af238bdee"}},"7bff4a81cf46475b929bc6592c2e4161":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_952a62fcebd649d6b2dc88e3bee069d3","placeholder":"​","style":"IPY_MODEL_6160b7c75f3e4ec7ac842acafb840fab","value":" 3.04G/3.04G [00:40&lt;00:00, 152MB/s]"}},"86d0c30ee992459dae44b579f0579fd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91dfe46a33f84cf3902e377acee4aaa9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a84430121948af945c3c6773893c26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"952a62fcebd649d6b2dc88e3bee069d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"982d6fb31d4345668f1f1c00f32c2edb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3cc5bc5a1a340c3b38952823566b75a","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67a424d65b90482bb4318cd7fe176e7c","value":1}},"9a8942ad0ef841159bb39893a5667d3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e3216dee31546b2a037f103ee463395":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9edd9fe6e3054e4d8e0046cd9fcdc86d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fbce32ae0e14f9d806d8c2efdc2c2d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc289f3a8e38441291c48e5dd569e114","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b63bb3a641b4a138a2671acdbc67c40","value":52}},"a16711800eea4c27a28ddf526f572979":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55d00e13126147dab0f81fc94f786b90","placeholder":"​","style":"IPY_MODEL_1daac04e74ca424589a8fec76a9e39ea","value":" 792/792 [00:00&lt;00:00, 47.9kB/s]"}},"a8268dbcd2ad403f854165bd6a841a81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba8d1d33021b49718130bddb032612fb","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7161c89dff64e02ac91607a8e346b22","value":456318}},"a9ab55c15b9a4f1eb37653ab41234bce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c13fa3c0a0f044a184bc8d8aae0f005e","placeholder":"​","style":"IPY_MODEL_fd5193ef8b4342be89a88b3825d503bb","value":"merges.txt: 100%"}},"ac3de79ec3a54abb89273afacca92091":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad7341e003e84a49911e68a753e9a0ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b4419d0f73044a51a392e6db6a7a5498":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d47160ac27f74be48681351e9f369496","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac3de79ec3a54abb89273afacca92091","value":898825}},"b5044d46be50462f8c85181b756b0e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb3934249665409d9b9ab155b7af22ea","placeholder":"​","style":"IPY_MODEL_0cd63cd74f1d4fa797e01fff05b25fed","value":"tokenizer_config.json: 100%"}},"b5a725d369fd4f9687e663d46b4e3a38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_379b309902fd429dad0b27cef6237339","placeholder":"​","style":"IPY_MODEL_9a8942ad0ef841159bb39893a5667d3c","value":"config.json: 100%"}},"b9b78ea979684754b06e454bb56bf936":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba8d1d33021b49718130bddb032612fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb3934249665409d9b9ab155b7af22ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd173f7fe5494ee2ae9e5e173cbf7ae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71791bd524bd4c748e2c85606ad168eb","IPY_MODEL_b4419d0f73044a51a392e6db6a7a5498","IPY_MODEL_161f0ae414be471ba5f0d45e6915267b"],"layout":"IPY_MODEL_b9b78ea979684754b06e454bb56bf936"}},"bde2fd8a99564570889ec8656feed599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c13fa3c0a0f044a184bc8d8aae0f005e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c737e24f09ac4837a13f0e010a65272d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc289f3a8e38441291c48e5dd569e114":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0b24ebc7c31483bb8d1420e9e5ddf4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c737e24f09ac4837a13f0e010a65272d","placeholder":"​","style":"IPY_MODEL_4252c4af33fd4aa2bd528c9bbb4b6e6d","value":"Sanity Checking DataLoader 0:  50%"}},"d47160ac27f74be48681351e9f369496":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7161c89dff64e02ac91607a8e346b22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de342fedd2384f19902e4465cee106d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"defdeb9c7adb4e3ba84b659edcd1e68a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3cc5bc5a1a340c3b38952823566b75a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e699c0f1c10c44a9bf2929152c856d42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8b9cdb6d620490e9b2df1e9de029105":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1deaa9dd3df4992a06d59e0befce881":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0b24ebc7c31483bb8d1420e9e5ddf4d","IPY_MODEL_982d6fb31d4345668f1f1c00f32c2edb","IPY_MODEL_13de83c6065748e28b804279a076ff95"],"layout":"IPY_MODEL_ad7341e003e84a49911e68a753e9a0ea"}},"fd5193ef8b4342be89a88b3825d503bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81b99cb9486e47d2b8d46d69f0168957":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc6cfe2d1a9c4f3385bdd8f6aa8e7388","IPY_MODEL_7553a7fed094460e87d8100f553dc99b","IPY_MODEL_51ce57c11c264d0e96121eb2521e1be8"],"layout":"IPY_MODEL_8f767507fbce4742a9a256bdd209ae25"}},"cc6cfe2d1a9c4f3385bdd8f6aa8e7388":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc713056af4a4353bbed3a7e9d6fa4a5","placeholder":"​","style":"IPY_MODEL_5d05a98afe3f4c7092061c3edf6cc5b9","value":"Sanity Checking DataLoader 0: 100%"}},"7553a7fed094460e87d8100f553dc99b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa555b44d92a4c47921e73ba14c01eac","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55902086704f4af98d0ca7166772582d","value":2}},"51ce57c11c264d0e96121eb2521e1be8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e956e9e659bc47f0a6b0e0874933e483","placeholder":"​","style":"IPY_MODEL_41b5e62784a1460f90dff82364320f10","value":" 2/2 [00:00&lt;00:00, 17.89it/s]"}},"8f767507fbce4742a9a256bdd209ae25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"bc713056af4a4353bbed3a7e9d6fa4a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d05a98afe3f4c7092061c3edf6cc5b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa555b44d92a4c47921e73ba14c01eac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55902086704f4af98d0ca7166772582d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e956e9e659bc47f0a6b0e0874933e483":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b5e62784a1460f90dff82364320f10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ede9f856e4147309c06853bdd522122":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fa4941ac6064416ae538e7337be96f0","IPY_MODEL_e2cacdf1e2564dcf953e2eb81b91da9e","IPY_MODEL_fceac39341374e7685dd55afc829e90f"],"layout":"IPY_MODEL_00fb315e0f474a90a93c1ab3465eaffd"}},"5fa4941ac6064416ae538e7337be96f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_740e6ef305e74ef89cb1af3e2d83106b","placeholder":"​","style":"IPY_MODEL_6adfe37a597e4075af11d391e4b933d9","value":"Epoch 2: 100%"}},"e2cacdf1e2564dcf953e2eb81b91da9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a02ce4aef3945648190d83ef374c6ef","max":1165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04337c2ef44847c09c7e5a3351b1726b","value":1165}},"fceac39341374e7685dd55afc829e90f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_191e251105bc42e59a85754e3c8f5dcf","placeholder":"​","style":"IPY_MODEL_e01348e4464a4184b8d9b7a903ea3c9d","value":" 1165/1165 [04:52&lt;00:00,  3.98it/s, v_num=1, train_loss=3.080, val_loss=3.620]"}},"00fb315e0f474a90a93c1ab3465eaffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"740e6ef305e74ef89cb1af3e2d83106b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6adfe37a597e4075af11d391e4b933d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a02ce4aef3945648190d83ef374c6ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04337c2ef44847c09c7e5a3351b1726b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"191e251105bc42e59a85754e3c8f5dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e01348e4464a4184b8d9b7a903ea3c9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5de8c30f91f947b2bb0a5a4a4a54ebd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74b0db8b7cb9467f9369c511f4d24c87","IPY_MODEL_abf04a00355b41eab426e1c133329046","IPY_MODEL_02590ca280d144988f4943000179af2d"],"layout":"IPY_MODEL_b31c662ee4d34fbf9162048a90cedaa7"}},"74b0db8b7cb9467f9369c511f4d24c87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc0e7b3017484db295a8ef16473798e0","placeholder":"​","style":"IPY_MODEL_f9c2e1c574684b4c8809cc3bd1cdeac3","value":"Validation DataLoader 0: 100%"}},"abf04a00355b41eab426e1c133329046":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_82336a5912d1444e9a332fb25d641d1f","max":138,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc34902cfb4945119369a2997560a67a","value":138}},"02590ca280d144988f4943000179af2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d6084aeae846c3b14c285435a442c3","placeholder":"​","style":"IPY_MODEL_1839082a8a7f48419837d89ccbcb3435","value":" 138/138 [00:12&lt;00:00, 10.77it/s]"}},"b31c662ee4d34fbf9162048a90cedaa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"fc0e7b3017484db295a8ef16473798e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9c2e1c574684b4c8809cc3bd1cdeac3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82336a5912d1444e9a332fb25d641d1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc34902cfb4945119369a2997560a67a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52d6084aeae846c3b14c285435a442c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1839082a8a7f48419837d89ccbcb3435":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59ee72774c7d40f79b96b474366b345f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52e63309627f49c6af4ed56e30d52236","IPY_MODEL_57d80fc928e340c6ba3d6d93cbf0f090","IPY_MODEL_0248b07362974dfdb1d1015131dc6b84"],"layout":"IPY_MODEL_b170aaf5af4049a390cc186444b0d245"}},"52e63309627f49c6af4ed56e30d52236":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9e97fa3ab424ee5ad50ac2c316d4c94","placeholder":"​","style":"IPY_MODEL_7b2c2a16ad8e420ca26f0691bc726892","value":"Validation DataLoader 0: 100%"}},"57d80fc928e340c6ba3d6d93cbf0f090":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf06dd7a72aa4c2f8792672a5dc02c17","max":138,"min":0,"orientation":"horizontal","style":"IPY_MODEL_637ef16ab5d24ebd94297331861b2c8c","value":138}},"0248b07362974dfdb1d1015131dc6b84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e51c89c9bbc42df9ebb0aeb1d178ad5","placeholder":"​","style":"IPY_MODEL_3b1b229b6d6d4541ac578e2dc440bc15","value":" 138/138 [00:12&lt;00:00, 10.77it/s]"}},"b170aaf5af4049a390cc186444b0d245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"b9e97fa3ab424ee5ad50ac2c316d4c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b2c2a16ad8e420ca26f0691bc726892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf06dd7a72aa4c2f8792672a5dc02c17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"637ef16ab5d24ebd94297331861b2c8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e51c89c9bbc42df9ebb0aeb1d178ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b1b229b6d6d4541ac578e2dc440bc15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e915324061c547a8ab5b150865aa908b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_710779ecd118438bb8b10d26980f5731","IPY_MODEL_965520ca605642c5a7fea06c7bf8eecd","IPY_MODEL_5b86b6a1d58a43848999dee2b7946d24"],"layout":"IPY_MODEL_f23855cc39814d3cb78345905ce994b9"}},"710779ecd118438bb8b10d26980f5731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f01406253f824f298ec74c20dc877ded","placeholder":"​","style":"IPY_MODEL_a46e6a699a314cf293f3c4ff443c6082","value":"Validation DataLoader 0: 100%"}},"965520ca605642c5a7fea06c7bf8eecd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ba8e4d5928d4a53b6a7865174907eea","max":138,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4214294f538f49f18dcf23b5a2425bde","value":138}},"5b86b6a1d58a43848999dee2b7946d24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_670c13365a3044a8818c0f0b645b2103","placeholder":"​","style":"IPY_MODEL_09bfdb93a366480881f016b550a61ab1","value":" 138/138 [00:12&lt;00:00, 10.77it/s]"}},"f23855cc39814d3cb78345905ce994b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"f01406253f824f298ec74c20dc877ded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46e6a699a314cf293f3c4ff443c6082":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ba8e4d5928d4a53b6a7865174907eea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4214294f538f49f18dcf23b5a2425bde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"670c13365a3044a8818c0f0b645b2103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09bfdb93a366480881f016b550a61ab1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}