{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"68f85296ce91429b87a8e8ac239b9c22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f5fe20e905145f49e7fc057f553e82e","IPY_MODEL_dd0939540863411e81579622819f579c","IPY_MODEL_8cc80cf4c2a641e7a715b6f3295862c1"],"layout":"IPY_MODEL_c3891c72d2844331a225ec4cd0024332"}},"9f5fe20e905145f49e7fc057f553e82e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d04d00b94a874e689621394ad560c65a","placeholder":"​","style":"IPY_MODEL_46c560ca9fb649b4bbbebc78930ef951","value":"Sanity Checking DataLoader 0: 100%"}},"dd0939540863411e81579622819f579c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_46a771836bce4670b8775e658bb55dbb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1677988c9e747639cac087311805dae","value":2}},"8cc80cf4c2a641e7a715b6f3295862c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7114a7fcc1ea4eea867d741ef6f5e762","placeholder":"​","style":"IPY_MODEL_b649b6baf1744ef1a90e4ab78312b98c","value":" 2/2 [00:01&lt;00:00,  1.04it/s]"}},"c3891c72d2844331a225ec4cd0024332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"d04d00b94a874e689621394ad560c65a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46c560ca9fb649b4bbbebc78930ef951":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46a771836bce4670b8775e658bb55dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1677988c9e747639cac087311805dae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7114a7fcc1ea4eea867d741ef6f5e762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b649b6baf1744ef1a90e4ab78312b98c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30893a4d61c547b6bac48d2fd81be4da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae64a788be92424996b1700e10cde4b3","IPY_MODEL_57960d2d5e0a47dea5e9ee14be94fd78","IPY_MODEL_ac90c5c5687e41a89c4808dc0468d188"],"layout":"IPY_MODEL_83b5bc42260d4ee2a42eacb589011c26"}},"ae64a788be92424996b1700e10cde4b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33b3ffa581c14eac8d31962038d4151f","placeholder":"​","style":"IPY_MODEL_446c9ba1505042319cc17bc2736e9697","value":"Epoch 0:   0%"}},"57960d2d5e0a47dea5e9ee14be94fd78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_426d28dcd2fd44cbb4606da8ba677f6e","max":120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a58de6976e843a490c81acc7ce96a0e","value":0}},"ac90c5c5687e41a89c4808dc0468d188":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d829babd3be486db9949075e1647e17","placeholder":"​","style":"IPY_MODEL_1c52d16c5ab04a7f80ab6c59b08e32b2","value":" 0/120 [00:00&lt;?, ?it/s]"}},"83b5bc42260d4ee2a42eacb589011c26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"33b3ffa581c14eac8d31962038d4151f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"446c9ba1505042319cc17bc2736e9697":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"426d28dcd2fd44cbb4606da8ba677f6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a58de6976e843a490c81acc7ce96a0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d829babd3be486db9949075e1647e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c52d16c5ab04a7f80ab6c59b08e32b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# BERT Fine Tuning"],"metadata":{"id":"r0aGw5WC3EQJ"}},{"cell_type":"markdown","source":["# Import Statements"],"metadata":{"id":"MlH1trW-xxyh"}},{"cell_type":"code","source":["!pip install googletrans==4.0.0-rc1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vi4g8YjN-3nF","executionInfo":{"status":"ok","timestamp":1733642555507,"user_tz":480,"elapsed":6878,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"0b159d70-ca28-497d-9272-26477766820c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans==4.0.0-rc1\n","  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=b6cd68fab5a30a3c9de2a774a03c57457033b9431878ab7efb9e518049393bab\n","  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: h11\n","    Found existing installation: h11 0.14.0\n","    Uninstalling h11-0.14.0:\n","      Successfully uninstalled h11-0.14.0\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.7\n","    Uninstalling httpcore-1.0.7:\n","      Successfully uninstalled httpcore-1.0.7\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.0\n","    Uninstalling httpx-0.28.0:\n","      Successfully uninstalled httpx-0.28.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langsmith 0.1.147 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","openai 1.54.5 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}]},{"cell_type":"code","source":["!pip install pytorch-lightning\n","import torch\n","import pandas as pd\n","import numpy as np\n","import pytorch_lightning as pl\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","from torch.utils.data import DataLoader, TensorDataset\n","#translation and eval\n","from googletrans import Translator\n","from sklearn.model_selection import train_test_split\n","import os"],"metadata":{"id":"1jV636rJ-hDV","executionInfo":{"status":"ok","timestamp":1733642613868,"user_tz":480,"elapsed":15318,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e0a523b-81e4-417e-94d9-89c7387f7405"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.10)\n","Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s8nwDBy-J0VX","executionInfo":{"status":"ok","timestamp":1733642867882,"user_tz":480,"elapsed":28608,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"2ac81b9c-0b86-4bca-dd1b-a7267eec373f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Import libraries\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","import torch\n","\n","# Path to saved model\n","model_path = '/content/drive/MyDrive/266 Final Project/Our Models/BART_All_Data'\n","\n","# Load tokenizer and model\n","tokenizer = BartTokenizer.from_pretrained(model_path)\n","model = BartForConditionalGeneration.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n"],"metadata":{"id":"lUEeWPRg_D6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"rYyqJouzx95D"}},{"cell_type":"code","source":["# Load lyrics data\n","print(\"Loading lyrics data from Google Drive...\")\n","df_list = []\n","lyrics_folder_path = \"/content/drive/My Drive/266 Final Project/Song Files\"\n","for filename in os.listdir(lyrics_folder_path):\n","    if filename.endswith('.csv'):\n","        file_path = os.path.join(lyrics_folder_path, filename)\n","        df = pd.read_csv(file_path)\n","        df_list.append(df)\n","\n","lyrics_df = pd.concat(df_list, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1Rxzgku_FEl","executionInfo":{"status":"ok","timestamp":1733642941265,"user_tz":480,"elapsed":51296,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"81b2871f-1f39-4ce9-f6da-d23261619422"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading lyrics data from Google Drive...\n"]}]},{"cell_type":"code","source":["# Load poetry data\n","print(\"\\nLoading poetry data...\")\n","poem_list = []\n","poetry_files = {\n","    'test': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_test.csv\",\n","    'train': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_train.csv\",\n","    'valid': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_valid.csv\"\n","}\n","\n","for dataset_type, filepath in poetry_files.items():\n","    print(f\"Loading poetry {dataset_type} dataset\")\n","    poem_data = pd.read_csv(filepath)\n","    poem_list.append(poem_data)\n","\n","poem_df = pd.concat(poem_list, ignore_index=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tArb3_h-_QQv","executionInfo":{"status":"ok","timestamp":1733642943608,"user_tz":480,"elapsed":2346,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"94ac871c-43ff-447b-d29d-23b520e27b3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading poetry data...\n","Loading poetry test dataset\n","Loading poetry train dataset\n","Loading poetry valid dataset\n"]}]},{"cell_type":"code","source":["# Split datasets\n","print(\"\\nSplitting datasets...\")\n","train_val_df, test_df = train_test_split(lyrics_df, test_size=0.2, random_state=42)\n","train_val_poem, test_poem = train_test_split(poem_df, test_size=0.2, random_state=42)\n","\n","print(f\"Lyrics data split - Training+Validation: {len(train_val_df)}, Test: {len(test_df)}\")\n","print(f\"Poetry data split - Training+Validation: {len(train_val_poem)}, Test: {len(test_poem)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUZM75Zl_L9r","executionInfo":{"status":"ok","timestamp":1733642943608,"user_tz":480,"elapsed":11,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"48ce00e3-4b9f-4dbf-8e3e-fb20120476c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Splitting datasets...\n","Lyrics data split - Training+Validation: 2385, Test: 597\n","Poetry data split - Training+Validation: 2408, Test: 603\n"]}]},{"cell_type":"markdown","source":["# Data Augmentation Experiment Functions"],"metadata":{"id":"IinxHfrcyH1K"}},{"cell_type":"code","source":["# Data Augmentation Functions\n","def backtranslate(text: str, src_lang: str = \"en\", tgt_lang: str = \"fr\") -> str:\n","    \"\"\"Perform backtranslation using Google Translate.\"\"\"\n","    translator = Translator()\n","    try:\n","        translated = translator.translate(text, src=src_lang, dest=tgt_lang).text\n","        back_translated = translator.translate(translated, src=tgt_lang, dest=src_lang).text\n","        return back_translated\n","    except Exception as e:\n","        print(f\"Backtranslation error: {e}\")\n","        return text\n","\n","def synonym_replacement(text: str, synonym_dict: dict) -> str:\n","    \"\"\"Replace words in text with synonyms.\"\"\"\n","    words = text.split()\n","    augmented_text = \" \".join([synonym_dict.get(word, word) for word in words])\n","    return augmented_text\n"],"metadata":{"id":"3fIPWrmi_foA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Module"],"metadata":{"id":"deuPQfHlyZFJ"}},{"cell_type":"code","source":["class BARTDataModule(pl.LightningDataModule):\n","    def __init__(self, train_df, val_df=None, tokenizer=None, batch_size=16, max_length=512, augment=False):\n","        super().__init__()\n","        self.train_df = train_df\n","        self.val_df = val_df  # Validation data is optional\n","        self.tokenizer = tokenizer\n","        self.batch_size = batch_size\n","        self.max_length = max_length\n","        self.augment = augment\n","\n","    def setup(self, stage=None):\n","        if self.augment:\n","            synonym_dict = {\"example\": \"sample\", \"text\": \"content\"}  # Example synonym dictionary\n","            self.train_df['source'] = self.train_df['source'].apply(\n","                lambda x: backtranslate(x) if np.random.rand() < 0.3 else synonym_replacement(x, synonym_dict)\n","            )\n","        self.train_encodings = self._encode_data(self.train_df)\n","        if self.val_df is not None:\n","            self.val_encodings = self._encode_data(self.val_df)\n","\n","    def _encode_data(self, df):\n","        df['target'] = df['target'].astype(str)  # Ensure 'target' column is of string type\n","        df['target'] = df['target'].apply(lambda x: x if isinstance(x, str) else str(x)) #Convert non-string values\n","\n","        target_encodings = self.tokenizer(\n","            df['target'].tolist(),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        source_encodings = self.tokenizer(\n","            df['source'].tolist(),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        target_encodings = self.tokenizer(\n","            df['target'].tolist(),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': source_encodings['input_ids'],\n","            'attention_mask': source_encodings['attention_mask'],\n","            'labels': target_encodings['input_ids']\n","        }\n","\n","    def train_dataloader(self):\n","        dataset = TensorDataset(\n","            self.train_encodings['input_ids'],\n","            self.train_encodings['attention_mask'],\n","            self.train_encodings['labels']\n","        )\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        # Provide a dummy DataLoader if validation data is not available\n","        if self.val_df is None:\n","            return None\n","        dataset = TensorDataset(\n","            self.val_encodings['input_ids'],\n","            self.val_encodings['attention_mask'],\n","            self.val_encodings['labels']\n","        )\n","        return DataLoader(dataset, batch_size=self.batch_size)\n"],"metadata":{"id":"5hTyVJCu_juB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Definition"],"metadata":{"id":"kX53EY32ydPp"}},{"cell_type":"code","source":["# Define Model\n","class BARTLitModel(pl.LightningModule):\n","    def __init__(self, model, learning_rate=2e-5, use_label_smoothing=False, smoothing=0.1):\n","        super().__init__()\n","        self.model = model\n","        self.learning_rate = learning_rate\n","        self.use_label_smoothing = use_label_smoothing\n","        self.smoothing = smoothing\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        return outputs\n","\n","    def label_smoothing_loss(self, logits, labels):\n","        # Implement label smoothing\n","        vocab_size = logits.size(-1)\n","        one_hot = torch.nn.functional.one_hot(labels, num_classes=vocab_size).float()\n","        smoothed_labels = one_hot * (1 - self.smoothing) + (self.smoothing / vocab_size)\n","        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","        loss = -(smoothed_labels * log_probs).sum(dim=-1).mean()\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch\n","        outputs = self(input_ids, attention_mask, labels)\n","        loss = outputs.loss\n","\n","        # Apply label smoothing if enabled\n","        if self.use_label_smoothing:\n","            loss = self.label_smoothing_loss(outputs.logits, labels)\n","\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch\n","        outputs = self(input_ids, attention_mask, labels)\n","        val_loss = outputs.loss\n","\n","        self.log('val_loss', val_loss)\n","        return val_loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"],"metadata":{"id":"zVXmzV-A_l_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Functions"],"metadata":{"id":"Bu8SNnGmygUw"}},{"cell_type":"code","source":["# Experiment Runner\n","def run_experiment(\n","    experiment_name,\n","    train_df,\n","    val_df,\n","    model,\n","    tokenizer,\n","    batch_size,\n","    max_length,\n","    learning_rate,\n","    drive_path,\n","    augment=False,\n","    use_label_smoothing=False\n","):\n","    print(f\"Running Experiment: {experiment_name}\")\n","\n","\n","    # Set up experiment directory in Google Drive\n","    experiment_dir = os.path.join(drive_path, experiment_name.replace(\" \", \"_\"))\n","    os.makedirs(experiment_dir, exist_ok=True)\n","\n","    # Initialize data module with augmentation logic\n","    data_module = BARTDataModule(\n","        train_df=train_df,\n","        val_df=val_df,\n","        tokenizer=tokenizer,\n","        batch_size=batch_size,\n","        max_length=max_length,\n","        augment=augment\n","    )\n","\n","    # Initialize Lightning module with label smoothing logic\n","    bart_model = BARTLitModel(model=model, learning_rate=learning_rate, use_label_smoothing=use_label_smoothing)\n","\n","    # Set up Trainer\n","    trainer = pl.Trainer(\n","        max_epochs=3,\n","        devices=1 if torch.cuda.is_available() else None,\n","        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n","        gradient_clip_val=1.0,\n","        log_every_n_steps=10,\n","        default_root_dir=experiment_dir,\n","        enable_checkpointing=False  # Disable validation-based checkpointing\n","    )\n","\n","\n","    # Fine-tune the model\n","    trainer.fit(bart_model, datamodule=data_module)\n","\n","    # Save the model and tokenizer\n","    model_save_path = os.path.join(experiment_dir, \"fine_tuned_model\")\n","    tokenizer_save_path = os.path.join(experiment_dir, \"fine_tuned_tokenizer\")\n","    bart_model.model.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(tokenizer_save_path)\n","\n","    print(f\"Experiment '{experiment_name}' results saved to {experiment_dir}\")"],"metadata":{"id":"9aXIpT89_o70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rename columns in train_val_df and test_df\n","train_val_df.rename(columns={\"Lyrics\": \"source\", \"Combined Annotations\": \"target\"}, inplace=True)\n","test_df.rename(columns={\"Lyrics\": \"source\", \"Combined Annotations\": \"target\"}, inplace=True)\n"],"metadata":{"id":"izCbTgcFBMkW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for any problematic values\n","print(train_val_df['source'].isnull().sum())  # Check for null values\n","print(train_val_df['source'].apply(lambda x: isinstance(x, str)).value_counts())  # Check if all are strings\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMNYqHtKBiOz","executionInfo":{"status":"ok","timestamp":1733642943608,"user_tz":480,"elapsed":8,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"17c26100-1f83-42a4-ce82-88286b010381"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","source\n","True    2385\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["def is_tokenizable(text):\n","    try:\n","        # Attempt tokenization\n","        tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","        return True\n","    except ValueError:\n","        return False\n"],"metadata":{"id":"Jns7aWYzCME5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the validation function\n","train_val_df['is_valid'] = train_val_df['source'].apply(is_tokenizable)\n","\n","# Filter out invalid rows\n","valid_train_val_df = train_val_df[train_val_df['is_valid']].drop(columns=['is_valid'])\n"],"metadata":{"id":"zmLJQgdTCMrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df['is_valid'] = test_df['source'].apply(is_tokenizable)\n","valid_test_df = test_df[test_df['is_valid']].drop(columns=['is_valid'])\n"],"metadata":{"id":"t3Hl4Uc4CkHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Original training rows: {len(train_val_df)}\")\n","print(f\"Valid training rows: {len(valid_train_val_df)}\")\n","print(f\"Removed rows: {len(train_val_df) - len(valid_train_val_df)}\")\n","\n","print(f\"Original test rows: {len(test_df)}\")\n","print(f\"Valid test rows: {len(valid_test_df)}\")\n","print(f\"Removed rows: {len(test_df) - len(valid_test_df)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpQq7lYaCneM","executionInfo":{"status":"ok","timestamp":1733642952550,"user_tz":480,"elapsed":11,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"2edac807-ff20-45bd-ef34-fa5119e72627"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original training rows: 2385\n","Valid training rows: 2385\n","Removed rows: 0\n","Original test rows: 597\n","Valid test rows: 597\n","Removed rows: 0\n"]}]},{"cell_type":"code","source":["# Define Experiments\n","#could not run last experiment due to memory loss\n","experiments = [\n","    #{\"name\": \"Base Fine-Tuning\", \"batch_size\": 16, \"max_length\": 512, \"learning_rate\": 2e-5},\n","    {\"name\": \"Hyperparameter Tuning\", \"batch_size\": 8, \"max_length\": 256, \"learning_rate\": 5e-5},\n","    {\"name\": \"Data Augmentation\", \"batch_size\": 16, \"max_length\": 512, \"learning_rate\": 2e-5, \"augment\": True},\n","    #{\"name\": \"Loss Function Experiment\", \"batch_size\": 16, \"max_length\": 512, \"learning_rate\": 2e-5, \"use_label_smoothing\": True}\n","]\n","\n","# Set Drive Path\n","drive_path = \"/content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned\"\n","\n","# Run Experiments\n","for exp in experiments:\n","    # Pass a subset of the training data as validation data\n","    train_data, val_data = train_test_split(valid_train_val_df, test_size=0.2, random_state=42)\n","    run_experiment(\n","        experiment_name=exp[\"name\"],\n","        train_df=train_data,\n","        val_df=val_data,\n","        model=model,\n","        tokenizer=tokenizer,\n","        batch_size=exp[\"batch_size\"],\n","        max_length=exp[\"max_length\"],\n","        learning_rate=exp[\"learning_rate\"],\n","        drive_path=drive_path,\n","        augment=exp.get(\"augment\", False),\n","        use_label_smoothing=exp.get(\"use_label_smoothing\", False)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935,"referenced_widgets":["68f85296ce91429b87a8e8ac239b9c22","9f5fe20e905145f49e7fc057f553e82e","dd0939540863411e81579622819f579c","8cc80cf4c2a641e7a715b6f3295862c1","c3891c72d2844331a225ec4cd0024332","d04d00b94a874e689621394ad560c65a","46c560ca9fb649b4bbbebc78930ef951","46a771836bce4670b8775e658bb55dbb","b1677988c9e747639cac087311805dae","7114a7fcc1ea4eea867d741ef6f5e762","b649b6baf1744ef1a90e4ab78312b98c","30893a4d61c547b6bac48d2fd81be4da","ae64a788be92424996b1700e10cde4b3","57960d2d5e0a47dea5e9ee14be94fd78","ac90c5c5687e41a89c4808dc0468d188","83b5bc42260d4ee2a42eacb589011c26","33b3ffa581c14eac8d31962038d4151f","446c9ba1505042319cc17bc2736e9697","426d28dcd2fd44cbb4606da8ba677f6e","6a58de6976e843a490c81acc7ce96a0e","7d829babd3be486db9949075e1647e17","1c52d16c5ab04a7f80ab6c59b08e32b2"]},"id":"NV2IvsQZ_uL0","executionInfo":{"status":"error","timestamp":1733643015572,"user_tz":480,"elapsed":18532,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"01e20aaa-ee19-44d7-ff7f-5a83c3c9da0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Running Experiment: Base Fine-Tuning\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                         | Params | Mode\n","--------------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 139 M  | eval\n","--------------------------------------------------------------\n","139 M     Trainable params\n","0         Non-trainable params\n","139 M     Total params\n","557.682   Total estimated model params size (MB)\n","0         Modules in train mode\n","182       Modules in eval mode\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f85296ce91429b87a8e8ac239b9c22"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n","/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30893a4d61c547b6bac48d2fd81be4da"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:\n","Detected KeyboardInterrupt, attempting graceful shutdown ...\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'exit' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \"\"\"\n\u001b[0;32m-> 1306\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-9faed1d68871>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-9faed1d68871>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1643\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m   1314\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;31m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-d671e36e50ba>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Pass a subset of the training data as validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_train_val_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     run_experiment(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use the split training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-705877c4e635>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(experiment_name, train_df, val_df, model, tokenizer, batch_size, max_length, learning_rate, drive_path, augment, use_label_smoothing)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbart_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Save the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"1UbO3nVmWjIt"}},{"cell_type":"code","source":["!pip install bert_score\n","from transformers import BartForConditionalGeneration, BartTokenizer\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from typing import Dict, List, Tuple\n","from bert_score import score\n","from rouge_score import rouge_scorer\n","import torch\n"],"metadata":{"id":"bATjSH24Ubiw","executionInfo":{"status":"error","timestamp":1733643041974,"user_tz":480,"elapsed":3075,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"abebe076-f004-4fe6-edce-b11988fa0507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.46.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'rouge_score'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-0008c772c818>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["#checkinf test df\n","test_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"b-94-Deybndx","executionInfo":{"status":"ok","timestamp":1733643044952,"user_tz":480,"elapsed":272,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"6cb24199-20c5-410f-f599-b7e8343d7d0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Song ID                     Title  \\\n","2409  7222324  Santa, Can’t You Hear Me   \n","1547  2451400       Dance Like Yo Daddy   \n","881    328275             Act Like That   \n","331   2820857                   Bad Guy   \n","2225   745209                 DEATHCAMP   \n","\n","                                             Lyrics URL  \\\n","2409  https://genius.com/Kelly-clarkson-and-ariana-g...   \n","1547  https://genius.com/Meghan-trainor-dance-like-y...   \n","881   https://genius.com/French-montana-act-like-tha...   \n","331   https://genius.com/21-savage-and-metro-boomin-...   \n","2225  https://genius.com/Tyler-the-creator-deathcamp...   \n","\n","                                                 target  \\\n","2409  The Voice coaches Ariana Grande and Kelly Clar...   \n","1547  This song is yet another anthem about bravery ...   \n","881                                                   d   \n","331   On “Bad Guy,” 21 Savage is embracing his ego. ...   \n","2225  “DEATHCAMP” is the first track off Tyler’s fou...   \n","\n","                                   Wikipedia Annotation  \\\n","2409  \"Santa, Can't You Hear Me\" is a duet by Americ...   \n","1547  No Wikipedia annotation found (artist name not...   \n","881   No Wikipedia annotation found (artist name not...   \n","331   No Wikipedia annotation found (artist name not...   \n","2225  \"Deathcamp\" is a song by American rapper Tyler...   \n","\n","                                                 source  is_valid  \n","2409  Keep the snow and sleigh rides\\r\\nKeep those s...      True  \n","1547  Dance like yo daddy\\r\\nDance like yo daddy\\r\\n...      True  \n","881   [Intro]\\r\\nSee people always remember\\r\\nWhen ...      True  \n","331   I'm a fuckin' bad guy nigga\\r\\nMurder Gang nig...      True  \n","2225  Um, excuse me mister but can you please turn d...      True  "],"text/html":["\n","  <div id=\"df-ad1e30aa-4870-49bb-a8c1-65e10ea95715\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Song ID</th>\n","      <th>Title</th>\n","      <th>Lyrics URL</th>\n","      <th>target</th>\n","      <th>Wikipedia Annotation</th>\n","      <th>source</th>\n","      <th>is_valid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2409</th>\n","      <td>7222324</td>\n","      <td>Santa, Can’t You Hear Me</td>\n","      <td>https://genius.com/Kelly-clarkson-and-ariana-g...</td>\n","      <td>The Voice coaches Ariana Grande and Kelly Clar...</td>\n","      <td>\"Santa, Can't You Hear Me\" is a duet by Americ...</td>\n","      <td>Keep the snow and sleigh rides\\r\\nKeep those s...</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1547</th>\n","      <td>2451400</td>\n","      <td>Dance Like Yo Daddy</td>\n","      <td>https://genius.com/Meghan-trainor-dance-like-y...</td>\n","      <td>This song is yet another anthem about bravery ...</td>\n","      <td>No Wikipedia annotation found (artist name not...</td>\n","      <td>Dance like yo daddy\\r\\nDance like yo daddy\\r\\n...</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>881</th>\n","      <td>328275</td>\n","      <td>Act Like That</td>\n","      <td>https://genius.com/French-montana-act-like-tha...</td>\n","      <td>d</td>\n","      <td>No Wikipedia annotation found (artist name not...</td>\n","      <td>[Intro]\\r\\nSee people always remember\\r\\nWhen ...</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>331</th>\n","      <td>2820857</td>\n","      <td>Bad Guy</td>\n","      <td>https://genius.com/21-savage-and-metro-boomin-...</td>\n","      <td>On “Bad Guy,” 21 Savage is embracing his ego. ...</td>\n","      <td>No Wikipedia annotation found (artist name not...</td>\n","      <td>I'm a fuckin' bad guy nigga\\r\\nMurder Gang nig...</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2225</th>\n","      <td>745209</td>\n","      <td>DEATHCAMP</td>\n","      <td>https://genius.com/Tyler-the-creator-deathcamp...</td>\n","      <td>“DEATHCAMP” is the first track off Tyler’s fou...</td>\n","      <td>\"Deathcamp\" is a song by American rapper Tyler...</td>\n","      <td>Um, excuse me mister but can you please turn d...</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad1e30aa-4870-49bb-a8c1-65e10ea95715')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ad1e30aa-4870-49bb-a8c1-65e10ea95715 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ad1e30aa-4870-49bb-a8c1-65e10ea95715');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-04eac0eb-fa88-408f-8a3f-246910d031d6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04eac0eb-fa88-408f-8a3f-246910d031d6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-04eac0eb-fa88-408f-8a3f-246910d031d6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"test_df","summary":"{\n  \"name\": \"test_df\",\n  \"rows\": 597,\n  \"fields\": [\n    {\n      \"column\": \"Song ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3420073,\n        \"min\": 2093,\n        \"max\": 10970124,\n        \"num_unique_values\": 597,\n        \"samples\": [\n          3768943,\n          2923942,\n          2328642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 594,\n        \"samples\": [\n          \"Lick (Remix)\",\n          \"\\u200bbury a friend\",\n          \"Asphalt Cowboy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lyrics URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 597,\n        \"samples\": [\n          \"https://genius.com/Girl-in-red-girls-lyrics\",\n          \"https://genius.com/Kane-brown-cold-spot-lyrics\",\n          \"https://genius.com/Raye-tell-me-lyrics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 522,\n        \"samples\": [\n          \"\\u201cBlue Jeans\\u201d is a funky track that details Chris' desire to take his girl home and take her jeans off for some intimacy. The track is possibly written about his former flame   as the pre-chorus alludes to her green eyes and her frequently worn red lipstick.  The track was originally included on the   of   alongside \",\n          \" and   \\u2014 who previously collaborated on the likes of  ,   and   \\u2014 are back again with a remix of Travi$ Scott\\u2019s Antidote.\",\n          \"Demo of popular unreleased Harry Styles song \\u201c .\\u201d This demo features an alternate mix and lyrics to the performed and leaked version of the song. It also features a one-minute instrumental intro, like the leaked and performed versions.  The first snippet of the song surfaced online on August 30, 2023 from X (Twitter) user @nastyshouse. The file name of the song is \\u201cMedicine (Demo Mix 1)\\u201d indicating that there may be multiple mixes of this demo.  The full version of the demo leaked one week later, on September 6, 2023.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Wikipedia Annotation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"\\\"Chantaje\\\" (Spanish pronunciation: [t\\u0283an\\u02c8taxe]; English: \\\"Blackmail\\\") is a song by Colombian singer-songwriter Shakira, featuring fellow Colombian singer-songwriter and rapper Maluma. It was released on October 28, 2016, via Sony Music Latin as the lead single from Shakira's eleventh studio album, El Dorado (2017). The song was written by Shakira and Maluma and produced by the two and The Rudeboyz, with musical composition done by the former three and Kenai. The song is Maluma and Shakira's second collaboration, after the two were featured on a remix of Carlos Vives' single \\\"La Bicicleta\\\".\\n\\\"Chantaje\\\" is a pop and reggaeton song. Lyrically, the song was considered a \\\"battle of the sexes\\\", where the male protagonist is not sure where he stands with his lover, and the female protagonist does not exactly clear things up. The music video for \\\"Chantaje\\\" was filmed in Barcelona, Spain, with video director Jaume de Laiguana, Shakira's long-time collaborator. Commercially, the song topped the Brazil, Ecuador, Guatemala, Spain, Uruguay, and US Billboard Latin Songs charts, while also reaching the top five in nine additional countries. Maluma called it his \\\"first global hit\\\". It became the most successful Latin single of the decade by a female artist in the United States. The track is also certified Platinum or higher in ten countries, including Diamond in Argentina, Brazil (three times), Colombia, France, Mexico and United States (Latin). The song was nominated for Record of the Year, Song of the Year and Best Urban Fusion Performance at the 18th Latin Grammy Awards.\\n\\n\\n== Background and release ==\\nPuerto Rican composer and recording artist Kenai had the word \\\"Chantaje\\\" in his mind months before the idea of a collaboration with Shakira and Maluma emerged. He stated in an interview that the idea of the song was intended to be his own single. While working on music demos in Colombia with The Rudeboyz, he received a call from the production duo to work on a collaborative single between Shakira and Maluma that was intended to be presented to the artists the next day. They were supposed to offer three different demos to Shakira and Maluma, but ended up working on just one feeling confident enough about its potential. The three worked together on the demo one night in Colombia. The Rudeboyz traveled without Kenai the next day to Barcelona, Spain to meet with Shakira and Maluma to keep working on the song together.\\nBefore collaborating with Shakira in \\\"Chantaje\\\", Maluma served guest vocals for the official remix of Carlos Vives and Shakira's 2016 song \\\"La Bicicleta\\\". Later, Maluma travelled to Barcelona in early September 2016 to work with Shakira, along with his production team. He confessed to Billboard that \\\"he couldn't miss the opportunity to work with [her] again.\\\" The pairing was possible due to Sony Music Latin, who proposed the idea for both artists working together again after the remix of \\\"La Bicicleta\\\". Maluma also commented about Shakira, stating: \\\"It was an amazing experience. [...] She's a great artist, but she's also a composer. I learned a lot.\\\"\\nBefore releasing the song, Shakira shared clues on her Instagram account on 25 October 2016, and if someone solved the mystery, they would get a personal message from the singer confirming the correct answer. The first photo had Shakira \\\"wearing a magician's hat and holding a rabbit,\\\" which meant \\\"Magic\\\", name of both Shakira's and Maluma's first albums; the second photo had a drawing of a crown in a white board, meaning Maluma's crown tattoo; and the third photo was Shakira holding a poster with a photo of Alejandro Sanz (which stands for his nickname \\\"Chan\\\") + TA + G, which was the title of the track: \\\"Chantaje\\\", literally \\\"blackmail.\\\" Later, the singer shared the artwork of the single, featuring both artists. Eventually, the song was released on 28 October 2016 through digital download by Ace Entertainment.\\n\\n\\n== Composition ==\\n\\\"Chantaje\\\" was written by Shakira, Maluma, Kenai and production duo The Rudeboyz. It was also produced by Shakira and Maluma with The Rudeboyz. \\\"Chantaje\\\" is a pop and reggaeton song, with tropical synths in its background. The song is built from a \\\"pitched-up vocal\\\" saying \\\"Hola, mira\\\" (\\\"Hello, look\\\"). Lyrically, the song is a chase \\\"between a lustful man and an unattainable woman,\\\" where \\\"he\\u2019s not sure where he stands with her, and she's not exactly clearing things up.\\\" \\\"Yo soy masoquista\\\" (\\\"I'm a masochist\\\"), he says, and she responds, \\\"Con mi cuerpo, un ego\\u00edsta\\\" (\\\"With my body, an egoistic\\\"), adding: [...] \\\"En esta relaci\\u00f3n, soy yo la que manda\\\" (\\\"In this relationship, I\\u2019m in command\\\"). In the chorus, she warns him: \\\"No soy de ti ni de nadie\\\" (\\\"I'm not yours or anyone else's\\\").\\n\\n\\n== Critical reception ==\\nJon Pareles of The New York Times was positive, noting that \\\"Shakira gets even more intense when she sings in Spanish, nervier and more committed,\\\" and calling the song \\\"a sparse, insinuatingly catchy reggaet\\u00f3n battle of the sexes.\\\" Jeff Nelson of People named it \\\"a sexy, buoyant jam,\\\" while Diana Martin of E! Online wrote that the song is \\\"all sorts of sexy and infectious.\\\" Lucy Morris of Digital Spy admitted: \\\"We may have no clue what the lyrics mean, but the song is infectious.\\\" Mike Wass of Idolator opined that the song \\\"is instantly hummable regardless of your Spanish language skills and will have you moving before the end of the first verse. It\\u2019s a winner and has the potential to be a 'La Tortura'-sized crossover hit.\\\" Lucas Villa of AXS said that the song \\\"plays to Shakira's strengths as a global seductress with a banger that's ready to make club-goers bend to its will.\\\"\\nBillboard included Chantaje among the 50 essential Latin songs of the 2010s decade.\\n\\n\\n== Commercial performance ==\\nIn the United States, the song became the second track to ever debut atop Billboard Top Latin Songs after Shakira's and Man\\u00e1's \\\"Mi Verdad\\\", after the chart became a multi-metric chart. With its first-week sales of 13,000 digital downloads, 1.6 million streams and 13.8 million audience impressions in the US, the song became the second highest sales week for a Spanish-language track in 2016. The single was \\\"only the 14th track to debut atop the chart in its 30-year history\\\"Chantaje\\\", as reported by Billboard. It is also Shakira's eleventh number-one on the chart, and Maluma's first. \\\"Chantaje\\\" also debuted at number 96 on the Billboard Hot 100 chart, becoming Shakira's 19th entry there and Maluma's first. In its eighth charting week, the song managed to jump from number 65 to number 51, becoming her highest Hot 100 Spanish-language single after \\\"La Tortura\\\" (2005). In the same week, the song spent its eighth non-consecutive week at the top of the Latin Songs, matching the amount of time that \\\"Hips Don't Lie\\\" spent in 2006, while also becoming her second longest chart reign, spending eleven weeks at number one. It has been certified 16\\u00d7 Platinum by the RIAA's Latin certifications program, denoting sales of over 960,000 units. According to the 2017 year-end Nielsen Canada report, the song has sold 177,000 units in the country. \\\"Chantaje\\\" went on to become the most successful Latin single of the decade by a female artist in the United States, according to Billboard.\\nIn the United Kingdom, the single became the first Spanish song by a female artist in history to be certified Silver for surpassing 200,000 equivalent units in the country.\\nIn France, the song debuted at number 36 on the week of 11 November 2016. Later, it fell to numbers 91 and 126 in the following weeks, before climbing once again to number 76. After spending three weeks climbing on the charts, \\\"Chantaje\\\" cracked the top-forty for the second time, going to number forty. Five weeks later, the song managed to climb from number 22 to number 14, becoming Shakira's fifteenth top-twenty single and highest since \\\"Dare (La La La)\\\" (2014).\\nIn Italy, \\\"Chantaje\\\" debuted at number 15 and peaked at number 11 for two weeks.\\nIn Netherlands, \\\"Chantaje\\\" managed to peak at number 18, becoming her highest-charting single since \\\"Waka Waka (This Time for Africa)\\\" (2010).\\nIn Switzerland, the song peaked at number 10, becoming Shakira's fifteenth top-ten single.\\nIn Spain, \\\"Chantaje\\\" entered the PROMUSICAE chart at number eight and reached number-one the following week, becoming Shakira's eighth number-one single.\\n\\n\\n== Music video ==\\nThe music video was shot in Barcelona, on 13 and 14 October 2016, and it was directed by Spanish director and photographer Jaume de Laiguana, who has already directed various Shakira's music videos. The official lyric video to the song was released on 16 November, while the official music video was released on 18 November 2016. In the beginning of the music video, as described by Rolling Stone's Sarah Grant, \\\"Shakira strolls through a bodega with her pet pig on a leash and her wild mane of hair hidden under a pink wig.\\\" \\\"She then lures him in to an underground bar\\\", as Billboard added, while also \\\"swivel[ing] her hips from the top of the bar to the men's bathroom, where she winds, kicks and grinds solo amid the urinals.\\\"\\n\\n\\n=== Reception ===\\nThe music video earned 100 million views in just 19 days, breaking the previous 21 days record, and becoming the fastest Spanish video ever to do so. On 7 April 2017, it reached 1 billion views, becoming the fastest Spanish video to hit the billion visits landmark. It was also the second-most viewed video on Vevo in 2017, after \\\"Despacito\\\".\\n\\n\\n=== Salsa version ===\\nOn 2 February, Shakira celebrated her birthday with a salsa version of \\\"Chantaje\\\" with Chelito de Castro.\\n\\n\\n== Accolades ==\\n\\n\\n== Live performances ==\\nTo date, Shakira and Maluma have not yet  performed Chantaje together; other  than appearing in the music video itself, there are several behind-the-scenes recording studio clips on YouTube. Neither singer has made a guest appearance at the other's concerts, nor television appearances, awards show performances or otherwise.\\nShakira performed Chantaje with Coldplay's lead vocalist Chris Martin at Global Citizen Festival Hamburg, on 6 July 2017. She tried to get him to sing and understand the Spanish words.\\nShakira also performed a shortened, Salsa-inspired version of the song with Bad Bunny, during her medley of songs at the Super Bowl LIV halftime show.\\nMaluma performed the song at the Vi\\u00f1a del Mar Festival in Valpara\\u00edso, Chile, as well as on his various tours since the song's release. He normally has a female backup vocalist \\\"act\\\" as Shakira and sing her lines. Likewise, Shakira has also performed the song, solo, during her El Dorado World Tour.\\n\\n\\n== Credits and personnel ==\\nCredits adapted from Tidal.\\n\\n\\n== Charts ==\\n\\n\\n== Certifications ==\\n\\n\\n== Release history ==\\n\\n\\n== See also ==\\nList of number-one singles of 2016 (Spain)\\nList of number-one singles of 2017 (Spain)\\nList of number-one Billboard Hot Latin Songs of 2016\\nList of number-one Billboard Hot Latin Songs of 2017\\nList of most liked YouTube videos\\nList of best-selling singles in Spain\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\\"Chantaje\\\" on YouTube\",\n          \"\\\"On My Way to You\\\" is a song written by Brett James and Tony Lane, and recorded by American country music singer Cody Johnson. It is the first single from his seventh studio album Ain't Nothin' to It.\\n\\n\\n== Background ==\\nThe song was written by Brett James and Tony Lane. It is Johnson's first song released under a major label.\\n\\n\\n== Music video ==\\nA video for the song, directed by Sean Hagwell, was released in October 2018.\\n\\n\\n== Commercial performance ==\\nThe song has sold 107,000 copies in the United States as of April 2019.\\n\\n\\n== Charts ==\\n\\n\\n== Certifications ==\\n\\n\\n== References ==\",\n          \"Be Like That may refer to:\\n\\n\\\"Be Like That\\\" (3 Doors Down song), 2000\\n\\\"Be Like That\\\" (Kane Brown, Swae Lee and Khalid song), 2020\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 566,\n        \"samples\": [\n          \"I only threw this party for you\\r\\nI only threw this party for you, for you, for you\\r\\nI was hopin' you would come through\\r\\nI was hopin' you would come through, it's true, it's true\\r\\nI only threw this party for you\\r\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nOne dance and I paind my nails\\n\\nDJ with your favorite tunes\\n\\n[?]\\n\\nBut you were born [?] or two\\n\\nChampagne poured in your mouth\\n\\nCaught your friends from out of town\\n\\nBut the party back with purple bills\\n\\nAnd I'm waiting for you by the window, yeah\\n\\nCaught your digits, but my phone can't ring it, eh\\n\\nWish I knew what you were thinking\\n\\nNa-na-na 1000 pink balloons\\n\\nDancin' on to your favorite tunes\\n\\nHope you lookin' to party\\n\\n'Cause I threw the party just for you now, boy\\n\\n\\n\\nI only threw this party for you\\n\\nOnly threw this party for you, for you, for you\\n\\nI was hopin' you would come through\\n\\nI was hopin' you would come through, it's true, it's true\\n\\nI only threw this party for you\\n\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nI only threw this party for you\\n\\nOnly threw this party for you, for you, for you\\n\\nI was hopin' you would come through\\n\\nI was hopin' you would come through, it's true, it's true\\n\\nOnly threw this party for you\\n\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nI only threw this party for you\\n\\nOnly threw this party for you, for you, for you\\n\\nI was hopin' you would come through\\n\\nI was hopin' you would come through, it's true, it's true\\n\\nOnly threw this party for you\\n\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nI only threw this party for you\\n\\nOnly threw this party for you, for you, for you\\n\\nI was hopin' you would come through\\n\\nI was hopin' you would come through, it's true, it's true\\n\\nOnly threw this party for you\\n\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nI only threw this party for you\\n\\nOnly threw this party for you, for you, for you\\n\\nI was hopin' you would come through\\n\\nI was hopin' you would come through, it's true, it's true\\n\\nOnly threw this party for you\\n\\nI only threw this party for you, for you, for you\\n\\nI would love to party on you\\n\\nWatch me, watch me party on you, yeah\\n\\n\\n\\nFor you, you\\n\\nFor you, y-you, y-you\\n\\nFor you, you\\n\\nFor you, y-you, y-you\\n\\nFor you, you\\n\\nFor you, y-you, y-you\\n\\nFor you, you\\n\\nFor you, y-you, y-you\",\n          \"Wish I could be a little more mature\\r\\nDid my time, did myself for sure\\r\\nRiver wide, got myself afloat\\r\\nDemons die if I don't support\\r\\nDemons die if I don't support\\r\\nAnd why can't I be like everybody else? (Oh, woah)\\n\\nLosin' my mind, think I look good when I'm really just high\\n\\nScared of my life, can a bitch get by? (Get by)\\n\\nSick of listening to everyone else (Oh, ooh)\\n\\nSick of my pride (Ooh), sick of just sayin' shit just to be nice (Oh)\\n\\nScared of this world, how do I get by?\\n\\n\\n\\nMiles runnin' wild in my head (Miles runnin' wild)\\n\\nPacin', going back instead of movin' forward\\n\\nTime is movin' forward (Y\\u0435ah, yeah, yeah)\\n\\nMiles runnin' wild in my h\\u0435ad (Miles runnin' wild)\\n\\nI'm scared of movin' forward\\n\\nTime keep movin' forward, forward\",\n          \"I love women and power, we used to sit and talk for hours\\r\\nOn the balcony, my condo play your favorite, shawty\\r\\nNo love lost, just the perfect soulmate, that's all\\r\\nIt's my fault, caught in a rupture\\r\\nShould've been you I was touchin', I knew it wasn't\\r\\nMy ego, I was buzzin' but not how you wanted\\n\\nToo busy tryna flaunt it, like I just bought it\\n\\nYou know them bitches 'round you can't hold water\\n\\nThis s'posed to be our moment\\n\\nThese pills set it, I'm feelin' better\\n\\nCalled you to tell you I love you\\n\\nBut you too busy with some other shit, I'm guessin'\\n\\nNo texts, no proof, no voice messages\\n\\nHow we become enemies so effortless?\\n\\nOr was I blind when you was naked?\\n\\nOr high when you was level\\n\\nKnow time is of the essence\\n\\nNo lie, I live and regret it\\n\\nDecisions I made, embedded in my mind\\n\\nHate to admit it, but I gotta say it one time\\n\\n\\n\\nShould've treated you right\\n\\nBut every night I was livin' life in the fast lane\\n\\nI should've put my focus on you\\n\\nIn the club every night\\n\\nI just get it wrong when you do right\\n\\nAnd I keep just fillin' your head with the lies\\n\\nThis shit comes with a price\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you\\n\\n\\n\\nWhat happened to me, babe?\\n\\nThis ain't who you fell in love with\\n\\nYou say you need space\\n\\nDon't do this\\n\\nJust look at me, one more time\\n\\nI know I fucked up but I'm tryin' to please you\\n\\nI'ma give you all\\n\\nAnd your heartache and pain's gonna stop\\n\\nBaby right here\\n\\nDon't walk away\\n\\nThere is no one left to call\\n\\nDon't wanna be alone when you disappear\\n\\nI know I hurt you more than three times\\n\\nNow you need me time\\n\\nFuck I'm s'posed to do in the meantime?\\n\\nThen she told me \\\"figure it out\\\"\\n\\n\\n\\nShould've treated you right\\n\\nBut every night I was livin' life in the fast lane\\n\\nI should've put my focus on you\\n\\nIn the club every night\\n\\nI just get it wrong when you do right\\n\\nAnd I keep just fillin' your head with the lies\\n\\nThis shit comes with a price\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you better\\n\\nA nigga should've loved you\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_valid\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# Check for NaN values\n","print(\"NaN values in test_df:\")\n","print(test_df.isna().sum())\n","\n","# Check data types\n","print(\"\\nData types:\")\n","print(test_df.dtypes)\n","\n","# Clean the data\n","test_df['source'] = test_df['source'].fillna('')\n","test_df['target'] = test_df['target'].fillna('')\n","\n","# Convert to string type\n","test_df['source'] = test_df['source'].astype(str)\n","test_df['target'] = test_df['target'].astype(str)\n","\n","# Verify no empty strings that might cause issues\n","print(\"\\nNumber of empty lyrics:\", len(test_df[test_df['source'] == '']))\n","print(\"Number of empty annotations:\", len(test_df[test_df['target'] == '']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cm5tEsR4d5-3","executionInfo":{"status":"ok","timestamp":1733643047756,"user_tz":480,"elapsed":848,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"c266349b-e1f3-4535-8d0f-debc2d3151b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NaN values in test_df:\n","Song ID                 0\n","Title                   0\n","Lyrics URL              0\n","target                  1\n","Wikipedia Annotation    0\n","source                  0\n","is_valid                0\n","dtype: int64\n","\n","Data types:\n","Song ID                  int64\n","Title                   object\n","Lyrics URL              object\n","target                  object\n","Wikipedia Annotation    object\n","source                  object\n","is_valid                  bool\n","dtype: object\n","\n","Number of empty lyrics: 0\n","Number of empty annotations: 1\n"]}]},{"cell_type":"code","source":["# Initial data cleanup\n","print(\"Initial data shape:\", test_df.shape)\n","\n","# Fill NaN values\n","test_df['target'] = test_df['target'].fillna('')\n","\n","# Convert source and target to string type\n","test_df['source'] = test_df['source'].astype(str)\n","test_df['target'] = test_df['target'].astype(str)\n","\n","# Remove rows where either source or target is empty (optional)\n","# test_df = test_df[test_df['source'].str.strip() != '']\n","# test_df = test_df[test_df['target'].str.strip() != '']\n","\n","# Double check the cleaned data\n","print(\"\\nAfter cleaning:\")\n","print(\"Number of empty lyrics:\", len(test_df[test_df['source'] == '']))\n","print(\"Number of empty annotations:\", len(test_df[test_df['target'] == '']))\n","print(\"Final data shape:\", test_df.shape)\n","\n","# Verify a few examples\n","print(\"\\nSample data check:\")\n","print(test_df[['source', 'target']].head(2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MXbHT4cgeCdC","executionInfo":{"status":"ok","timestamp":1733643049655,"user_tz":480,"elapsed":2,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"09793a9d-2929-411d-cc13-588eba5e2c49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial data shape: (597, 7)\n","\n","After cleaning:\n","Number of empty lyrics: 0\n","Number of empty annotations: 1\n","Final data shape: (597, 7)\n","\n","Sample data check:\n","                                                 source  \\\n","2409  Keep the snow and sleigh rides\\r\\nKeep those s...   \n","1547  Dance like yo daddy\\r\\nDance like yo daddy\\r\\n...   \n","\n","                                                 target  \n","2409  The Voice coaches Ariana Grande and Kelly Clar...  \n","1547  This song is yet another anthem about bravery ...  \n"]}]},{"cell_type":"code","source":["def evaluate_supervised_model(\n","    model: BartForConditionalGeneration,\n","    tokenizer: BartTokenizer,\n","    test_data: pd.DataFrame,\n","    batch_size: int = 16\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","    \"\"\"\n","    Evaluate supervised lyrics model comparing against Genius annotations using BART\n","    \"\"\"\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    evaluation_results = {\n","        'content_coverage': [],\n","        'consistency_score': [],\n","        'semantic_similarity': [],\n","        'rouge1_scores': [],\n","        'rouge2_scores': [],\n","        'rougeL_scores': [],\n","        'bert_scores': []\n","    }\n","\n","    examples = []\n","    previous_bert_score = 0.0\n","\n","    for idx in tqdm(range(0, len(test_data), batch_size)):\n","        batch_lyrics = test_data['source'].iloc[idx:idx + batch_size].tolist()\n","        batch_annotations = test_data['target'].iloc[idx:idx + batch_size].tolist()\n","\n","        # Generate summaries (BART-specific encoding)\n","        inputs = tokenizer(\n","            [f\"summarize lyrics and capture meaning: {lyric}\" for lyric in batch_lyrics],\n","            padding=True,\n","            truncation=True,\n","            max_length=1024,\n","            return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_ids=inputs['input_ids'],\n","                attention_mask=inputs['attention_mask'],\n","                max_length=150,\n","                min_length=50,\n","                num_beams=4,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                no_repeat_ngram_size=3,\n","                length_penalty=1.0,\n","                repetition_penalty=1.2\n","            )\n","\n","            generated_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","        # Evaluate each summary against its reference annotation\n","        for i in range(len(generated_summaries)):\n","            original_lyric = batch_lyrics[i]\n","            generated_summary = generated_summaries[i]\n","            reference_annotation = batch_annotations[i]\n","\n","            # Content Coverage (between summary and lyrics)\n","            coverage_score = calculate_content_coverage(original_lyric, generated_summary)\n","            evaluation_results['content_coverage'].append(coverage_score)\n","\n","            # Semantic Similarity (between summary and lyrics)\n","            semantic_score = calculate_semantic_similarity(original_lyric, generated_summary)\n","            evaluation_results['semantic_similarity'].append(semantic_score)\n","\n","            # ROUGE Scores (between generated summary and reference annotation)\n","            rouge_scores = calculate_rouge_scores([generated_summary, reference_annotation])\n","            evaluation_results['rouge1_scores'].append(rouge_scores['rouge1'])\n","            evaluation_results['rouge2_scores'].append(rouge_scores['rouge2'])\n","            evaluation_results['rougeL_scores'].append(rouge_scores['rougeL'])\n","\n","            # BERTScore (between generated summary and reference annotation)\n","            if i % 8 == 0:  # Compute less frequently to save time\n","                P, R, F1 = score([generated_summary], [reference_annotation], lang='en', verbose=False)\n","                previous_bert_score = F1.mean().item()\n","            evaluation_results['bert_scores'].append(previous_bert_score)\n","\n","            # Store examples\n","            if len(examples) < 5:\n","                examples.append({\n","                    'lyrics': original_lyric,\n","                    'reference_annotation': reference_annotation,\n","                    'generated_summary': generated_summary,\n","                    'metrics': {\n","                        'content_coverage': coverage_score,\n","                        'semantic_similarity': semantic_score,\n","                        'rouge1': rouge_scores['rouge1'],\n","                        'rouge2': rouge_scores['rouge2'],\n","                        'rougeL': rouge_scores['rougeL'],\n","                        'bert_score': previous_bert_score\n","                    }\n","                })\n","\n","        # Memory cleanup\n","        if idx % 5 == 0:\n","            torch.cuda.empty_cache()\n","\n","    # Aggregate results\n","    metrics = {\n","        'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","        'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","        'avg_rouge1': np.mean(evaluation_results['rouge1_scores']),\n","        'avg_rouge2': np.mean(evaluation_results['rouge2_scores']),\n","        'avg_rougeL': np.mean(evaluation_results['rougeL_scores']),\n","        'avg_bert_score': np.mean(evaluation_results['bert_scores'])\n","    }\n","\n","    return metrics, examples\n","\n","def calculate_rouge_scores(texts: List[str]) -> Dict[str, float]:\n","    \"\"\"Calculate ROUGE scores between texts\"\"\"\n","    rouge_scorer_obj = rouge_scorer.RougeScorer(\n","        ['rouge1', 'rouge2', 'rougeL'],\n","        use_stemmer=True\n","    )\n","    score = rouge_scorer_obj.score(texts[0], texts[1])\n","    return {\n","        'rouge1': score['rouge1'].fmeasure,\n","        'rouge2': score['rouge2'].fmeasure,\n","        'rougeL': score['rougeL'].fmeasure\n","    }\n","\n","def calculate_content_coverage(lyrics: str, summary: str) -> float:\n","    \"\"\"Calculate content coverage between lyrics and summary\"\"\"\n","    if isinstance(lyrics, float) or isinstance(summary, float):\n","        return 0.0\n","\n","    try:\n","        lyrics_tokens = set(str(lyrics).lower().split())\n","        summary_tokens = set(str(summary).lower().split())\n","        overlap = len(lyrics_tokens.intersection(summary_tokens))\n","        coverage = overlap / len(lyrics_tokens) if lyrics_tokens else 0.0\n","        return coverage\n","    except Exception as e:\n","        print(f\"Error processing lyrics/summary: {e}\")\n","        return 0.0\n","\n","def calculate_semantic_similarity(lyrics: str, summary: str) -> float:\n","    \"\"\"Calculate semantic similarity using token overlap\"\"\"\n","    if isinstance(lyrics, float) or isinstance(summary, float):\n","        return 0.0\n","\n","    try:\n","        lyrics_tokens = set(str(lyrics).lower().split())\n","        summary_tokens = set(str(summary).lower().split())\n","        intersection = len(lyrics_tokens.intersection(summary_tokens))\n","        union = len(lyrics_tokens.union(summary_tokens))\n","        return intersection / union if union > 0 else 0.0\n","    except Exception as e:\n","        print(f\"Error processing lyrics/summary: {e}\")\n","        return 0.0\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","    \"\"\"Print evaluation results and examples\"\"\"\n","    print(\"\\nEvaluation Results:\")\n","    print(f\"Average Content Coverage: {metrics['avg_content_coverage']:.3f}\")\n","    print(f\"Average Semantic Similarity: {metrics['avg_semantic_similarity']:.3f}\")\n","    print(f\"Average ROUGE-1: {metrics['avg_rouge1']:.3f}\")\n","    print(f\"Average ROUGE-2: {metrics['avg_rouge2']:.3f}\")\n","    print(f\"Average ROUGE-L: {metrics['avg_rougeL']:.3f}\")\n","    print(f\"Average BERTScore: {metrics['avg_bert_score']:.3f}\")\n","\n","    print(\"\\nExample Generations:\")\n","    for i, example in enumerate(examples, 1):\n","        print(f\"\\nExample {i}:\")\n","        print(f\"Original Lyrics (truncated): {example['lyrics'][:200]}...\")\n","        print(f\"\\nReference Annotation: {example['reference_annotation']}\")\n","        print(f\"\\nGenerated Summary: {example['generated_summary']}\")\n","        print(\"\\nMetrics:\")\n","        for metric, value in example['metrics'].items():\n","            print(f\"{metric}: {value:.3f}\")"],"metadata":{"id":"_9FpgGXiVqS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_evaluation(model_path: str, tokenizer_path: str, test_df: pd.DataFrame, save_dir: str):\n","    \"\"\"\n","    Run evaluation and save results for a given model and tokenizer path.\n","    Results are saved in CSV format.\n","    \"\"\"\n","    tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n","    model = BartForConditionalGeneration.from_pretrained(model_path)\n","    print(f\"Model loaded from {model_path}, tokenizer loaded from {tokenizer_path}!\")\n","\n","    metrics, examples = evaluate_supervised_model(\n","        model,\n","        tokenizer,\n","        test_data=test_df,\n","        batch_size=16\n","    )\n","\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Save metrics to CSV\n","    metrics_df = pd.DataFrame([metrics])\n","    metrics_file = os.path.join(save_dir, \"evaluation_metrics.csv\")\n","    metrics_df.to_csv(metrics_file, index=False)\n","    print(f\"Saved evaluation metrics to: {metrics_file}\")\n","\n","    # Save examples to CSV\n","    examples_df = pd.DataFrame(examples)\n","    examples_file = os.path.join(save_dir, \"evaluation_examples.csv\")\n","    examples_df.to_csv(examples_file, index=False)\n","    print(f\"Saved evaluation examples to: {examples_file}\")\n","\n","    return metrics, examples"],"metadata":{"id":"Ee1mUo63W34k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation code for one song"],"metadata":{"id":"o00OiQtay258"}},{"cell_type":"code","source":["import os\n","from transformers import BartForConditionalGeneration, BartTokenizer\n","import pandas as pd\n","\n","# Define paths\n","root_path = \"/content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned\"\n","output_path = \"/content/drive/MyDrive/266 Final Project/Evaluation Results\"\n","\n","# List of experiment directories\n","experiment_dirs = [\n","    os.path.join(root_path, \"Base_Fine-Tuning\"),\n","    os.path.join(root_path, \"Data_Augmentation\"),\n","    os.path.join(root_path, \"Hyperparameter_Tuning\")\n","]\n","\n","# Ensure output directory exists\n","os.makedirs(output_path, exist_ok=True)\n","\n","# Assuming `test_df` is already loaded as a DataFrame\n","# Replace with the actual loading method if needed (e.g., pd.read_csv)\n","\n","# Find song ID 141615 in the test dataset\n","song_id = 141615\n","song_row = test_df[test_df['Song ID'] == song_id]\n","\n","if song_row.empty:\n","    print(f\"Song ID {song_id} not found in the test dataset.\")\n","else:\n","    lyrics = song_row.iloc[0]['Lyrics']  # Adjust 'Lyrics' column name as necessary\n","    print(f\"Running evaluation for Song ID {song_id}...\\n\")\n","\n","    # Iterate over experiment directories\n","    for experiment_dir in experiment_dirs:\n","        print(f\"Evaluating model from: {experiment_dir}\")\n","\n","        # Load tokenizer and model\n","        tokenizer = BartTokenizer.from_pretrained(experiment_dir)\n","        model = BartForConditionalGeneration.from_pretrained(experiment_dir)\n","\n","        # Tokenize the lyrics\n","        inputs = tokenizer(lyrics, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","        # Generate annotation\n","        outputs = model.generate(inputs['input_ids'], max_length=150, num_beams=5, early_stopping=True)\n","        generated_annotation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Print the generated annotation\n","        print(f\"Generated Annotation for Song ID {song_id} from {os.path.basename(experiment_dir)}:\\n{generated_annotation}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2A2_N6rvK9-D","executionInfo":{"status":"ok","timestamp":1733643495136,"user_tz":480,"elapsed":858,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"720e396f-4aa4-461f-9e42-d171d5dd93e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Song ID 141615 not found in the test dataset.\n"]}]},{"cell_type":"markdown","source":["# Evaluation code for test df"],"metadata":{"id":"pIyQrKHKy8ae"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","from tqdm import tqdm\n","from typing import Dict, List, Tuple\n","from bert_score import score\n","from rouge_score import rouge_scorer\n","\n","# Define paths\n","root_path = \"/content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned\"\n","output_path = \"/content/drive/MyDrive/266 Final Project/Evaluation Results\"\n","\n","# List of experiment directories\n","experiment_dirs = [\n","    os.path.join(root_path, \"Base_Fine-Tuning\"),\n","    os.path.join(root_path, \"Data_Augmentation\"),\n","    os.path.join(root_path, \"Hyperparameter_Tuning\")\n","]\n","\n","# Ensure output directory exists\n","os.makedirs(output_path, exist_ok=True)\n","\n","def run_evaluation(model_path: str, tokenizer_path: str, test_df: pd.DataFrame, save_dir: str):\n","    \"\"\"Run evaluation and save results for a given model and tokenizer path\"\"\"\n","    # Load model and tokenizer\n","    tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n","    model = BartForConditionalGeneration.from_pretrained(model_path)\n","    print(f\"Model loaded from {model_path}, tokenizer loaded from {tokenizer_path}!\")\n","\n","    # Run evaluation\n","    metrics, examples = evaluate_supervised_model(\n","        model=model,\n","        tokenizer=tokenizer,\n","        test_data=test_df,\n","        batch_size=16\n","    )\n","\n","    # Create save directory\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Save metrics to CSV\n","    metrics_df = pd.DataFrame([metrics])\n","    metrics_csv_path = os.path.join(save_dir, \"evaluation_metrics.csv\")\n","    metrics_df.to_csv(metrics_csv_path, index=False)\n","    print(f\"Metrics saved to {metrics_csv_path}\")\n","\n","    # Save examples to CSV\n","    examples_df = pd.DataFrame(examples)\n","    examples_csv_path = os.path.join(save_dir, \"evaluation_examples.csv\")\n","    examples_df.to_csv(examples_csv_path, index=False)\n","    print(f\"Examples saved to {examples_csv_path}\")\n","\n","    return metrics, examples\n","\n","# Evaluate each experiment\n","for experiment_dir in experiment_dirs:\n","    experiment_name = os.path.basename(experiment_dir)\n","    model_path = os.path.join(experiment_dir, \"fine_tuned_model\")\n","    tokenizer_path = os.path.join(experiment_dir, \"fine_tuned_tokenizer\")\n","    save_dir = os.path.join(output_path, experiment_name)\n","\n","    print(f\"\\nEvaluating experiment: {experiment_name}\")\n","\n","    try:\n","        metrics, examples = run_evaluation(model_path, tokenizer_path, test_df, save_dir)\n","    except Exception as e:\n","        print(f\"Error evaluating {experiment_name}: {str(e)}\")\n","        continue"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TA3mT4mgW-pb","executionInfo":{"status":"ok","timestamp":1733615629326,"user_tz":480,"elapsed":1230184,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"80efcf81-46af-4309-ace5-637f41d4af88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating experiment: Base_Fine-Tuning\n","Model loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Base_Fine-Tuning/fine_tuned_model, tokenizer loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Base_Fine-Tuning/fine_tuned_tokenizer!\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/38 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  3%|▎         | 1/38 [00:09<06:09,  9.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/38 [00:19<05:58,  9.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/38 [00:29<05:49,  9.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 11%|█         | 4/38 [00:40<05:50, 10.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 13%|█▎        | 5/38 [00:51<05:40, 10.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 16%|█▌        | 6/38 [01:01<05:35, 10.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/38 [01:12<05:24, 10.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 21%|██        | 8/38 [01:22<05:09, 10.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 24%|██▎       | 9/38 [01:33<05:10, 10.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 26%|██▋       | 10/38 [01:43<04:47, 10.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 29%|██▉       | 11/38 [01:53<04:37, 10.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▏      | 12/38 [02:04<04:35, 10.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 34%|███▍      | 13/38 [02:15<04:25, 10.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 37%|███▋      | 14/38 [02:25<04:09, 10.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 39%|███▉      | 15/38 [02:34<03:52, 10.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▏     | 16/38 [02:46<03:51, 10.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▍     | 17/38 [02:56<03:41, 10.55s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 47%|████▋     | 18/38 [03:07<03:31, 10.57s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 19/38 [03:18<03:23, 10.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 53%|█████▎    | 20/38 [03:27<03:05, 10.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 21/38 [03:37<02:53, 10.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 58%|█████▊    | 22/38 [03:49<02:50, 10.65s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 61%|██████    | 23/38 [03:59<02:35, 10.36s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 63%|██████▎   | 24/38 [04:09<02:24, 10.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 66%|██████▌   | 25/38 [04:18<02:10, 10.07s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 26/38 [04:29<02:02, 10.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 71%|███████   | 27/38 [04:40<01:53, 10.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 74%|███████▎  | 28/38 [04:49<01:40, 10.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 76%|███████▋  | 29/38 [04:59<01:30, 10.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 79%|███████▉  | 30/38 [05:09<01:19,  9.94s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▏ | 31/38 [05:18<01:08,  9.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 84%|████████▍ | 32/38 [05:28<00:59,  9.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 87%|████████▋ | 33/38 [05:39<00:50, 10.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 89%|████████▉ | 34/38 [05:49<00:40, 10.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▏| 35/38 [05:59<00:30, 10.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▍| 36/38 [06:10<00:20, 10.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 97%|█████████▋| 37/38 [06:20<00:10, 10.12s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 38/38 [06:23<00:00, 10.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Metrics saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Base_Fine-Tuning/evaluation_metrics.csv\n","Examples saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Base_Fine-Tuning/evaluation_examples.csv\n","\n","Evaluating experiment: Data_Augmentation\n","Model loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Data_Augmentation/fine_tuned_model, tokenizer loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Data_Augmentation/fine_tuned_tokenizer!\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/38 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  3%|▎         | 1/38 [00:12<07:27, 12.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/38 [00:23<07:10, 11.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/38 [00:35<06:54, 11.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 11%|█         | 4/38 [00:47<06:37, 11.68s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 13%|█▎        | 5/38 [00:58<06:24, 11.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 16%|█▌        | 6/38 [01:10<06:16, 11.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/38 [01:22<06:05, 11.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 21%|██        | 8/38 [01:34<05:52, 11.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 24%|██▎       | 9/38 [01:45<05:35, 11.55s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 26%|██▋       | 10/38 [01:57<05:26, 11.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 29%|██▉       | 11/38 [02:08<05:14, 11.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▏      | 12/38 [02:20<05:04, 11.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 34%|███▍      | 13/38 [02:31<04:42, 11.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 37%|███▋      | 14/38 [02:42<04:34, 11.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 39%|███▉      | 15/38 [02:54<04:25, 11.52s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▏     | 16/38 [03:06<04:13, 11.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▍     | 17/38 [03:18<04:07, 11.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 47%|████▋     | 18/38 [03:29<03:53, 11.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 19/38 [03:41<03:42, 11.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 53%|█████▎    | 20/38 [03:53<03:31, 11.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 21/38 [04:04<03:15, 11.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 58%|█████▊    | 22/38 [04:15<03:04, 11.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 61%|██████    | 23/38 [04:27<02:52, 11.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 63%|██████▎   | 24/38 [04:37<02:36, 11.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 66%|██████▌   | 25/38 [04:49<02:26, 11.23s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 26/38 [05:00<02:14, 11.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 71%|███████   | 27/38 [05:12<02:04, 11.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 74%|███████▎  | 28/38 [05:23<01:54, 11.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 76%|███████▋  | 29/38 [05:35<01:42, 11.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 79%|███████▉  | 30/38 [05:46<01:31, 11.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▏ | 31/38 [05:57<01:19, 11.40s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 84%|████████▍ | 32/38 [06:09<01:08, 11.42s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 87%|████████▋ | 33/38 [06:20<00:57, 11.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 89%|████████▉ | 34/38 [06:32<00:46, 11.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▏| 35/38 [06:44<00:34, 11.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▍| 36/38 [06:55<00:22, 11.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 97%|█████████▋| 37/38 [07:07<00:11, 11.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 38/38 [07:11<00:00, 11.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Metrics saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Data_Augmentation/evaluation_metrics.csv\n","Examples saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Data_Augmentation/evaluation_examples.csv\n","\n","Evaluating experiment: Hyperparameter_Tuning\n","Model loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Hyperparameter_Tuning/fine_tuned_model, tokenizer loaded from /content/drive/MyDrive/266 Final Project/Our Models/BART Fine Tuned/Hyperparameter_Tuning/fine_tuned_tokenizer!\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/38 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  3%|▎         | 1/38 [00:10<06:18, 10.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/38 [00:19<05:51,  9.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/38 [00:31<06:12, 10.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 11%|█         | 4/38 [00:42<06:12, 10.94s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 13%|█▎        | 5/38 [00:55<06:17, 11.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 16%|█▌        | 6/38 [01:06<06:10, 11.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/38 [01:18<06:01, 11.65s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 21%|██        | 8/38 [01:29<05:39, 11.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 24%|██▎       | 9/38 [01:39<05:22, 11.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 26%|██▋       | 10/38 [01:51<05:15, 11.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 29%|██▉       | 11/38 [02:03<05:05, 11.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▏      | 12/38 [02:14<04:54, 11.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 34%|███▍      | 13/38 [02:26<04:45, 11.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 37%|███▋      | 14/38 [02:36<04:25, 11.08s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 39%|███▉      | 15/38 [02:46<04:10, 10.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▏     | 16/38 [02:58<04:02, 11.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▍     | 17/38 [03:09<03:53, 11.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 47%|████▋     | 18/38 [03:19<03:38, 10.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 19/38 [03:30<03:26, 10.86s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 53%|█████▎    | 20/38 [03:42<03:19, 11.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 21/38 [03:53<03:07, 11.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 58%|█████▊    | 22/38 [04:02<02:49, 10.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 61%|██████    | 23/38 [04:14<02:43, 10.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 63%|██████▎   | 24/38 [04:24<02:30, 10.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 66%|██████▌   | 25/38 [04:36<02:22, 11.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 26/38 [04:48<02:14, 11.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 71%|███████   | 27/38 [04:57<01:59, 10.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 74%|███████▎  | 28/38 [05:09<01:48, 10.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 76%|███████▋  | 29/38 [05:20<01:40, 11.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 79%|███████▉  | 30/38 [05:32<01:30, 11.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▏ | 31/38 [05:42<01:15, 10.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 84%|████████▍ | 32/38 [05:53<01:05, 10.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 87%|████████▋ | 33/38 [06:03<00:53, 10.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 89%|████████▉ | 34/38 [06:15<00:44, 11.08s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▏| 35/38 [06:25<00:32, 10.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▍| 36/38 [06:35<00:21, 10.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 97%|█████████▋| 37/38 [06:47<00:10, 10.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 38/38 [06:51<00:00, 10.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Metrics saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Hyperparameter_Tuning/evaluation_metrics.csv\n","Examples saved to /content/drive/MyDrive/266 Final Project/Evaluation Results/Hyperparameter_Tuning/evaluation_examples.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}