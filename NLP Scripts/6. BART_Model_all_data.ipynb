{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0724fca60cd94f818b84c730ff27a7a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c2f6e6c455f4ba2b27c8d188e6a32be","IPY_MODEL_dcbe62df91b24fb1bc94070d13d27953","IPY_MODEL_8ecb590b07824e2daa60a88aaea99657"],"layout":"IPY_MODEL_237226d1af15433c95ea11214d78f325"}},"5c2f6e6c455f4ba2b27c8d188e6a32be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45791055984c4fa288211311058aba23","placeholder":"​","style":"IPY_MODEL_8bdd4271d6b34effa5169f80b62f0d5d","value":"Sanity Checking DataLoader 0: 100%"}},"dcbe62df91b24fb1bc94070d13d27953":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d11b01d3c3a0402993af69c82d2bbf4b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_585007153b7a47db82e409d6611a1237","value":2}},"8ecb590b07824e2daa60a88aaea99657":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e726119307194e069864b3869db65710","placeholder":"​","style":"IPY_MODEL_4be10bfab6fb4902bdadf5f4d819ec5e","value":" 2/2 [00:08&lt;00:00,  0.25it/s]"}},"237226d1af15433c95ea11214d78f325":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"45791055984c4fa288211311058aba23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bdd4271d6b34effa5169f80b62f0d5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d11b01d3c3a0402993af69c82d2bbf4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"585007153b7a47db82e409d6611a1237":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e726119307194e069864b3869db65710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4be10bfab6fb4902bdadf5f4d819ec5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b84bbad1c94427aaf0e7ec7f0275fff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba2ccfc03c8c4fbc8c43c8d3e3475e0e","IPY_MODEL_6b7487d4a3094db09dbb776947b26dc2","IPY_MODEL_091ce816b90a4967bb64cde5f4f777b0"],"layout":"IPY_MODEL_60bb9d4a2a3142d184c834f2ee24fb20"}},"ba2ccfc03c8c4fbc8c43c8d3e3475e0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4868c67f741f40af922ca06a04dc09f0","placeholder":"​","style":"IPY_MODEL_c1c82a338025435ca2b35d2c1f2fab8d","value":"Epoch 2: 100%"}},"6b7487d4a3094db09dbb776947b26dc2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08c0901ca8704d45b7e092d5b7fad128","max":240,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06da4b5c5b734df19e8ce0e0cc8fbae0","value":240}},"091ce816b90a4967bb64cde5f4f777b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e28dffccc5564682b7d01094351f48d3","placeholder":"​","style":"IPY_MODEL_e53240c49e7042abbc1644b8fd15a345","value":" 240/240 [29:08&lt;00:00,  0.14it/s, v_num=0]"}},"60bb9d4a2a3142d184c834f2ee24fb20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"4868c67f741f40af922ca06a04dc09f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1c82a338025435ca2b35d2c1f2fab8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08c0901ca8704d45b7e092d5b7fad128":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06da4b5c5b734df19e8ce0e0cc8fbae0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e28dffccc5564682b7d01094351f48d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e53240c49e7042abbc1644b8fd15a345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a09ffd53c508497e96022137e9b61744":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7398238c204e4fed8f79f5da7f8596b6","IPY_MODEL_2a9d26e9e00c4709a2dd85be1a385448","IPY_MODEL_773cca0ea6ad46be9c4f465b52b35f83"],"layout":"IPY_MODEL_323cd485a303443b9951e098f13135aa"}},"7398238c204e4fed8f79f5da7f8596b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2e0b10a25f44687907107d31809edf1","placeholder":"​","style":"IPY_MODEL_529ba1cfebc143d2ba532aee2fd9c172","value":"Validation DataLoader 0: 100%"}},"2a9d26e9e00c4709a2dd85be1a385448":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4459e422e424b7183f1744a15d50b33","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_940935dc56be4fee83c40a2376901196","value":60}},"773cca0ea6ad46be9c4f465b52b35f83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_befd979daad04fb7b20806a0cea0fb45","placeholder":"​","style":"IPY_MODEL_ba17f423bde84c59961891fdceb30d53","value":" 60/60 [01:20&lt;00:00,  0.75it/s]"}},"323cd485a303443b9951e098f13135aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"a2e0b10a25f44687907107d31809edf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529ba1cfebc143d2ba532aee2fd9c172":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4459e422e424b7183f1744a15d50b33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"940935dc56be4fee83c40a2376901196":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"befd979daad04fb7b20806a0cea0fb45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba17f423bde84c59961891fdceb30d53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a50965d79124b8ca0374702cb557958":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_312adb2425df4b2bb012e25127b4c6e0","IPY_MODEL_d882b12342f44675a68790c3eb5e3081","IPY_MODEL_664fca4dcbd649e398c49bc5d2f088b7"],"layout":"IPY_MODEL_28666c030a26419a9d91f080aa619ff6"}},"312adb2425df4b2bb012e25127b4c6e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c7b430d1f0483ba2b6a2857ab81405","placeholder":"​","style":"IPY_MODEL_3e722243b82a410597e6ad8911810b19","value":"Validation DataLoader 0: 100%"}},"d882b12342f44675a68790c3eb5e3081":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa0c108a8441405785b4be0a7b43a9d0","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18378f504c034ec0badfab23fedeb591","value":60}},"664fca4dcbd649e398c49bc5d2f088b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81c1edffd2f94eecb764dc09f529f711","placeholder":"​","style":"IPY_MODEL_ad35afa38f2041fab05a3f4c3af49131","value":" 60/60 [01:19&lt;00:00,  0.75it/s]"}},"28666c030a26419a9d91f080aa619ff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"74c7b430d1f0483ba2b6a2857ab81405":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e722243b82a410597e6ad8911810b19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa0c108a8441405785b4be0a7b43a9d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18378f504c034ec0badfab23fedeb591":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81c1edffd2f94eecb764dc09f529f711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad35afa38f2041fab05a3f4c3af49131":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b611112126004b858be5a067ddb41605":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c0156f86b88426db9a30cfc3d2f52a8","IPY_MODEL_dd7385bccb8745f7bc1334048e0504df","IPY_MODEL_a45e4308e3ce4ca7a7bfc00576285850"],"layout":"IPY_MODEL_e1b9f1f5ba12450d9761de6a1021a40b"}},"4c0156f86b88426db9a30cfc3d2f52a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe91ddfcec774fcd9199087aa765e048","placeholder":"​","style":"IPY_MODEL_ebe979822f54425f9039e4ea48f28fe7","value":"Validation DataLoader 0: 100%"}},"dd7385bccb8745f7bc1334048e0504df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa1ca5b8d57446098c7b9ad5343b71ef","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c1360f8120e48fdad1756f22ca294e2","value":60}},"a45e4308e3ce4ca7a7bfc00576285850":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be91d57b13664a7c82b7402e19ff75f3","placeholder":"​","style":"IPY_MODEL_944f2d2cb6da44019f0dbfcdf3b78368","value":" 60/60 [01:21&lt;00:00,  0.74it/s]"}},"e1b9f1f5ba12450d9761de6a1021a40b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"fe91ddfcec774fcd9199087aa765e048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebe979822f54425f9039e4ea48f28fe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa1ca5b8d57446098c7b9ad5343b71ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c1360f8120e48fdad1756f22ca294e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be91d57b13664a7c82b7402e19ff75f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"944f2d2cb6da44019f0dbfcdf3b78368":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1af67d1ba5de4b50a865711cc7a23e4d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ca9378f771841d28e9293c3bac579a9","IPY_MODEL_c32adaa0581c488cab212b6f8fdbacfb","IPY_MODEL_b23930b3c1aa407f9d29805d594f53ea"],"layout":"IPY_MODEL_8e31235e62ef45ab861b9e0f2e471cea"}},"1ca9378f771841d28e9293c3bac579a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e6ba71dbe1d409d9cd6e40f9a299dea","placeholder":"​","style":"IPY_MODEL_65e723d02f4340f2b9d4e1273c46f8fb","value":"tokenizer_config.json: 100%"}},"c32adaa0581c488cab212b6f8fdbacfb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4597095cddc40bba060b4894ceef897","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc0c21c20e134878a778d7e24c5170b9","value":25}},"b23930b3c1aa407f9d29805d594f53ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43ced1a598b14a4d8ecbb82d29122062","placeholder":"​","style":"IPY_MODEL_367c9982dac54ee493e1802d50080aba","value":" 25.0/25.0 [00:00&lt;00:00, 2.44kB/s]"}},"8e31235e62ef45ab861b9e0f2e471cea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6ba71dbe1d409d9cd6e40f9a299dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65e723d02f4340f2b9d4e1273c46f8fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4597095cddc40bba060b4894ceef897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc0c21c20e134878a778d7e24c5170b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43ced1a598b14a4d8ecbb82d29122062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"367c9982dac54ee493e1802d50080aba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"463a75401c41413e87f575abfc5d52ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6372ae77a7e641118bc5e3dd7a5d7ad0","IPY_MODEL_75f370caa0d74017abb367ddbe605dc4","IPY_MODEL_1476c28e3d7d475c8abbfa48e4a7b9bd"],"layout":"IPY_MODEL_5d34d55bca8b4305a89ec63d0abbee6d"}},"6372ae77a7e641118bc5e3dd7a5d7ad0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cdc5478f1424211bc5b6bc28a3529d6","placeholder":"​","style":"IPY_MODEL_6e3b09944c5843ec890de7e2a01134bc","value":"config.json: 100%"}},"75f370caa0d74017abb367ddbe605dc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d03c9a4e2c6f475d894f6fe5c2113f84","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e3ee3f9176742debce9a132fda3e203","value":482}},"1476c28e3d7d475c8abbfa48e4a7b9bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45d693dcdef44d85b8bd5a2bd858c950","placeholder":"​","style":"IPY_MODEL_12e2dac4eae24ceba88805c5dac25b7d","value":" 482/482 [00:00&lt;00:00, 52.4kB/s]"}},"5d34d55bca8b4305a89ec63d0abbee6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cdc5478f1424211bc5b6bc28a3529d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e3b09944c5843ec890de7e2a01134bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d03c9a4e2c6f475d894f6fe5c2113f84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e3ee3f9176742debce9a132fda3e203":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45d693dcdef44d85b8bd5a2bd858c950":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e2dac4eae24ceba88805c5dac25b7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87222a5b40dc48089b66519e1856c892":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3eba43be7d35415e9935ad634f6a5517","IPY_MODEL_3120cf7f9f03488b8a0101a35bed34a8","IPY_MODEL_9b772286aa4940f9a97b365607119211"],"layout":"IPY_MODEL_455d244ccad54a0da46db50d9600eb58"}},"3eba43be7d35415e9935ad634f6a5517":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e029535f6daa41cf966497189086c849","placeholder":"​","style":"IPY_MODEL_810bc5c467964bbc866561676d1bda26","value":"vocab.json: 100%"}},"3120cf7f9f03488b8a0101a35bed34a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc85c8edb2a34796ac4d5dfed9a81e94","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71ff016389e445b8b06701d3e5b2970a","value":898823}},"9b772286aa4940f9a97b365607119211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a88ed2aee5b94588a54d476ba95b238c","placeholder":"​","style":"IPY_MODEL_99f763d6446145528a1b7725cee66ecb","value":" 899k/899k [00:00&lt;00:00, 14.7MB/s]"}},"455d244ccad54a0da46db50d9600eb58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e029535f6daa41cf966497189086c849":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"810bc5c467964bbc866561676d1bda26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc85c8edb2a34796ac4d5dfed9a81e94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71ff016389e445b8b06701d3e5b2970a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a88ed2aee5b94588a54d476ba95b238c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99f763d6446145528a1b7725cee66ecb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccbcd095c48d458c8dc1b626d52235bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0c2f91ccbff4a6b97baec739aa487d7","IPY_MODEL_ee7b8a9bba474ba2bf15e4fbd0a60748","IPY_MODEL_e2e3c7c0ea684bdf9bd87d3c6108d22a"],"layout":"IPY_MODEL_0f7c68f4b50e4086b1b6a46bd727e148"}},"b0c2f91ccbff4a6b97baec739aa487d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f49849862d3a4bb1b54832f329f89892","placeholder":"​","style":"IPY_MODEL_c68dcb555f8c4429960ed48edb66c4b2","value":"merges.txt: 100%"}},"ee7b8a9bba474ba2bf15e4fbd0a60748":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_023cdfbdae1a4a8ca98e73be18f4daa9","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25e3bcb4eb1940178553e788346d32b5","value":456318}},"e2e3c7c0ea684bdf9bd87d3c6108d22a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8443315dc49a43c5a259c1c7967cf945","placeholder":"​","style":"IPY_MODEL_a6b2a117e2a24583b8a7810538ecd631","value":" 456k/456k [00:00&lt;00:00, 13.1MB/s]"}},"0f7c68f4b50e4086b1b6a46bd727e148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f49849862d3a4bb1b54832f329f89892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c68dcb555f8c4429960ed48edb66c4b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"023cdfbdae1a4a8ca98e73be18f4daa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25e3bcb4eb1940178553e788346d32b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8443315dc49a43c5a259c1c7967cf945":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6b2a117e2a24583b8a7810538ecd631":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d377ec3a518140808d435d3308114294":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1434b9d737b94de8a1fa8e381645cb81","IPY_MODEL_a31779dd3f3a433581e3bbfc8bcd254f","IPY_MODEL_13bffb3372f74c52a01826c96be4fe06"],"layout":"IPY_MODEL_454427b4c84e41ed8a9771ce9f7c4b9b"}},"1434b9d737b94de8a1fa8e381645cb81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ff26bab2f11486586d59006bdcb074d","placeholder":"​","style":"IPY_MODEL_2ae829b84d1542eebb736cccb70dd2c6","value":"tokenizer.json: 100%"}},"a31779dd3f3a433581e3bbfc8bcd254f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_864915fdb18b49c48ba166d82b0b86bb","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8b5fbcf35d64e30b9b7cc184ca033a4","value":1355863}},"13bffb3372f74c52a01826c96be4fe06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d461667083294520a48c810ecff60efe","placeholder":"​","style":"IPY_MODEL_3fb0b28f55a440e7bea511358b558f96","value":" 1.36M/1.36M [00:00&lt;00:00, 26.4MB/s]"}},"454427b4c84e41ed8a9771ce9f7c4b9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ff26bab2f11486586d59006bdcb074d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ae829b84d1542eebb736cccb70dd2c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"864915fdb18b49c48ba166d82b0b86bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8b5fbcf35d64e30b9b7cc184ca033a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d461667083294520a48c810ecff60efe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fb0b28f55a440e7bea511358b558f96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f339c376ec24491cb649c6a7ba21800b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_244667cec0cd4e9f82a4782899fba2b8","IPY_MODEL_a82c89d4e2a94361af1c9283ec44084b","IPY_MODEL_2407339844354695a4bcea42d8b6100b"],"layout":"IPY_MODEL_74034e29cdff4a8297b49074cfe6cada"}},"244667cec0cd4e9f82a4782899fba2b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_600f0513af034cd5857692e3a0f65ac1","placeholder":"​","style":"IPY_MODEL_6a911f47081940bfbd9e2d0a1a0bd878","value":"model.safetensors: 100%"}},"a82c89d4e2a94361af1c9283ec44084b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_050db5a779c1453589b5856ead8459d3","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d98d4929f7384427ab9321e1f313d369","value":1421700479}},"2407339844354695a4bcea42d8b6100b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6f99845abd4bd8b7ce7296d3e82746","placeholder":"​","style":"IPY_MODEL_aec11531fdb7421aa0db4e89dac0aeac","value":" 1.42G/1.42G [00:06&lt;00:00, 238MB/s]"}},"74034e29cdff4a8297b49074cfe6cada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"600f0513af034cd5857692e3a0f65ac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a911f47081940bfbd9e2d0a1a0bd878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"050db5a779c1453589b5856ead8459d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d98d4929f7384427ab9321e1f313d369":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d6f99845abd4bd8b7ce7296d3e82746":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aec11531fdb7421aa0db4e89dac0aeac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# BART Model\n","\n","### Note: this does not contain fine tuning code"],"metadata":{"id":"lEMPcuI80zve"}},{"cell_type":"markdown","source":["# Import Statements"],"metadata":{"id":"c-0ynd7Y07lB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WB94TiO1Pbs","executionInfo":{"status":"ok","timestamp":1733441010539,"user_tz":480,"elapsed":21644,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"e991e839-47e3-4045-ba08-4da60b416318"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting tensorflow-cpu\n","  Using cached tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.68.1)\n","Collecting tensorboard<2.19,>=2.18 (from tensorflow-cpu)\n","  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting keras>=3.5.0 (from tensorflow-cpu)\n","  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.26.4)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.12.1)\n","Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-cpu)\n","  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow-cpu) (13.9.4)\n","Collecting namex (from keras>=3.5.0->tensorflow-cpu)\n","  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n","Collecting optree (from keras>=3.5.0->tensorflow-cpu)\n","  Using cached optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-cpu) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n","Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Downloading optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.3/381.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: namex, optree, ml-dtypes, tensorboard, keras, tensorflow-cpu\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","Successfully installed keras-3.7.0 ml-dtypes-0.4.1 namex-0.0.8 optree-0.13.1 tensorboard-2.18.0 tensorflow-cpu-2.18.0\n"]}],"source":["# Fix tensorflow conflict and install required packages\n","!pip uninstall -y tensorflow\n","!pip install tensorflow-cpu\n","!pip install -q pytorch-lightning transformers torch bert-score rouge-score scikit-learn"]},{"cell_type":"code","source":["# Imports\n","import torch\n","import pandas as pd\n","import numpy as np\n","import pytorch_lightning as pl\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from sklearn.model_selection import train_test_split\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","from tqdm import tqdm\n","import math\n","import random\n","import re\n","import os\n","from typing import List, Dict, Tuple"],"metadata":{"id":"W2JLO4YQIP2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9UiJe6Z21vF","executionInfo":{"status":"ok","timestamp":1733441033739,"user_tz":480,"elapsed":1147,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"26742a9a-d568-413d-ac55-6bc4cbcee2d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Check device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tCket9G1n4Q","executionInfo":{"status":"ok","timestamp":1733441036701,"user_tz":480,"elapsed":120,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"0a7f5b61-8f2f-483a-eaee-3a2d55ddc520"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"zLJbEf3c0_fB"}},{"cell_type":"code","source":["# Load lyrics data\n","print(\"Loading lyrics data from Google Drive...\")\n","df_list = []\n","lyrics_folder_path = \"/content/drive/My Drive/266 Final Project/Song Files\"\n","for filename in os.listdir(lyrics_folder_path):\n","    if filename.endswith('.csv'):\n","        file_path = os.path.join(lyrics_folder_path, filename)\n","        df = pd.read_csv(file_path)\n","        df_list.append(df)\n","\n","lyrics_df = pd.concat(df_list, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCV9Gw8EDHDT","executionInfo":{"status":"ok","timestamp":1733441309327,"user_tz":480,"elapsed":16968,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"5bca6865-bc8b-43eb-d8e3-8d5d14ce2d65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading lyrics data from Google Drive...\n"]}]},{"cell_type":"code","source":["# Load poetry data\n","print(\"\\nLoading poetry data...\")\n","poem_list = []\n","poetry_files = {\n","    'test': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_test.csv\",\n","    'train': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_train.csv\",\n","    'valid': \"/content/drive/My Drive/266 Final Project/PoemSum Model/poemsum_valid.csv\"\n","}\n","\n","for dataset_type, filepath in poetry_files.items():\n","    print(f\"Loading poetry {dataset_type} dataset\")\n","    poem_data = pd.read_csv(filepath)\n","    poem_list.append(poem_data)\n","\n","poem_df = pd.concat(poem_list, ignore_index=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1StqFCI6DKnb","executionInfo":{"status":"ok","timestamp":1733441310391,"user_tz":480,"elapsed":1066,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"69539477-9376-4730-97ed-00b74ba1ca7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading poetry data...\n","Loading poetry test dataset\n","Loading poetry train dataset\n","Loading poetry valid dataset\n"]}]},{"cell_type":"code","source":["# Split datasets\n","print(\"\\nSplitting datasets...\")\n","train_val_df, test_df = train_test_split(lyrics_df, test_size=0.2, random_state=42)\n","train_val_poem, test_poem = train_test_split(poem_df, test_size=0.2, random_state=42)\n","\n","print(f\"Lyrics data split - Training+Validation: {len(train_val_df)}, Test: {len(test_df)}\")\n","print(f\"Poetry data split - Training+Validation: {len(train_val_poem)}, Test: {len(test_poem)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4xTk33WDYuF","executionInfo":{"status":"ok","timestamp":1733441312432,"user_tz":480,"elapsed":124,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"0aa9bb40-8195-4fe8-c53e-ec7ba63b879c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Splitting datasets...\n","Lyrics data split - Training+Validation: 2385, Test: 597\n","Poetry data split - Training+Validation: 2408, Test: 603\n"]}]},{"cell_type":"markdown","source":["# Model Definition"],"metadata":{"id":"_j0T4ZLs1CFK"}},{"cell_type":"code","source":["class BARTDataModule(pl.LightningDataModule):\n","    def __init__(self, train_df, val_df, tokenizer, batch_size=16, max_length=512):\n","        super().__init__()\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.tokenizer = tokenizer\n","        self.batch_size = batch_size\n","        self.max_length = max_length\n","\n","    def setup(self, stage=None):\n","        self.train_encodings = self._encode_data(self.train_df)\n","        self.val_encodings = self._encode_data(self.val_df)\n","\n","    def _encode_data(self, df):\n","        source_encodings = self.tokenizer(\n","            df['source'].tolist(),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        target_encodings = self.tokenizer(\n","            df['target'].tolist(),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': source_encodings['input_ids'],\n","            'attention_mask': source_encodings['attention_mask'],\n","            'labels': target_encodings['input_ids']\n","        }\n","\n","    def train_dataloader(self):\n","        dataset = TensorDataset(\n","            self.train_encodings['input_ids'],\n","            self.train_encodings['attention_mask'],\n","            self.train_encodings['labels']\n","        )\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        dataset = TensorDataset(\n","            self.val_encodings['input_ids'],\n","            self.val_encodings['attention_mask'],\n","            self.val_encodings['labels']\n","        )\n","        return DataLoader(dataset, batch_size=self.batch_size)\n","\n","class BARTLitModel(pl.LightningModule):\n","    def __init__(self, model, learning_rate=2e-5):\n","        super().__init__()\n","        self.model = model\n","        self.learning_rate = learning_rate\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        return outputs\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch\n","        outputs = self(input_ids, attention_mask, labels)\n","        self.log('train_loss', outputs.loss)\n","        return outputs.loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch\n","        outputs = self(input_ids, attention_mask, labels)\n","        self.log('val_loss', outputs.loss)\n","        return outputs.loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"],"metadata":{"id":"-neijiKuKQ5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Evaluation Code"],"metadata":{"id":"cRuQ5DgF1RaX"}},{"cell_type":"code","source":["def calculate_content_coverage(text: str, summary: str) -> float:\n","    \"\"\"Calculate content coverage based on token overlap.\"\"\"\n","    text_tokens = set(text.lower().split())\n","    summary_tokens = set(summary.lower().split())\n","    overlap = len(text_tokens.intersection(summary_tokens))\n","    return overlap / len(text_tokens) if text_tokens else 0.0\n","\n","def calculate_consistency_score(summaries: List[str]) -> float:\n","    \"\"\"Calculate consistency between multiple generations using ROUGE scores.\"\"\"\n","    if len(summaries) < 2:\n","        return 1.0\n","\n","    rouge_scorer_obj = rouge_scorer.RougeScorer(\n","        ['rouge1', 'rouge2', 'rougeL'],\n","        use_stemmer=True\n","    )\n","\n","    scores = []\n","    for i in range(len(summaries)):\n","        for j in range(i + 1, len(summaries)):\n","            score = rouge_scorer_obj.score(summaries[i], summaries[j])\n","            avg_score = (\n","                score['rouge1'].fmeasure +\n","                score['rouge2'].fmeasure +\n","                score['rougeL'].fmeasure\n","            ) / 3\n","            scores.append(avg_score)\n","\n","    return np.mean(scores)\n","\n","def calculate_semantic_similarity(text: str, summary: str) -> float:\n","    \"\"\"Calculate semantic similarity using token overlap.\"\"\"\n","    text_tokens = set(text.lower().split())\n","    summary_tokens = set(summary.lower().split())\n","    intersection = len(text_tokens.intersection(summary_tokens))\n","    union = len(text_tokens.union(summary_tokens))\n","    return intersection / union if union > 0 else 0.0\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","    \"\"\"Print evaluation results and examples.\"\"\"\n","    print(\"\\nEvaluation Results:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.3f}\")\n","\n","    print(\"\\nExample Generations:\")\n","    for i, example in enumerate(examples, 1):\n","        print(f\"\\nExample {i}:\")\n","        print(f\"Original Text (truncated): {example['original_text'][:200]}...\")\n","        print(f\"Actual Summary: {example['actual_summary']}\")\n","        print(\"\\nGenerated Summaries:\")\n","        for j, summary in enumerate(example['generated_summaries'], 1):\n","            print(f\"{j}. {summary}\")\n","        print(\"\\nMetrics:\")\n","        for metric, value in example['metrics'].items():\n","            print(f\"{metric}: {value:.3f}\")"],"metadata":{"id":"Sbvnp52NEyLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_bart_model(\n","    model: BartForConditionalGeneration,\n","    tokenizer: BartTokenizer,\n","    test_data: pd.DataFrame,\n","    batch_size: int = 8\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","    \"\"\"\n","    Evaluate BART model with multiple metrics\n","    \"\"\"\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    evaluation_results = {\n","        'content_coverage': [],\n","        'consistency_score': [],\n","        'semantic_similarity': [],\n","        'bert_score': []\n","    }\n","\n","    examples = []\n","\n","    for idx in tqdm(range(0, len(test_data), batch_size)):\n","        batch_texts = test_data['source'].iloc[idx:idx + batch_size].tolist()\n","\n","        summaries_per_text = []\n","        for _ in range(3):  # Generate 3 summaries per text\n","            inputs = tokenizer(\n","                batch_texts,\n","                padding=True,\n","                truncation=True,\n","                max_length=512,\n","                return_tensors=\"pt\"\n","            ).to(device)\n","\n","            with torch.no_grad():\n","                outputs = model.generate(\n","                    input_ids=inputs['input_ids'],\n","                    attention_mask=inputs['attention_mask'],\n","                    max_length=150,\n","                    min_length=40,\n","                    num_beams=4,\n","                    do_sample=True,\n","                    temperature=0.3,\n","                    top_k=50,\n","                    no_repeat_ngram_size=3,\n","                    length_penalty=0.8,\n","                    repetition_penalty=1.5\n","                )\n","\n","                decoded_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","                summaries_per_text.append(decoded_summaries)\n","\n","        for text_idx in range(len(batch_texts)):\n","            original_text = batch_texts[text_idx]\n","            actual_summary = test_data['target'].iloc[idx + text_idx]\n","            text_summaries = [summaries[text_idx] for summaries in summaries_per_text]\n","\n","            coverage_score = calculate_content_coverage(original_text, text_summaries[0])\n","            consistency_score = calculate_consistency_score(text_summaries)\n","            semantic_score = calculate_semantic_similarity(original_text, text_summaries[0])\n","\n","            P, R, F1 = score(\n","                [text_summaries[0]],\n","                [actual_summary],\n","                model_type=\"microsoft/deberta-xlarge-mnli\",\n","                device=device\n","            )\n","\n","            evaluation_results['content_coverage'].append(coverage_score)\n","            evaluation_results['consistency_score'].append(consistency_score)\n","            evaluation_results['semantic_similarity'].append(semantic_score)\n","            evaluation_results['bert_score'].append(F1.mean().item())\n","\n","            if len(examples) < 5:\n","                examples.append({\n","                    'original_text': original_text,\n","                    'actual_summary': actual_summary,\n","                    'generated_summaries': text_summaries,\n","                    'metrics': {\n","                        'content_coverage': coverage_score,\n","                        'consistency': consistency_score,\n","                        'semantic_similarity': semantic_score,\n","                        'bert_score': F1.mean().item()\n","                    }\n","                })\n","\n","        torch.cuda.empty_cache()\n","\n","    metrics = {\n","        'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","        'avg_consistency': np.mean(evaluation_results['consistency_score']),\n","        'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","        'avg_bert_score': np.mean(evaluation_results['bert_score'])\n","    }\n","\n","    return metrics, examples"],"metadata":{"id":"8U54ca33E1fl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating baseline model"],"metadata":{"id":"dgJXkX061Urv"}},{"cell_type":"code","source":["def create_baseline_model(train_val_df, train_val_poem):\n","    # 1. Initialize model and tokenizer\n","    print(\"Initializing BART model and tokenizer...\")\n","    MODEL_NAME = 'facebook/bart-base'\n","    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n","    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n","\n","    # 2. Prepare lyrics data\n","    print(\"Preparing lyrics data...\")\n","    train_val_df['Combined Annotations'] = train_val_df['Combined Annotations'].astype(str)\n","    train_val_df['Combined Annotations'] = train_val_df['Combined Annotations'].fillna('')\n","\n","    lyrics_data = pd.DataFrame({\n","        'source': train_val_df.apply(\n","            lambda x: f\"summarize lyrics and capture meaning: {x['Lyrics']}\",\n","            axis=1\n","        ),\n","        'target': train_val_df['Combined Annotations'].apply(\n","            lambda x: f\"Meaning and themes: {' '.join(x.split()[:100])}\"\n","        )\n","    })\n","\n","    # 3. Prepare poem data\n","    print(\"Preparing poetry data...\")\n","    poem_data = pd.DataFrame({\n","        'source': train_val_poem.apply(\n","            lambda x: f\"summarize poem and capture meaning: {x['ctext']}\",\n","            axis=1\n","        ),\n","        'target': train_val_poem['text'].apply(\n","            lambda x: f\"Meaning and themes: {x}\"\n","        )\n","    })\n","\n","    # 4. Combine and clean data\n","    print(\"Combining and cleaning data...\")\n","    combined_data = pd.concat([lyrics_data, poem_data], ignore_index=True)\n","    combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # 5. Split data\n","    train_size = int(0.8 * len(combined_data))\n","    train_data = combined_data[:train_size]\n","    val_data = combined_data[train_size:]\n","\n","    print(f\"\\nDataset Statistics:\")\n","    print(f\"Total samples: {len(combined_data)}\")\n","    print(f\"Training samples: {len(train_data)}\")\n","    print(f\"Validation samples: {len(val_data)}\")\n","\n","    # 6. Initialize LightningModule and DataModule\n","    lit_model = BARTLitModel(model)\n","    data_module = BARTDataModule(\n","        train_df=train_data,\n","        val_df=val_data,\n","        tokenizer=tokenizer,\n","        batch_size=16\n","    )\n","\n","    # 7. Setup trainer with modified configuration\n","    trainer = pl.Trainer(\n","        max_epochs=3,\n","        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n","        devices=1,\n","        gradient_clip_val=1.0,\n","        precision=16 if torch.cuda.is_available() else 32,\n","        strategy='auto'\n","    )\n","\n","    # 8. Train model\n","    print(\"Starting training...\")\n","    trainer.fit(lit_model, data_module)\n","\n","    # 9. Save model\n","    print(\"Saving model and tokenizer...\")\n","    drive_path = '/content/drive/MyDrive/266 Final Project/Our Models/BART_All_Data'\n","    os.makedirs(drive_path, exist_ok=True)\n","\n","    try:\n","        lit_model.model.save_pretrained(drive_path)\n","        tokenizer.save_pretrained(drive_path)\n","        print(f\"Model and tokenizer successfully saved to {drive_path}\")\n","    except Exception as e:\n","        print(f\"Failed to save model and tokenizer: {e}\")\n","\n","    return lit_model, tokenizer, trainer"],"metadata":{"id":"12J4UYs_E5QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","model, tokenizer, trainer = create_baseline_model(train_val_df, train_val_poem)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":689,"referenced_widgets":["0724fca60cd94f818b84c730ff27a7a9","5c2f6e6c455f4ba2b27c8d188e6a32be","dcbe62df91b24fb1bc94070d13d27953","8ecb590b07824e2daa60a88aaea99657","237226d1af15433c95ea11214d78f325","45791055984c4fa288211311058aba23","8bdd4271d6b34effa5169f80b62f0d5d","d11b01d3c3a0402993af69c82d2bbf4b","585007153b7a47db82e409d6611a1237","e726119307194e069864b3869db65710","4be10bfab6fb4902bdadf5f4d819ec5e","9b84bbad1c94427aaf0e7ec7f0275fff","ba2ccfc03c8c4fbc8c43c8d3e3475e0e","6b7487d4a3094db09dbb776947b26dc2","091ce816b90a4967bb64cde5f4f777b0","60bb9d4a2a3142d184c834f2ee24fb20","4868c67f741f40af922ca06a04dc09f0","c1c82a338025435ca2b35d2c1f2fab8d","08c0901ca8704d45b7e092d5b7fad128","06da4b5c5b734df19e8ce0e0cc8fbae0","e28dffccc5564682b7d01094351f48d3","e53240c49e7042abbc1644b8fd15a345","a09ffd53c508497e96022137e9b61744","7398238c204e4fed8f79f5da7f8596b6","2a9d26e9e00c4709a2dd85be1a385448","773cca0ea6ad46be9c4f465b52b35f83","323cd485a303443b9951e098f13135aa","a2e0b10a25f44687907107d31809edf1","529ba1cfebc143d2ba532aee2fd9c172","c4459e422e424b7183f1744a15d50b33","940935dc56be4fee83c40a2376901196","befd979daad04fb7b20806a0cea0fb45","ba17f423bde84c59961891fdceb30d53","5a50965d79124b8ca0374702cb557958","312adb2425df4b2bb012e25127b4c6e0","d882b12342f44675a68790c3eb5e3081","664fca4dcbd649e398c49bc5d2f088b7","28666c030a26419a9d91f080aa619ff6","74c7b430d1f0483ba2b6a2857ab81405","3e722243b82a410597e6ad8911810b19","fa0c108a8441405785b4be0a7b43a9d0","18378f504c034ec0badfab23fedeb591","81c1edffd2f94eecb764dc09f529f711","ad35afa38f2041fab05a3f4c3af49131","b611112126004b858be5a067ddb41605","4c0156f86b88426db9a30cfc3d2f52a8","dd7385bccb8745f7bc1334048e0504df","a45e4308e3ce4ca7a7bfc00576285850","e1b9f1f5ba12450d9761de6a1021a40b","fe91ddfcec774fcd9199087aa765e048","ebe979822f54425f9039e4ea48f28fe7","aa1ca5b8d57446098c7b9ad5343b71ef","9c1360f8120e48fdad1756f22ca294e2","be91d57b13664a7c82b7402e19ff75f3","944f2d2cb6da44019f0dbfcdf3b78368"]},"id":"7N2Cndn6FUtB","outputId":"78f7f903-fd74-4c23-cd4e-5da809ad3ea6","executionInfo":{"status":"ok","timestamp":1733446733101,"user_tz":480,"elapsed":2008702,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Initializing BART model and tokenizer...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: True, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Preparing lyrics data...\n","Preparing poetry data...\n","Combining and cleaning data...\n","\n","Dataset Statistics:\n","Total samples: 4793\n","Training samples: 3834\n","Validation samples: 959\n","Starting training...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:183: TPU available but not used. You can set it by doing `Trainer(accelerator='tpu')`.\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                         | Params | Mode\n","--------------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 139 M  | eval\n","--------------------------------------------------------------\n","139 M     Trainable params\n","0         Non-trainable params\n","139 M     Total params\n","557.682   Total estimated model params size (MB)\n","0         Modules in train mode\n","182       Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0724fca60cd94f818b84c730ff27a7a9","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n","/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b84bbad1c94427aaf0e7ec7f0275fff","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a09ffd53c508497e96022137e9b61744","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a50965d79124b8ca0374702cb557958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b611112126004b858be5a067ddb41605"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Saving model and tokenizer...\n","Model and tokenizer successfully saved to /content/drive/MyDrive/266 Final Project/Our Models/BART_All_Data\n"]}]},{"cell_type":"markdown","source":["# Code to load the model"],"metadata":{"id":"-J2DisZA1YBz"}},{"cell_type":"code","source":["# Import necessary libraries\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","import torch\n","\n","# Path to your saved model\n","model_path = '/content/drive/MyDrive/266 Final Project/Our Models/BART_All_Data'\n","\n","# Load tokenizer and model\n","tokenizer = BartTokenizer.from_pretrained(model_path)\n","model = BartForConditionalGeneration.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n"],"metadata":{"id":"yELcQ8JSxCxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(bart_all_model))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXyXlBmhxOwb","executionInfo":{"status":"ok","timestamp":1733462895898,"user_tz":480,"elapsed":122,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"6c5407ff-4f33-4196-93a6-293356f43e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class '__main__.BARTLitModel'>\n"]}]},{"cell_type":"markdown","source":["# Example Summary Generation"],"metadata":{"id":"Tk1bKCIg1ahv"}},{"cell_type":"code","source":["def generate_song_summary(model, tokenizer, data, song_index, max_length=150):\n","    \"\"\"Generate a summary for a single song using BART\"\"\"\n","\n","    # Move model to correct device\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = model.to(device)\n","\n","    # Training input format\n","    input_text = f\"summarize lyrics and capture meaning: {data.iloc[song_index]['Lyrics']}\"\n","\n","    # Encode the text (BART-specific encoding)\n","    inputs = tokenizer(\n","        input_text,\n","        max_length=1024,  # BART's max input length\n","        truncation=True,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    ).to(device)  # Move inputs to same device as model\n","\n","    # Generate summary\n","    summary_ids = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_length=max_length,\n","        min_length=50,\n","        num_beams=5,\n","        length_penalty=0.5,\n","        early_stopping=True,\n","        no_repeat_ngram_size=2,\n","        do_sample=True,\n","        top_k=50,\n","        temperature=0.7\n","    )\n","\n","    # Decode summary\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary"],"metadata":{"id":"Vmsv8iChcNCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage:\n","summary = generate_song_summary(model, tokenizer, df, 0)\n","print(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apXvxlQUcblU","executionInfo":{"status":"ok","timestamp":1733463076137,"user_tz":480,"elapsed":2555,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"cb001ccf-c4fe-4b4b-bce2-13db50e1b5c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Meaning and themes: “My universe” is a song by American singer-songwriter . It was released as the lead single from her third studio album, . The song was first previewed at the end of the album.\n"]}]},{"cell_type":"code","source":["print(test_df.shape)\n","print(test_df.columns)\n","print(test_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NF-pFaAOd5W1","executionInfo":{"status":"ok","timestamp":1733463425506,"user_tz":480,"elapsed":123,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"bc4cb0be-94aa-42d3-869d-f035c98c7372"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(597, 6)\n","Index(['Song ID', 'Title', 'Lyrics URL', 'Combined Annotations',\n","       'Wikipedia Annotation', 'Lyrics'],\n","      dtype='object')\n","      Song ID                     Title  \\\n","2409  7222324  Santa, Can’t You Hear Me   \n","1547  2451400       Dance Like Yo Daddy   \n","881    328275             Act Like That   \n","331   2820857                   Bad Guy   \n","2225   745209                 DEATHCAMP   \n","\n","                                             Lyrics URL  \\\n","2409  https://genius.com/Kelly-clarkson-and-ariana-g...   \n","1547  https://genius.com/Meghan-trainor-dance-like-y...   \n","881   https://genius.com/French-montana-act-like-tha...   \n","331   https://genius.com/21-savage-and-metro-boomin-...   \n","2225  https://genius.com/Tyler-the-creator-deathcamp...   \n","\n","                                   Combined Annotations  \\\n","2409  The Voice coaches Ariana Grande and Kelly Clar...   \n","1547  This song is yet another anthem about bravery ...   \n","881                                                   d   \n","331   On “Bad Guy,” 21 Savage is embracing his ego. ...   \n","2225  “DEATHCAMP” is the first track off Tyler’s fou...   \n","\n","                                   Wikipedia Annotation  \\\n","2409  \"Santa, Can't You Hear Me\" is a duet by Americ...   \n","1547  No Wikipedia annotation found (artist name not...   \n","881   No Wikipedia annotation found (artist name not...   \n","331   No Wikipedia annotation found (artist name not...   \n","2225  \"Deathcamp\" is a song by American rapper Tyler...   \n","\n","                                                 Lyrics  \n","2409  Keep the snow and sleigh rides\\r\\nKeep those s...  \n","1547  Dance like yo daddy\\r\\nDance like yo daddy\\r\\n...  \n","881   [Intro]\\r\\nSee people always remember\\r\\nWhen ...  \n","331   I'm a fuckin' bad guy nigga\\r\\nMurder Gang nig...  \n","2225  Um, excuse me mister but can you please turn d...  \n"]}]},{"cell_type":"code","source":["# Check for NaN values\n","print(\"NaN values in test_df:\")\n","print(test_df.isna().sum())\n","\n","# Check data types\n","print(\"\\nData types:\")\n","print(test_df.dtypes)\n","\n","# Clean the data\n","test_df['Lyrics'] = test_df['Lyrics'].fillna('')\n","test_df['Combined Annotations'] = test_df['Combined Annotations'].fillna('')\n","\n","# Convert to string type\n","test_df['Lyrics'] = test_df['Lyrics'].astype(str)\n","test_df['Combined Annotations'] = test_df['Combined Annotations'].astype(str)\n","\n","# Verify no empty strings that might cause issues\n","print(\"\\nNumber of empty lyrics:\", len(test_df[test_df['Lyrics'] == '']))\n","print(\"Number of empty annotations:\", len(test_df[test_df['Combined Annotations'] == '']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yR4WKfdyeM2v","executionInfo":{"status":"ok","timestamp":1733463433678,"user_tz":480,"elapsed":136,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"fd5fab4c-1d55-491f-9f5c-0e1419096de9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NaN values in test_df:\n","Song ID                 0\n","Title                   0\n","Lyrics URL              0\n","Combined Annotations    1\n","Wikipedia Annotation    0\n","Lyrics                  0\n","dtype: int64\n","\n","Data types:\n","Song ID                  int64\n","Title                   object\n","Lyrics URL              object\n","Combined Annotations    object\n","Wikipedia Annotation    object\n","Lyrics                  object\n","dtype: object\n","\n","Number of empty lyrics: 0\n","Number of empty annotations: 1\n"]}]},{"cell_type":"markdown","source":["# Model Evaluation Section"],"metadata":{"id":"icKCyGh11ds2"}},{"cell_type":"code","source":["# Install required packages\n","!pip install -q bert-score rouge-score\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from typing import List, Dict, Tuple\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","from tqdm import tqdm\n"],"metadata":{"id":"Re1Uz_6weOvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_supervised_model(\n","    model: BartForConditionalGeneration,\n","    tokenizer: BartTokenizer,\n","    test_data: pd.DataFrame,\n","    batch_size: int = 16\n",") -> Tuple[Dict[str, float], List[Dict]]:\n","    \"\"\"\n","    Evaluate supervised lyrics model comparing against Genius annotations using BART\n","    \"\"\"\n","    model.eval()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    evaluation_results = {\n","        'content_coverage': [],\n","        'consistency_score': [],\n","        'semantic_similarity': [],\n","        'rouge1_scores': [],\n","        'rouge2_scores': [],\n","        'rougeL_scores': [],\n","        'bert_scores': []\n","    }\n","\n","    examples = []\n","    previous_bert_score = 0.0\n","\n","    for idx in tqdm(range(0, len(test_data), batch_size)):\n","        batch_lyrics = test_data['Lyrics'].iloc[idx:idx + batch_size].tolist()\n","        batch_annotations = test_data['Combined Annotations'].iloc[idx:idx + batch_size].tolist()\n","\n","        # Generate summaries (BART-specific encoding)\n","        inputs = tokenizer(\n","            [f\"summarize lyrics and capture meaning: {lyric}\" for lyric in batch_lyrics],\n","            padding=True,\n","            truncation=True,\n","            max_length=1024,\n","            return_tensors=\"pt\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_ids=inputs['input_ids'],\n","                attention_mask=inputs['attention_mask'],\n","                max_length=150,\n","                min_length=50,\n","                num_beams=4,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                no_repeat_ngram_size=3,\n","                length_penalty=1.0,\n","                repetition_penalty=1.2\n","            )\n","\n","            generated_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","        # Evaluate each summary against its reference annotation\n","        for i in range(len(generated_summaries)):\n","            original_lyric = batch_lyrics[i]\n","            generated_summary = generated_summaries[i]\n","            reference_annotation = batch_annotations[i]\n","\n","            # Content Coverage (between summary and lyrics)\n","            coverage_score = calculate_content_coverage(original_lyric, generated_summary)\n","            evaluation_results['content_coverage'].append(coverage_score)\n","\n","            # Semantic Similarity (between summary and lyrics)\n","            semantic_score = calculate_semantic_similarity(original_lyric, generated_summary)\n","            evaluation_results['semantic_similarity'].append(semantic_score)\n","\n","            # ROUGE Scores (between generated summary and reference annotation)\n","            rouge_scores = calculate_rouge_scores([generated_summary, reference_annotation])\n","            evaluation_results['rouge1_scores'].append(rouge_scores['rouge1'])\n","            evaluation_results['rouge2_scores'].append(rouge_scores['rouge2'])\n","            evaluation_results['rougeL_scores'].append(rouge_scores['rougeL'])\n","\n","            # BERTScore (between generated summary and reference annotation)\n","            if i % 8 == 0:  # Compute less frequently to save time\n","                P, R, F1 = score([generated_summary], [reference_annotation], lang='en', verbose=False)\n","                previous_bert_score = F1.mean().item()\n","            evaluation_results['bert_scores'].append(previous_bert_score)\n","\n","            # Store examples\n","            if len(examples) < 5:\n","                examples.append({\n","                    'lyrics': original_lyric,\n","                    'reference_annotation': reference_annotation,\n","                    'generated_summary': generated_summary,\n","                    'metrics': {\n","                        'content_coverage': coverage_score,\n","                        'semantic_similarity': semantic_score,\n","                        'rouge1': rouge_scores['rouge1'],\n","                        'rouge2': rouge_scores['rouge2'],\n","                        'rougeL': rouge_scores['rougeL'],\n","                        'bert_score': previous_bert_score\n","                    }\n","                })\n","\n","        # Memory cleanup\n","        if idx % 5 == 0:\n","            torch.cuda.empty_cache()\n","\n","    # Aggregate results\n","    metrics = {\n","        'avg_content_coverage': np.mean(evaluation_results['content_coverage']),\n","        'avg_semantic_similarity': np.mean(evaluation_results['semantic_similarity']),\n","        'avg_rouge1': np.mean(evaluation_results['rouge1_scores']),\n","        'avg_rouge2': np.mean(evaluation_results['rouge2_scores']),\n","        'avg_rougeL': np.mean(evaluation_results['rougeL_scores']),\n","        'avg_bert_score': np.mean(evaluation_results['bert_scores'])\n","    }\n","\n","    return metrics, examples\n","\n","def calculate_rouge_scores(texts: List[str]) -> Dict[str, float]:\n","    \"\"\"Calculate ROUGE scores between texts\"\"\"\n","    rouge_scorer_obj = rouge_scorer.RougeScorer(\n","        ['rouge1', 'rouge2', 'rougeL'],\n","        use_stemmer=True\n","    )\n","    score = rouge_scorer_obj.score(texts[0], texts[1])\n","    return {\n","        'rouge1': score['rouge1'].fmeasure,\n","        'rouge2': score['rouge2'].fmeasure,\n","        'rougeL': score['rougeL'].fmeasure\n","    }\n","\n","def calculate_content_coverage(lyrics: str, summary: str) -> float:\n","    \"\"\"Calculate content coverage between lyrics and summary\"\"\"\n","    if isinstance(lyrics, float) or isinstance(summary, float):\n","        return 0.0\n","\n","    try:\n","        lyrics_tokens = set(str(lyrics).lower().split())\n","        summary_tokens = set(str(summary).lower().split())\n","        overlap = len(lyrics_tokens.intersection(summary_tokens))\n","        coverage = overlap / len(lyrics_tokens) if lyrics_tokens else 0.0\n","        return coverage\n","    except Exception as e:\n","        print(f\"Error processing lyrics/summary: {e}\")\n","        return 0.0\n","\n","def calculate_semantic_similarity(lyrics: str, summary: str) -> float:\n","    \"\"\"Calculate semantic similarity using token overlap\"\"\"\n","    if isinstance(lyrics, float) or isinstance(summary, float):\n","        return 0.0\n","\n","    try:\n","        lyrics_tokens = set(str(lyrics).lower().split())\n","        summary_tokens = set(str(summary).lower().split())\n","        intersection = len(lyrics_tokens.intersection(summary_tokens))\n","        union = len(lyrics_tokens.union(summary_tokens))\n","        return intersection / union if union > 0 else 0.0\n","    except Exception as e:\n","        print(f\"Error processing lyrics/summary: {e}\")\n","        return 0.0\n","\n","def print_evaluation_results(metrics: Dict[str, float], examples: List[Dict]):\n","    \"\"\"Print evaluation results and examples\"\"\"\n","    print(\"\\nEvaluation Results:\")\n","    print(f\"Average Content Coverage: {metrics['avg_content_coverage']:.3f}\")\n","    print(f\"Average Semantic Similarity: {metrics['avg_semantic_similarity']:.3f}\")\n","    print(f\"Average ROUGE-1: {metrics['avg_rouge1']:.3f}\")\n","    print(f\"Average ROUGE-2: {metrics['avg_rouge2']:.3f}\")\n","    print(f\"Average ROUGE-L: {metrics['avg_rougeL']:.3f}\")\n","    print(f\"Average BERTScore: {metrics['avg_bert_score']:.3f}\")\n","\n","    print(\"\\nExample Generations:\")\n","    for i, example in enumerate(examples, 1):\n","        print(f\"\\nExample {i}:\")\n","        print(f\"Original Lyrics (truncated): {example['lyrics'][:200]}...\")\n","        print(f\"\\nReference Annotation: {example['reference_annotation']}\")\n","        print(f\"\\nGenerated Summary: {example['generated_summary']}\")\n","        print(\"\\nMetrics:\")\n","        for metric, value in example['metrics'].items():\n","            print(f\"{metric}: {value:.3f}\")\n","\n","def run_evaluation(model_path: str, test_df: pd.DataFrame):\n","    \"\"\"Run complete evaluation pipeline\"\"\"\n","    tokenizer = BartTokenizer.from_pretrained(model_path)\n","    model = BartForConditionalGeneration.from_pretrained(model_path)\n","    print(\"Model and tokenizer loaded successfully!\")\n","\n","    metrics, examples = evaluate_supervised_model(\n","        model,\n","        tokenizer,\n","        test_data=test_df,\n","        batch_size=16\n","    )\n","\n","    print_evaluation_results(metrics, examples)\n","    return metrics, examples\n"],"metadata":{"id":"A-yUhp_Oe1Vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics, examples = run_evaluation(model_path, test_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1af67d1ba5de4b50a865711cc7a23e4d","1ca9378f771841d28e9293c3bac579a9","c32adaa0581c488cab212b6f8fdbacfb","b23930b3c1aa407f9d29805d594f53ea","8e31235e62ef45ab861b9e0f2e471cea","7e6ba71dbe1d409d9cd6e40f9a299dea","65e723d02f4340f2b9d4e1273c46f8fb","b4597095cddc40bba060b4894ceef897","cc0c21c20e134878a778d7e24c5170b9","43ced1a598b14a4d8ecbb82d29122062","367c9982dac54ee493e1802d50080aba","463a75401c41413e87f575abfc5d52ad","6372ae77a7e641118bc5e3dd7a5d7ad0","75f370caa0d74017abb367ddbe605dc4","1476c28e3d7d475c8abbfa48e4a7b9bd","5d34d55bca8b4305a89ec63d0abbee6d","6cdc5478f1424211bc5b6bc28a3529d6","6e3b09944c5843ec890de7e2a01134bc","d03c9a4e2c6f475d894f6fe5c2113f84","2e3ee3f9176742debce9a132fda3e203","45d693dcdef44d85b8bd5a2bd858c950","12e2dac4eae24ceba88805c5dac25b7d","87222a5b40dc48089b66519e1856c892","3eba43be7d35415e9935ad634f6a5517","3120cf7f9f03488b8a0101a35bed34a8","9b772286aa4940f9a97b365607119211","455d244ccad54a0da46db50d9600eb58","e029535f6daa41cf966497189086c849","810bc5c467964bbc866561676d1bda26","dc85c8edb2a34796ac4d5dfed9a81e94","71ff016389e445b8b06701d3e5b2970a","a88ed2aee5b94588a54d476ba95b238c","99f763d6446145528a1b7725cee66ecb","ccbcd095c48d458c8dc1b626d52235bd","b0c2f91ccbff4a6b97baec739aa487d7","ee7b8a9bba474ba2bf15e4fbd0a60748","e2e3c7c0ea684bdf9bd87d3c6108d22a","0f7c68f4b50e4086b1b6a46bd727e148","f49849862d3a4bb1b54832f329f89892","c68dcb555f8c4429960ed48edb66c4b2","023cdfbdae1a4a8ca98e73be18f4daa9","25e3bcb4eb1940178553e788346d32b5","8443315dc49a43c5a259c1c7967cf945","a6b2a117e2a24583b8a7810538ecd631","d377ec3a518140808d435d3308114294","1434b9d737b94de8a1fa8e381645cb81","a31779dd3f3a433581e3bbfc8bcd254f","13bffb3372f74c52a01826c96be4fe06","454427b4c84e41ed8a9771ce9f7c4b9b","2ff26bab2f11486586d59006bdcb074d","2ae829b84d1542eebb736cccb70dd2c6","864915fdb18b49c48ba166d82b0b86bb","a8b5fbcf35d64e30b9b7cc184ca033a4","d461667083294520a48c810ecff60efe","3fb0b28f55a440e7bea511358b558f96","f339c376ec24491cb649c6a7ba21800b","244667cec0cd4e9f82a4782899fba2b8","a82c89d4e2a94361af1c9283ec44084b","2407339844354695a4bcea42d8b6100b","74034e29cdff4a8297b49074cfe6cada","600f0513af034cd5857692e3a0f65ac1","6a911f47081940bfbd9e2d0a1a0bd878","050db5a779c1453589b5856ead8459d3","d98d4929f7384427ab9321e1f313d369","3d6f99845abd4bd8b7ce7296d3e82746","aec11531fdb7421aa0db4e89dac0aeac"]},"id":"um479POye4sF","executionInfo":{"status":"ok","timestamp":1733464158631,"user_tz":480,"elapsed":538508,"user":{"displayName":"Sahana S","userId":"09133778862757081963"}},"outputId":"512712e1-0083-41fc-890e-7dce2468f8c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/38 [00:00<?, ?it/s]"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1af67d1ba5de4b50a865711cc7a23e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463a75401c41413e87f575abfc5d52ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87222a5b40dc48089b66519e1856c892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccbcd095c48d458c8dc1b626d52235bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d377ec3a518140808d435d3308114294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f339c376ec24491cb649c6a7ba21800b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  3%|▎         | 1/38 [00:23<14:23, 23.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  5%|▌         | 2/38 [00:37<10:48, 18.02s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  8%|▊         | 3/38 [00:50<09:09, 15.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 11%|█         | 4/38 [01:05<08:41, 15.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 13%|█▎        | 5/38 [01:22<08:47, 15.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 16%|█▌        | 6/38 [01:35<08:01, 15.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 18%|█▊        | 7/38 [01:53<08:11, 15.86s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 21%|██        | 8/38 [02:06<07:28, 14.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 24%|██▎       | 9/38 [02:18<06:52, 14.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 26%|██▋       | 10/38 [02:32<06:32, 14.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 29%|██▉       | 11/38 [02:44<05:59, 13.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 32%|███▏      | 12/38 [02:58<05:52, 13.55s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 34%|███▍      | 13/38 [03:12<05:43, 13.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 37%|███▋      | 14/38 [03:26<05:29, 13.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 39%|███▉      | 15/38 [03:40<05:18, 13.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 42%|████▏     | 16/38 [03:52<04:54, 13.40s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 45%|████▍     | 17/38 [04:07<04:48, 13.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 47%|████▋     | 18/38 [04:22<04:47, 14.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 50%|█████     | 19/38 [04:37<04:35, 14.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 53%|█████▎    | 20/38 [04:52<04:24, 14.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 55%|█████▌    | 21/38 [05:08<04:12, 14.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 58%|█████▊    | 22/38 [05:21<03:51, 14.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 61%|██████    | 23/38 [05:35<03:34, 14.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 63%|██████▎   | 24/38 [05:47<03:10, 13.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 66%|██████▌   | 25/38 [06:02<03:00, 13.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 68%|██████▊   | 26/38 [06:17<02:50, 14.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 71%|███████   | 27/38 [06:30<02:33, 13.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 74%|███████▎  | 28/38 [06:44<02:19, 13.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 76%|███████▋  | 29/38 [07:00<02:11, 14.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 79%|███████▉  | 30/38 [07:14<01:54, 14.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 82%|████████▏ | 31/38 [07:28<01:40, 14.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 84%|████████▍ | 32/38 [07:40<01:22, 13.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 87%|████████▋ | 33/38 [07:55<01:09, 13.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 89%|████████▉ | 34/38 [08:09<00:55, 13.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 92%|█████████▏| 35/38 [08:22<00:41, 13.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 95%|█████████▍| 36/38 [08:37<00:28, 14.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"," 97%|█████████▋| 37/38 [08:52<00:14, 14.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 38/38 [08:57<00:00, 14.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results:\n","Average Content Coverage: 0.064\n","Average Semantic Similarity: 0.051\n","Average ROUGE-1: 0.217\n","Average ROUGE-2: 0.052\n","Average ROUGE-L: 0.141\n","Average BERTScore: 0.836\n","\n","Example Generations:\n","\n","Example 1:\n","Original Lyrics (truncated): Keep the snow and sleigh rides\r\n","Keep those silver bells\r\n","Keep the gifts beneath the tree\r\n","Give them to someone else\r\n","Keep that magic snowman\r\n","Keep those twinkly lights\n","\n","Keep the reindeer\n","\n","My heart alr...\n","\n","Reference Annotation: The Voice coaches Ariana Grande and Kelly Clarkson sing of not wanting any of the Christmas festivities because they didn’t get what they wanted, and they beg for Santa to give them what they wanted because otherwise they don’t see a point in Christmas.\n","\n","Generated Summary: Meaning and themes: “Santa, Can’t You Hear Me?” is a song by American singer-songwriter . It was released as the lead single from her fourth studio album, . The song was first previewed on December 8th, 2017, when it was released on iTunes.\n","\n","Metrics:\n","content_coverage: 0.079\n","semantic_similarity: 0.062\n","rouge1: 0.135\n","rouge2: 0.000\n","rougeL: 0.090\n","bert_score: 0.820\n","\n","Example 2:\n","Original Lyrics (truncated): Dance like yo daddy\r\n","Dance like yo daddy\r\n","Da-Dance like yo daddy\r\n","Try not to dance too much\r\n","Been working way too much, I need a day off\n","\n","Damn this beat go hard (so hard)\n","\n","And all I wanna do (wanna da...\n","\n","Reference Annotation: This song is yet another anthem about bravery and standing up for yourself. To dance like your dad or any of your parents means that person is dancing in a way that makes everybody else laugh at him or her, even though they are trying their best to fit in. Meghan sings against that and says that you can do whatever you want to do and if people laugh at you, let them laugh – if you pay attention to what makes you happy, then nothing else should matter. In other words, it is a message which makes us have pure confidence in ourselves.\n","\n","Generated Summary: Meaning and themes: “Dance Like Yo Daddy” is the third single from Meghan Trainor’s debut album, . The song was released as a single from the album. The song is written by Meghan, who is also a part of the band.\n","\n","Metrics:\n","content_coverage: 0.056\n","semantic_similarity: 0.047\n","rouge1: 0.167\n","rouge2: 0.028\n","rougeL: 0.111\n","bert_score: 0.820\n","\n","Example 3:\n","Original Lyrics (truncated): [Intro]\r\n","See people always remember\r\n","When you ain't do nothing for them, they forget\r\n","Forget the best thing you can do for them, you feel me\r\n","So I just tell'em please don't act like act, no\n","\n","[Hook]\n","\n","A...\n","\n","Reference Annotation: d\n","\n","Generated Summary: Meaning and themes: “Don’t Act Like That” is the third single from Lil Uzi Vert’s second studio album, . The song was first teased on January 20th, 2021, via his Instagram story.\n","\n","Metrics:\n","content_coverage: 0.050\n","semantic_similarity: 0.045\n","rouge1: 0.000\n","rouge2: 0.000\n","rougeL: 0.000\n","bert_score: 0.820\n","\n","Example 4:\n","Original Lyrics (truncated): I'm a fuckin' bad guy nigga\r\n","Murder Gang nigga\r\n","Two guns up\r\n","I'm poppin' ain't I\r\n","I got 10 racks in every damn pocket ain't I\n","\n","(21, 21)\n","\n","I'm poppin' ain't I\n","\n","I got 10 racks in every damn pocket ain't ...\n","\n","Reference Annotation: On “Bad Guy,” 21 Savage is embracing his ego. He confidently brags how his money can get him anything he wants, from girls to guns. 21 also boasts how much of a villain he is, telling you he’ll take your girl, rob you, or kill you if he has to.\n","\n","Generated Summary: Meaning and themes: “Bad Guy” is the third single from 21 Savage’s third studio album, . The song was released on August 31st, 2021, as the lead single from the album. The song features 21 Savage as the featured artist on the track.\n","\n","Metrics:\n","content_coverage: 0.035\n","semantic_similarity: 0.030\n","rouge1: 0.170\n","rouge2: 0.043\n","rougeL: 0.128\n","bert_score: 0.820\n","\n","Example 5:\n","Original Lyrics (truncated): Um, excuse me mister but can you please turn down the lights\r\n","I don't really like all these cameras, man\r\n","And this shit just don't feel right\r\n","And I don't really wanna be rude to you, sir\r\n","But fuck yo...\n","\n","Reference Annotation: “DEATHCAMP” is the first track off Tyler’s fourth album,  , that went up for pre-order unannounced on April 8, 2015.  The song was also  , as the second half of the video for   Tyler sent out a series of tweets on the track, describing   by   by  .   Also, interestingly this is Tyler’s first release where the the opening track is not named after the album it’s on.\n","\n","Generated Summary: Meaning and themes: “Welcome to Death Camp” is the second single from Gunna’s third studio album, . The song was first teased on July 29th, 2018. Gunna teased the song on his Instagram story on July 31st.\n","\n","Metrics:\n","content_coverage: 0.031\n","semantic_similarity: 0.028\n","rouge1: 0.248\n","rouge2: 0.078\n","rougeL: 0.190\n","bert_score: 0.820\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"2D6M9LTbc9LY"}}]}